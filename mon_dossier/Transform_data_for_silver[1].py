#!/usr/bin/env python
# coding: utf-8

# ## Transform data for silver 
# 
# New notebook

# ## Documentation du Notebook : Transformation des donn√©es de Bronze √† Silver

# ###  Contexte

# ###### Ce notebook s'inscrit dans une architecture Medallion mise en place dans Microsoft Fabric, et assure la transformation des donn√©es brutes (bronze) vers un format nettoy√© et structur√© (silver), en pr√©paration pour leur exploitation ult√©rieure (gold / rapport Power BI).
# 
# 

# ### üì¶ Importation des biblioth√®ques n√©cessaires
# 
# Dans cette section, nous importons les biblioth√®ques requises pour la suite du traitement :
# 
# - `pyspark.sql.types` : permet de d√©finir les sch√©mas (types de colonnes) pour structurer correctement les DataFrames.
# - `delta.tables` : n√©cessaire pour manipuler les tables Delta (mise √† jour, suppression, merge, etc.), qui assurent la fiabilit√© et la tra√ßabilit√© des donn√©es dans la couche Silver.
# 

# In[1]:


# Importation des types de donn√©es Spark SQL n√©cessaires pour d√©finir ou manipuler les sch√©mas de DataFrames
from pyspark.sql.types import *

# Importation des classes n√©cessaires pour manipuler des tables Delta (lecture, √©criture, mise √† jour)
from delta.tables import *


# ### üßæ D√©finition du sch√©ma des donn√©es en entr√©e
# 
# Afin d'assurer une lecture fiable et coh√©rente des donn√©es CSV, nous d√©finissons manuellement le sch√©ma attendu du fichier. Cela permet :
# 
# - d'√©viter que Spark d√©duise automatiquement les types de colonnes, ce qui peut entra√Æner des erreurs ou des incoh√©rences,
# - de garantir une structure stable et ma√Ætris√©e pour les √©tapes de traitement suivantes.
# 
# Chaque champ est typ√© explicitement (ex. : `TimestampType` pour les dates, `StringType` pour les cha√Ænes de caract√®res).
# 

# In[2]:


#On d√©finit le sh√©ma du fichier csv pour √©viter que Spark inf√©re automatiquement le type de colonnes 

ticketSchema = StructType([
    StructField("Numero", StringType()),
    StructField("Ouvert", TimestampType()),
    StructField("Mis_a_jour", TimestampType()),
    StructField("Societe", StringType()),
    StructField("Priorite", StringType()),
    StructField("Groupe_affectation", StringType()),
    StructField("Etat", StringType()),
    StructField("Ouvert_par", StringType()),
    StructField("Categorie", StringType()),
    StructField("N_du_ticket_externe", StringType())
])


# ### üì• Chargement des donn√©es brutes (zone Bronze)
# 
# Cette cellule permet de charger les donn√©es issues de la couche Bronze :
# 
# - Le fichier source est un CSV situ√© dans le r√©pertoire `Files/bronze` du Lakehouse.
# - L‚Äôoption `.option("header", "true")` indique que la premi√®re ligne du fichier contient les noms de colonnes.
# - Le sch√©ma explicite d√©fini pr√©c√©demment (`ticketSchema`) est appliqu√© pour forcer le typage correct des colonnes d√®s la lecture.
# 
# Ce chargement constitue la premi√®re √©tape du traitement vers la couche Silver.
# 

# In[3]:


# Chargement des fichiers depuis le dossier bronze du lakehouse
df = spark.read.format("csv").option("header", "true").schema(ticketSchema).load("Files/bronze/test2.csv")


# ### üß¨ V√©rification du sch√©ma du DataFrame `df`
# 
# Apr√®s avoir d√©fini manuellement le sch√©ma `ticketSchema` pour garantir une lecture fiable des colonnes du fichier CSV, il est important de s'assurer que ce sch√©ma a bien √©t√© appliqu√© lors du chargement des donn√©es.
# 
# La commande `df.printSchema()` permet d'afficher la structure du DataFrame, en pr√©cisant pour chaque colonne :
# - son nom,
# - son type de donn√©es (ex. : `StringType`, `TimestampType`, etc.),
# - et si elle accepte les valeurs nulles.
# 
# Cela constitue une √©tape essentielle pour valider que les donn√©es sont bien conformes aux attentes avant d'entamer les traitements analytiques.
# 

# In[4]:


# Affiche la structure (sch√©ma) du DataFrame pour v√©rifier que les colonnes et types ont bien √©t√© interpr√©t√©s selon le sch√©ma d√©fini
df.printSchema()


# In[5]:


# Affichage des 10 premi√®res lignes
df.show(10, truncate=False)


# In[6]:


# Calcul du nombre total de records (lignes) dans le DataFrame `df`
total_records = df.count()

# Affichage du nombre de records
print(f"Nombre total de records : {total_records}")


# ### üìä V√©rification de la qualit√© des donn√©es
# 
# Dans cette section, nous v√©rifions deux aspects importants de la qualit√© des donn√©es dans notre DataFrame :
# 
# 1. **Comptage des valeurs nulles** : Nous comptons les valeurs nulles dans chaque colonne afin d'identifier les colonnes potentiellement incompl√®tes ou mal renseign√©es. Cela permet de prendre des mesures correctives, telles que l'imputation de donn√©es ou la suppression de colonnes probl√©matiques.
# 
# 2. **D√©tection des doublons** : Nous v√©rifions si des doublons existent dans le DataFrame en comparant le nombre total de lignes avec le nombre de lignes distinctes. Cette √©tape permet de garantir que les donn√©es ne sont pas redondantes, ce qui est essentiel pour des analyses pr√©cises et fiables.
# 

# In[7]:


from pyspark.sql.functions import col, when, sum

# Compter les valeurs nulles par colonne
null_counts = df.select([ 
    (when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns
])

# Effectuer l'agr√©gation pour obtenir le nombre de valeurs nulles
null_counts_agg = null_counts.agg(*[sum(col(c)).alias(c) for c in df.columns])

# Afficher le nombre de valeurs nulles par colonne
null_counts_agg.show()


# In[8]:


# Calcul du nombre total de lignes et du nombre de lignes distinctes
total_records = df.count()
distinct_records = df.distinct().count()

# Calcul et affichage du nombre de doublons
duplicates = total_records - distinct_records
print(f"Nombre de doublons : {duplicates}")


# ### üóÇÔ∏è Cr√©ation de la table Delta pour la couche Silver
# 
# Dans cette √©tape, nous d√©finissons la structure de la table `supervision_silver` au sein du sch√©ma `Inetum_Data`.  
# La m√©thode `createIfNotExists` permet de cr√©er la table **uniquement si elle n'existe pas d√©j√†**, ce qui √©vite les erreurs en cas de r√©ex√©cution du notebook.
# 
# #### üìå Pourquoi utiliser une table Delta dans la couche Silver ?
# 
# L'utilisation du **format Delta** pr√©sente plusieurs avantages cl√©s, notamment dans une architecture Medallion :
# 
# - ‚úÖ **Gestion des versions** (time travel) : possibilit√© de revenir √† une version ant√©rieure des donn√©es.
# - ‚úÖ **Mise √† jour et suppression facilit√©es** (`MERGE`, `UPDATE`, `DELETE`) : tr√®s utile pour g√©rer des donn√©es √©volutives.
# - ‚úÖ **Fiabilit√© et int√©grit√© des donn√©es** : transactions ACID garanties.
# - ‚úÖ **Optimisation des performances** : via la gestion des fichiers et l‚Äôindexation (Delta cache, Z-Ordering‚Ä¶).
# - ‚úÖ **Int√©gration native avec Spark & Fabric** : facilite l‚Äôexploitation en downstream (tables gold, dashboards‚Ä¶).
# 
# Cette table accueillera donc les donn√©es transform√©es, nettoy√©es et structur√©es issues de la couche Silver, pr√™tes √† √™tre utilis√©es pour l'analyse ou la visualisation.
# 

# In[9]:


# Cr√©ation de la table Delta 'supervision_silver' dans le sch√©ma 'Inetum_Data' si elle n'existe pas encore. 
# Cette table accueillera les donn√©es nettoy√©es et structur√©es issues de la couche Silver.   
DeltaTable.createIfNotExists(spark) \
     .tableName("Inetum_Data.supervision_silver") \
     .addColumn("Numero", StringType()) \
     .addColumn("Ouvert", TimestampType()) \
     .addColumn("Mis_a_jour", TimestampType()) \
     .addColumn("Societe", StringType()) \
     .addColumn("Priorite", StringType()) \
     .addColumn("Groupe_affectation", StringType()) \
     .addColumn("Etat", StringType()) \
     .addColumn("Ouvert_par", StringType()) \
     .addColumn("Categorie", StringType()) \
     .addColumn("N_du_ticket_externe", StringType()) \
     .execute() 


# ### üîÑ Synchronisation des donn√©es dans la table Delta (MERGE)
# 
# Dans cette √©tape, nous utilisons la commande `MERGE` pour synchroniser les donn√©es du DataFrame `df` avec la table Delta `supervision_silver`.  
# L'objectif est de **mettre √† jour** les enregistrements existants et **ins√©rer** les nouveaux.
# 
# #### Avantages du `MERGE` dans une table Delta :
# - ‚úÖ √âvite les doublons en mettant √† jour les donn√©es existantes selon une cl√© de correspondance.
# - ‚úÖ Ajoute automatiquement les nouvelles lignes non pr√©sentes dans la table cible.
# - ‚úÖ Garantit l'int√©grit√© des donn√©es gr√¢ce aux transactions ACID.
# - ‚úÖ Recommand√© dans les traitements **Silver** o√π les donn√©es peuvent √™tre mises √† jour r√©guli√®rement (ex : suivi de tickets).
# 
# Les conditions de correspondance (clause `ON`) sont bas√©es sur des colonnes stables permettant d‚Äôidentifier de mani√®re unique un ticket (`Numero`, `Ouvert`, etc.).
# 

# In[10]:


from delta.tables import *

# R√©cup√©ration de la table Delta existante dans le sch√©ma 'Inetum_Data'
deltaTable = DeltaTable.forName(spark, "Inetum_Data.supervision_silver")

# DataFrame contenant les nouvelles donn√©es nettoy√©es ou mises √† jour
dfUpdates = df  

# Synchronisation des donn√©es :
# - Si une correspondance est trouv√©e sur les colonnes cl√©s, les colonnes restantes sont mises √† jour.
# - Sinon, la ligne est ins√©r√©e comme nouvelle.

deltaTable.alias("silver") \
  .merge(
    dfUpdates.alias("updates"),
    """
    silver.Numero = updates.Numero
    AND silver.Ouvert = updates.Ouvert
    AND silver.Societe = updates.Societe
    AND silver.Groupe_affectation = updates.Groupe_affectation
    """
  ) \
  .whenMatchedUpdate(set =
    {
      "Mis_a_jour": "updates.Mis_a_jour",
      "Priorite": "updates.Priorite",
      "Etat": "updates.Etat",
      "Ouvert_par": "updates.Ouvert_par",
      "Categorie": "updates.Categorie",
      "N_du_ticket_externe": "updates.N_du_ticket_externe",
    }
  ) \
  .whenNotMatchedInsert(values =
    {
      "Numero": "updates.Numero",
      "Ouvert": "updates.Ouvert",
      "Mis_a_jour": "updates.Mis_a_jour",
      "Societe": "updates.Societe",
      "Priorite": "updates.Priorite",
      "Groupe_affectation": "updates.Groupe_affectation",
      "Etat": "updates.Etat",
      "Ouvert_par": "updates.Ouvert_par",
      "Categorie": "updates.Categorie",
      "N_du_ticket_externe": "updates.N_du_ticket_externe",
    }
  ) \
  .execute()


# In[11]:


# Chargement de la table 'Inetum_Data.supervision_silver' dans un DataFrame Spark
df = spark.read.table("Inetum_Data.supervision_silver")


# In[12]:


# Affichage des 10 premi√®res lignes du DataFrame pour une inspection rapide des donn√©es
display(df.head(10))


# ### üßæ D√©finition du sch√©ma des donn√©es en entr√©e
# 
# Afin d'assurer une lecture fiable et coh√©rente des donn√©es CSV, nous d√©finissons manuellement le sch√©ma attendu du fichier. Cela permet :
# 
# - d'√©viter que Spark d√©duise automatiquement les types de colonnes, ce qui peut entra√Æner des erreurs ou des incoh√©rences,
# - de garantir une structure stable et ma√Ætris√©e pour les √©tapes de traitement suivantes.
# 
# Chaque champ est typ√© explicitement, comme ici :
# - `TimestampType` pour repr√©senter des dates et heures (colonne `Ouvert`),
# - `StringType` pour repr√©senter des cha√Ænes de caract√®res (colonne `Plage_horaire`).
# 

# In[13]:


#On d√©finit le sh√©ma du fichier csv pour √©viter que Spark inf√©re automatiquement le type de colonnes 

ShiftSchema = StructType([
    StructField("Ouvert", TimestampType()),
    StructField("Plage_horaire", StringType()),
])


# In[14]:


# Chargement des fichiers depuis le dossier bronze du lakehouse
df_Shift = spark.read.format("csv").option("header", "true").schema(ShiftSchema).load("Files/bronze/dim_shift.csv")


# ### üß¨ Affichage du sch√©ma du DataFrame `df_Shift`
# 
# La commande `printSchema()` permet d'afficher la structure du DataFrame `df_Shift`. Elle montre, pour chaque colonne :
# 
# - le nom,
# - le type de donn√©es (ex. : `TimestampType`, `StringType`, etc.),
# - et si la colonne peut contenir des valeurs nulles.
# 
# Cette visualisation est utile pour v√©rifier que les colonnes du DataFrame sont bien typ√©es avant de les ins√©rer ou de les comparer dans une table Delta.
# 

# In[15]:


# Affiche la structure (sch√©ma) du DataFrame pour v√©rifier que les colonnes et types ont bien √©t√© interpr√©t√©s selon le sch√©ma d√©fini

df_Shift.printSchema()


# In[16]:


# Affichage des 10 premi√®res lignes
df_Shift.show(10, truncate=False)


# ### üóÇÔ∏è Cr√©ation conditionnelle d'une table Delta
# 
# Cette instruction permet de cr√©er la table Delta `Inetum_Data.Dim_Shift` uniquement si elle n'existe pas d√©j√† dans le catalogue. Cette approche √©vite les erreurs dues √† des tentatives de recr√©ation de table, tout en assurant la disponibilit√© de la structure n√©cessaire au traitement.
# 
# L'utilisation de `DeltaTable.createIfNotExists()` pr√©sente plusieurs avantages :
# 
# - Pr√©vention des erreurs lors de l'ex√©cution r√©p√©t√©e du notebook,
# - Garantie que la table est bien pr√©sente avant insertion ou transformation,
# - D√©finition explicite de la structure (nom des colonnes et types).
# 
# La table comporte :
# - une colonne `Ouvert` de type `TimestampType`,
# - une colonne `Plage_horaire` de type `StringType`.
# 

# In[17]:


# Cr√©ation conditionnelle de la table Delta 'Dim_Shift' si elle n'existe pas d√©j√†
    
DeltaTable.createIfNotExists(spark) \
     .tableName("Inetum_Data.Dim_Shift") \
     .addColumn("Ouvert", TimestampType()) \
     .addColumn("Plage_horaire", StringType()) \
     .execute() 


# ### ‚úÖ Mise √† jour incr√©mentielle de la table Delta avec `MERGE`
# 
# Cette op√©ration permet d‚Äôeffectuer une mise √† jour incr√©mentielle de la table `Inetum_Data.Dim_Shift` via une commande `MERGE`. Elle combine les actions d‚Äô**UPDATE** (mise √† jour des lignes existantes) et d‚Äô**INSERT** (insertion des nouvelles lignes), en fonction d‚Äôune condition de correspondance.
# 
# Avantages de cette approche :
# - √âvite les doublons dans la table cible,
# - G√®re automatiquement les mises √† jour et les insertions en une seule commande,
# - Id√©al pour les traitements de type *upsert* (update + insert) dans les pipelines Delta.
# 
# Le bloc ci-dessous :
# - recherche les correspondances entre la table `Dim_Shift` et le DataFrame `df_Shift` sur les colonnes `Ouvert` et `Plage_horaire`,
# - ins√®re les lignes non pr√©sentes dans la table cible.
# 

# In[18]:


from delta.tables import *

# R√©f√©rence √† la table Delta existante
deltaTable = DeltaTable.forName(spark, "Inetum_Data.Dim_Shift")

# DataFrame contenant les nouvelles donn√©es √† ins√©rer ou √† mettre √† jour
dfUpdates = df_Shift  

# Ex√©cution du MERGE : mise √† jour si correspondance, sinon insertion
deltaTable.alias("silver") \
  .merge(
    dfUpdates.alias("updates"),
    """
    silver.Ouvert = updates.Ouvert
    AND silver.Plage_horaire = updates.Plage_horaire
    """
  ) \
  .whenMatchedUpdate(set =
    {
      # Aucune mise √† jour √† faire ici car les cl√©s d'identification sont aussi les seules colonnes
      # Ce bloc est laiss√© vide intentionnellement
      
    }
  ) \
  .whenNotMatchedInsert(values =
    {
      "Ouvert": "updates.Ouvert",
      "Plage_horaire": "updates.Plage_horaire"
      
    }
  ) \
  .execute()


# ### üß¨ Affichage du sch√©ma du DataFrame `df_Shift`
# 
# La commande `printSchema()` permet d'afficher la structure du DataFrame `df_Shift`. Elle montre, pour chaque colonne :
# 
# - le nom,
# - le type de donn√©es (ex. : `TimestampType`, `StringType`, etc.),
# - et si la colonne peut contenir des valeurs nulles.
# 
# Cette visualisation est utile pour v√©rifier que les colonnes du DataFrame sont bien typ√©es avant de les ins√©rer ou de les comparer dans une table Delta.
# 

# In[19]:


# Affiche la structure (sch√©ma) du DataFrame pour v√©rifier que les colonnes et types ont bien √©t√© interpr√©t√©s selon le sch√©ma d√©fini
df_Shift.printSchema()


# In[20]:


# Lecture de la table Delta 'Dim_Shift' depuis le catalogue et affichage des 10 premi√®res lignes.
# Cela permet de v√©rifier visuellement le contenu de la dimension des shifts (cr√©neaux horaires),
# notamment les valeurs pr√©sentes dans les colonnes 'Ouvert' et 'Plage_horaire'.
df_Shift = spark.read.table("Inetum_Data.Dim_Shift")
display(df_Shift.head(10))


# ## üìä V√©rification de la qualit√© des donn√©es
# Dans cette section, nous v√©rifions deux aspects importants de la qualit√© des donn√©es dans notre DataFrame :
# 
# 1. **Comptage des valeurs nulles** : Nous comptons les valeurs nulles dans chaque colonne afin d'identifier les colonnes potentiellement incompl√®tes ou mal renseign√©es. Cela permet de prendre des mesures correctives, telles que l'imputation de donn√©es ou la suppression de colonnes probl√©matiques.
# 
# 2. **D√©tection des doublons** : Nous v√©rifions si des doublons existent dans le DataFrame en comparant le nombre total de lignes avec le nombre de lignes distinctes. Cette √©tape permet de garantir que les donn√©es ne sont pas redondantes, ce qui est essentiel pour des analyses pr√©cises et fiables.

# In[21]:


from pyspark.sql.functions import col, when, sum

# Compter les valeurs nulles par colonne dans df_Shift
null_counts = df_Shift.select([ 
    (when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df_Shift.columns
])

# Effectuer l'agr√©gation pour obtenir le nombre de valeurs nulles
null_counts_agg = null_counts.agg(*[sum(col(c)).alias(c) for c in df_Shift.columns])

# Afficher le nombre de valeurs nulles par colonne
null_counts_agg.show()


# ### üîç D√©tection des doublons dans `df_Shift` par `Ouvert` et `Plage_horaire`
# 
# Ce bloc de code permet de d√©tecter les lignes du DataFrame `df_Shift` qui ont des valeurs dupliqu√©es dans les colonnes `Ouvert` et `Plage_horaire`. Cela permet de v√©rifier s'il y a plusieurs tickets ouverts √† la m√™me heure et pour la m√™me plage horaire.
# 
# #### √âtapes du traitement :
# - `groupBy("Ouvert", "Plage_horaire")` : regroupe les lignes par les colonnes `Ouvert` et `Plage_horaire`, afin de trouver des doublons en fonction de ces deux champs.
# - `agg(count("*").alias("nb"))` : calcule le nombre d'occurrences pour chaque combinaison de `Ouvert` et `Plage_horaire`.
# - `filter("nb > 1")` : filtre pour ne conserver que les groupes o√π le nombre d'occurrences est sup√©rieur √† 1, c'est-√†-dire les doublons.
# - `show(truncate=False)` : affiche les doublons trouv√©s sans tronquer les valeurs des colonnes.
# 
# Cela est utile pour v√©rifier que chaque plage horaire n‚Äôa pas plusieurs tickets associ√©s √† la m√™me heure d‚Äôouverture.
# 

# In[22]:


# Identification des doublons dans le DataFrame `df_Shift` en fonction des colonnes 'Ouvert' et 'Plage_horaire'.
from pyspark.sql.functions import count

df_Shift.groupBy("Ouvert", "Plage_horaire") \
  .agg(count("*").alias("nb")) \
  .filter("nb > 1") \
  .show(truncate=False)


# ### üî¢ Calcul du nombre de doublons dans `df_Shift`
# 
# Ce bloc de code calcule et affiche le nombre total de lignes, le nombre de lignes distinctes, ainsi que le nombre de doublons dans le DataFrame `df_Shift`.
# 
# #### √âtapes du traitement :
# - `total = df_Shift.count()` : calcule le nombre total de lignes dans le DataFrame `df_Shift`.
# - `distincts = df_Shift.distinct().count()` : calcule le nombre de lignes distinctes en √©liminant les doublons.
# - `total - distincts` : calcule le nombre de doublons en faisant la diff√©rence entre le total des lignes et le nombre de lignes distinctes.
# - `print()` : affiche les r√©sultats sous forme de texte.
# 
# Cela permet de rapidement obtenir une vue d'ensemble des doublons dans le DataFrame.
# 

# In[23]:


from pyspark.sql.functions import countDistinct

# Calcul du nombre total de lignes dans le DataFrame df_Shift
total = df_Shift.count()

# Calcul du nombre de lignes distinctes (en √©liminant les doublons)
distincts = df_Shift.distinct().count()

# Affichage du total, des lignes distinctes et des doublons
# Le nombre de doublons est calcul√© comme la diff√©rence entre le total et les lignes distinctes.
print(f"Total lignes : {total}, Lignes distinctes : {distincts}, Doublons : {total - distincts}")


# In[24]:


# Identification des doublons dans le DataFrame `df_Shift` en fonction des colonnes 'Ouvert' et 'Plage_horaire'.
df_duplicates = df_Shift.groupBy("Ouvert", "Plage_horaire") \
  .agg(count("*").alias("nb")) \
  .filter("nb > 1") # On filtre pour ne garder que les combinaisons de 'Ouvert' et 'Plage_horaire' ayant plus d'une occurrence.

# Jointure entre le DataFrame d'origine et le DataFrame des doublons pour extraire les lignes dupliqu√©es.
df_Shift.join(df_duplicates, on=["Ouvert", "Plage_horaire"], how="inner").show(truncate=False) 
# Affichage des doublons trouv√©s, sans tronquer les valeurs des colonnes.


# ### üßπ Suppression des doublons dans `df_Shift`
# 
# Le code suivant permet de supprimer toutes les lignes dupliqu√©es dans le DataFrame `df_Shift`, en conservant uniquement les lignes uniques. Il utilise la m√©thode `dropDuplicates()`, qui supprime les lignes o√π toutes les colonnes sont identiques.
# 
# #### Explication :
# - `dropDuplicates()` : cette m√©thode supprime les doublons en v√©rifiant l'√©galit√© sur l'ensemble des colonnes du DataFrame. Les lignes ayant exactement les m√™mes valeurs dans toutes les colonnes seront consid√©r√©es comme des doublons et supprim√©es.
# 
# Cela est utile pour nettoyer les donn√©es et √©viter les biais ou les erreurs dans les analyses ou rapports ult√©rieurs.
# 

# In[25]:


# Suppression des doublons dans le DataFrame `df_Shift` en utilisant la m√©thode `dropDuplicates()`.
# Cette m√©thode √©limine les lignes identiques (toutes les colonnes doivent correspondre pour √™tre consid√©r√©es comme des doublons),
# laissant uniquement les lignes uniques dans le DataFrame.
df_Shift_clean = df_Shift.dropDuplicates()


# ### üî¢ V√©rification des doublons apr√®s nettoyage dans `df_Shift_clean`
# 
# Ce bloc de code permet de v√©rifier le nombre total de lignes, le nombre de lignes distinctes, et le nombre de doublons restants dans le DataFrame `df_Shift_clean`, apr√®s avoir √©limin√© les doublons √† l'aide de `dropDuplicates()`.
# 
# #### √âtapes du traitement :
# - `total = df_Shift_clean.count()` : calcule le nombre total de lignes dans le DataFrame `df_Shift_clean`.
# - `distincts = df_Shift_clean.distinct().count()` : calcule le nombre de lignes distinctes dans le DataFrame `df_Shift_clean`.
# - `total - distincts` : calcule le nombre de doublons restants apr√®s le nettoyage.
# - `print()` : affiche les r√©sultats sous forme de texte, permettant de v√©rifier le succ√®s du nettoyage des doublons.
# 
# Cela permet de confirmer que les doublons ont √©t√© correctement supprim√©s et de conna√Ætre le nombre exact de lignes uniques.
# 

# In[26]:


from pyspark.sql.functions import countDistinct

# Calcul du nombre total de lignes dans le DataFrame `df_Shift_clean`, apr√®s suppression des doublons.
total = df_Shift_clean.count()

# Calcul du nombre de lignes distinctes dans le DataFrame `df_Shift_clean`, en √©liminant les doublons.
distincts = df_Shift_clean.distinct().count()

# Affichage du nombre total de lignes, du nombre de lignes distinctes, et du nombre de doublons.
# Le nombre de doublons est calcul√© en faisant la diff√©rence entre le total et les lignes distinctes.
print(f"Total lignes : {total}, Lignes distinctes : {distincts}, Doublons : {total - distincts}")


# ### üíæ Sauvegarde des donn√©es dans une table Delta du Lakehouse `Inetum_Data`
# 
# Ce bloc de code permet de sauvegarder le DataFrame `df_Shift_clean` dans une table Delta appel√©e `Dim_Shift`, situ√©e dans le **Lakehouse** `Inetum_Data`. Cette action remplace les donn√©es existantes dans la table par les donn√©es trait√©es et nettoy√©es.
# 
# #### Explication des options utilis√©es :
# - `format("delta")` : sp√©cifie que la table doit √™tre sauvegard√©e au format Delta, qui offre des avantages de gestion de version, de transactions et de performance.
# - `mode("overwrite")` : remplace la table Delta existante. Si la table n'existe pas encore, elle sera cr√©√©e.
# - `option("overwriteSchema", "true")` : permet de mettre √† jour le sch√©ma de la table Delta pour qu'il corresponde √† la structure des donn√©es actuelles.
# - `saveAsTable("Inetum_Data.Dim_Shift")` : sauvegarde les donn√©es dans la table Delta `Dim_Shift` du **Lakehouse** `Inetum_Data`.
# 
# En enregistrant les donn√©es dans une table Delta, les donn√©es sont versionn√©es et optimis√©es pour des performances accrues, tout en restant disponibles pour des analyses futures dans l'architecture du Lakehouse.
# 

# In[27]:


# √âcriture du DataFrame nettoy√© (df_Shift_clean) dans une table Delta 'Dim_Shift' situ√©e dans le lakehouse 'Inetum_Data'.
# Cette op√©ration permet de sauvegarder les donn√©es trait√©es et nettoy√©es dans le format Delta, en rempla√ßant la table existante.
df_Shift_clean.write.format("delta") \
  .mode("overwrite") \
  .option("overwriteSchema", "true") \
  .saveAsTable("Inetum_Data.Dim_Shift")


# In[28]:


# Lecture de la table Delta 'Dim_Shift'.
df_result = spark.read.table("Inetum_Data.Dim_Shift")
# Affichage des 10 premi√®res lignes du DataFrame pour inspection.
display(df_result.head(10))


# ### üìã D√©finition du sch√©ma des donn√©es CSV
# 
# Afin d'assurer une lecture correcte et coh√©rente des donn√©es CSV dans Spark, nous d√©finissons manuellement le sch√©ma attendu du fichier. Cela permet d'√©viter que Spark inf√®re automatiquement les types de donn√©es, ce qui peut conduire √† des erreurs.
# 
# #### Sch√©ma d√©fini :
# - **"Num√©ro"** : type `StringType`, destin√© √† contenir des valeurs sous forme de cha√Æne de caract√®res.
# - **"Affect√© √†"** : type `StringType`, √©galement pour des valeurs sous forme de cha√Æne de caract√®res.
# 
# Cette √©tape permet d'assurer que les donn√©es seront lues dans le format attendu, en √©vitant des erreurs d'interpr√©tation.
# 

# In[29]:


#On d√©finit le sh√©ma du fichier csv pour √©viter que Spark inf√©re automatiquement le type de colonnes 

CollabSchema = StructType([
    StructField("Num√©ro", StringType()),
    StructField("Affect√© √†", StringType()),
])


# In[30]:


# Chargement des fichiers depuis le dossier bronze du lakehouse
df_Collab = spark.read.format("csv").option("header", "true").schema(CollabSchema).load("Files/bronze/dim_affecte.csv")


# In[31]:


# Affichage du sch√©ma du DataFrame `df_Collab` afin de v√©rifier la structure des donn√©es.
df_Collab.printSchema()


# In[32]:


# Affichage des 10 premi√®res lignes
df_Collab.show(10, truncate=False)


# ### üõ†Ô∏è Cr√©ation de la table Delta `Dim_Collaborateur`
# 
# Ce bloc de code permet de cr√©er la table Delta `Dim_Collaborateur` dans le **Lakehouse** `Inetum_Data`, en s'assurant que la table soit cr√©√©e uniquement si elle n'existe pas d√©j√†. Deux colonnes sont ajout√©es : `Numero` et `Affecte`, toutes deux de type `String`.
# 
# #### Explication des √©tapes :
# 1. `createIfNotExists(spark)` : cr√©e la table Delta si elle n'existe pas encore dans le lakehouse.
# 2. `.tableName("Inetum_Data.Dim_Collaborateur")` : d√©finit le nom de la table.
# 3. `.addColumn("Numero", StringType())` : ajoute la colonne `Numero` avec un type de donn√©es `String`.
# 4. `.addColumn("Affecte", StringType())` : ajoute la colonne `Affecte` avec un type de donn√©es `String`.
# 5. `.execute()` : ex√©cute la cr√©ation de la table avec les colonnes sp√©cifi√©es.
# 
# Cela garantit que la table est pr√™te √† √™tre utilis√©e pour des op√©rations de lecture ou d'√©criture dans le Lakehouse.
# 

# In[33]:


# Cr√©ation d'une table Delta 'Dim_Collaborateur' dans le lakehouse 'Inetum_Data' si elle n'existe pas encore.
    
DeltaTable.createIfNotExists(spark) \
     .tableName("Inetum_Data.Dim_Collaborateur") \
     .addColumn("Numero", StringType()) \
     .addColumn("Affecte", StringType()) \
     .execute() 


# ### üîÑ Mise √† jour ou insertion des donn√©es dans la table Delta `Dim_Collaborateur`
# 
# Ce bloc de code r√©alise un **MERGE** entre le DataFrame `df_Collab` (apr√®s renaming des colonnes) et la table Delta `Dim_Collaborateur` situ√©e dans le **Lakehouse** `Inetum_Data`. Le but est de mettre √† jour les lignes existantes ou d'ins√©rer de nouvelles lignes si aucune correspondance n'est trouv√©e.
# 
# #### √âtapes du processus :
# 1. **Renommage des colonnes** : les colonnes du DataFrame `df_Collab` sont renomm√©es pour correspondre aux noms des colonnes de la table Delta (ex. : "Num√©ro" devient "Numero").
# 2. **R√©cup√©ration de la table Delta** : la table Delta `Dim_Collaborateur` est r√©cup√©r√©e √† l'aide de la m√©thode `forName()`.
# 3. **Mise √† jour ou insertion avec MERGE** :
#    - Si une ligne correspondant √† `Numero` et `Affecte` existe dans la table Delta, elle est mise √† jour.
#    - Si aucune ligne correspondante n'est trouv√©e, une nouvelle ligne est ins√©r√©e dans la table.
# 4. **Ex√©cution de la commande MERGE** : les modifications sont appliqu√©es avec la m√©thode `execute()`.
# 
# Cette op√©ration permet de maintenir les donn√©es dans la table Delta √† jour, tout en √©vitant les doublons et en garantissant une gestion efficace des donn√©es.
# 

# In[34]:


from pyspark.sql.functions import col
from delta.tables import *

# Renommer les colonnes du DataFrame pour correspondre √† la table Delta
dfUpdates = df_Collab \
    .withColumnRenamed("Num√©ro", "Numero") \
    .withColumnRenamed("Affect√© √†", "Affecte")

# R√©cup√©rer la table Delta
deltaTable = DeltaTable.forName(spark, "Inetum_Data.Dim_Collaborateur")

# MERGE pour mettre √† jour ou ins√©rer les donn√©es
deltaTable.alias("silver") \
  .merge(
    dfUpdates.alias("updates"),
    """
    silver.Numero = updates.Numero
    AND silver.Affecte = updates.Affecte
    """
  ) \
  .whenNotMatchedInsert(values =
    {
      "Numero": "updates.Numero",
      "Affecte": "updates.Affecte"
    }
  ) \
  .execute()


# In[35]:


# Lecture de la table Delta 'dim_collaborateur'.
df_Shift = spark.read.table("Inetum_Data.dim_collaborateur")
# Affichage des 10 premi√®res lignes du DataFrame pour inspection.
display(df_Shift.head(10))


# ## üìä V√©rification de la qualit√© des donn√©es
# Dans cette section, nous v√©rifions deux aspects importants de la qualit√© des donn√©es dans notre DataFrame :
# 
# 1. **Comptage des valeurs nulles** : Nous comptons les valeurs nulles dans chaque colonne afin d'identifier les colonnes potentiellement incompl√®tes ou mal renseign√©es. Cela permet de prendre des mesures correctives, telles que l'imputation de donn√©es ou la suppression de colonnes probl√©matiques.
# 
# 2. **D√©tection des doublons** : Nous v√©rifions si des doublons existent dans le DataFrame en comparant le nombre total de lignes avec le nombre de lignes distinctes. Cette √©tape permet de garantir que les donn√©es ne sont pas redondantes, ce qui est essentiel pour des analyses pr√©cises et fiables.

# In[36]:


from pyspark.sql.functions import col, when, sum

# Compter les valeurs nulles par colonne dans df_Collab
null_counts = df_Collab.select([ 
    (when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df_Collab.columns
])

# Effectuer l'agr√©gation pour obtenir le nombre de valeurs nulles
null_counts_agg = null_counts.agg(*[sum(col(c)).alias(c) for c in df_Collab.columns])

# Afficher le nombre de valeurs nulles par colonne
null_counts_agg.show()


# ### üìä Calcul du nombre de doublons et de lignes distinctes
# 
# Ce bloc de code permet de calculer et d'afficher le nombre total de lignes, de lignes distinctes et de doublons dans le DataFrame `df_Collab`.
# 
# 
# #### Interpr√©tation :
# - **Total lignes** : Le nombre total de lignes dans le DataFrame est de 18 414.
# - **Lignes distinctes** : Le nombre de lignes distinctes apr√®s suppression des doublons est √©galement de 18 414.
# - **Doublons** : Aucun doublon n'a √©t√© trouv√© dans les donn√©es, ce qui signifie que toutes les lignes sont uniques.
# 

# In[37]:


from pyspark.sql.functions import countDistinct
# Identification des doublons dans le DataFrame `df_Collab`
total = df_Collab.count()
distincts = df_Collab.distinct().count()
print(f"Total lignes : {total}, Lignes distinctes : {distincts}, Doublons : {total - distincts}")

