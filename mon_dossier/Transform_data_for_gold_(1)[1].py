#!/usr/bin/env python
# coding: utf-8

# ## Transform data for gold
# 
# New notebook

# ### üìä Transformation des donn√©es de la couche Silver √† la couche Gold
# 
# #### Objectif :
# Ce processus consiste √† effectuer la transformation des donn√©es depuis la **couche Silver** vers la **couche Gold** dans le Lakehouse. La couche Silver contient des donn√©es nettoy√©es et enrichies, tandis que la couche Gold repr√©sente les donn√©es finales pr√™tes pour l'analyse et la visualisation. 
# 
# Dans cette √©tape, nous mettons en place le **sch√©ma en √©toile** en cr√©ant les **tables de dimension** et la **table de fait** qui alimenteront nos rapports analytiques.
# 
# #### √âtapes de la transformation
# 
# 1. **Chargement des donn√©es depuis la couche Silver :**
#    Les donn√©es sont lues depuis la table `Inetum_Data.supervision_silver`, qui contient les informations de niveau interm√©diaire, c'est-√†-dire des donn√©es nettoy√©es mais non encore totalement agr√©g√©es ou enrichies.

# In[31]:


# Chargement des donn√©es depuis la couche Silver
df = spark.read.table("Inetum_Data.supervision_silver")


# In[32]:


# Chargement des donn√©es depuis la couche Silver
df = spark.read.table("Inetum_Data.supervision_silver")


# In[33]:


# Affiche les 10 premi√®res lignes du DataFrame pour une inspection rapide
display(df.head(10))


# ### üõ†Ô∏è Cr√©ation de la table de dimension `Dim_Date_Ouverture`
# 
# Dans cette √©tape, nous cr√©ons une nouvelle table de dimension `Dim_Date_Ouverture` dans le lakehouse. Cette table sera utilis√©e pour stocker des informations sur les dates d'ouverture des tickets dans une structure dimensionnelle.
# 
# #### Processus de cr√©ation :
# 
# 1. **V√©rification de l'existence de la table** : Nous utilisons la m√©thode `createIfNotExists` pour garantir que la table est cr√©√©e uniquement si elle n'existe pas d√©j√†.
# 2. **D√©finition du sch√©ma** : La table contient plusieurs colonnes pour d√©tailler les informations relatives √† la date et √† l'heure d'ouverture des tickets :
#    - `Ouvert` : Le timestamp de l'ouverture du ticket.
#    - `Jour`, `Mois`, `Annee`, `Heure`, `Minute`, `Seconde` : Ces colonnes permettent de d√©composer la date d'ouverture pour une analyse plus pr√©cise.
# 3. **Ex√©cution de la cr√©ation** : La m√©thode `execute()` permet de finaliser la cr√©ation de la table.
# 
# Cette structure nous permettra de faire des jointures avec d'autres tables, comme les faits relatifs aux tickets, en utilisant des cl√©s dimensionnelles bas√©es sur la date et l'heure d'ouverture.
# 

# In[34]:


from pyspark.sql.types import *
from delta.tables import *

# Cr√©ation de la table de dimension Dim_Date_Ouverture
DeltaTable.createIfNotExists(spark) \
    .tableName("Inetum_Data.Dim_Date_Ouverture") \
    .addColumn("Ouvert", TimestampType()) \
    .addColumn("Jour", IntegerType()) \
    .addColumn("Mois", IntegerType()) \
    .addColumn("Annee", IntegerType()) \
    .addColumn("Heure", IntegerType()) \
    .addColumn("Minute", IntegerType()) \
    .addColumn("Seconde", IntegerType()) \
    .execute()


# ### üìÖ Cr√©ation et transformation de la table `Dim_Date_Ouverture`
# 
# Dans cette √©tape, nous pr√©parons les donn√©es pour cr√©er une table de dimension `Dim_Date_Ouverture`. Cette table sera utilis√©e pour stocker les informations d√©taill√©es sur la date et l'heure d'ouverture des tickets, et nous la pr√©parons √† l'aide de transformations sp√©cifiques sur la colonne `Ouvert` (timestamp).
# 
# #### Processus de transformation :
# 
# 1. **Suppression des doublons** : Nous utilisons la m√©thode `dropDuplicates` pour √©liminer les doublons dans la colonne `Ouvert`. Cela garantit qu'il n'y ait qu'une seule entr√©e par date d'ouverture.
# 2. **Cr√©ation de nouvelles colonnes** : Nous extrayons plusieurs √©l√©ments temporels de la colonne `Ouvert` :
#    - `Jour` : Le jour du mois de l'ouverture (`dayofmonth`).
#    - `Mois` : Le mois de l'ouverture (`month`).
#    - `Annee` : L'ann√©e de l'ouverture (`year`).
#    - `Heure`, `Minute`, `Seconde` : L'heure, la minute, et la seconde de l'ouverture, respectivement.
# 3. **Tri des r√©sultats** : Le DataFrame est ensuite tri√© par la colonne `Ouvert` pour assurer une organisation chronologique des donn√©es.
# 
# #### Aper√ßu des donn√©es :
# 
# Le code utilise la m√©thode `show(10)` pour afficher les 10 premi√®res lignes du DataFrame `dfdimDate_Ouverture`, ce qui permet de pr√©visualiser les r√©sultats et v√©rifier que les transformations ont √©t√© appliqu√©es correctement.
# 
# Cette table de dimension `Dim_Date_Ouverture` sera utilis√©e dans le mod√®le de donn√©es en √©toile, o√π elle sera li√©e √† une table de faits.
# 

# In[35]:


from pyspark.sql.functions import col, dayofmonth, month, year, hour, minute, second
from pyspark.sql import SparkSession
from delta.tables import *


# Cr√©er le DataFrame pour Dim_Date_Ouverture avec les transformations
dfdimDate_Ouverture = df.dropDuplicates(["Ouvert"]).select(
    col("Ouvert"),
    dayofmonth("Ouvert").alias("Jour"),
    month("Ouvert").alias("Mois"),
    year("Ouvert").alias("Annee"),
    hour("Ouvert").alias("Heure"),
    minute("Ouvert").alias("Minute"),
    second("Ouvert").alias("Seconde")
).orderBy("Ouvert")

# Afficher les 10 premi√®res lignes de la table charg√©e
dfdimDate_Ouverture.show(10)


# ### üîÑ Mise √† jour ou insertion dans la table Delta `dim_date_ouverture`
# 
# Dans cette √©tape, nous effectuons une op√©ration de **merge** entre le DataFrame `dfdimDate_Ouverture` (contenant les nouvelles donn√©es) et la table Delta `dim_date_ouverture` (qui contient les donn√©es existantes). Cette op√©ration est effectu√©e dans deux sc√©narios :
# 
# 1. **Mise √† jour des lignes existantes** : Si une ligne dans la table `dim_date_ouverture` correspond √† une ligne du DataFrame sur la base de la colonne `Ouvert`, les colonnes `Jour`, `Mois`, `Annee`, `Heure`, `Minute`, et `Seconde` de la table Delta sont mises √† jour avec les nouvelles valeurs du DataFrame.
#    
# 2. **Insertion de nouvelles lignes** : Si une ligne n'existe pas d√©j√† dans la table `dim_date_ouverture` (bas√©e sur la colonne `Ouvert`), de nouvelles lignes sont ins√©r√©es dans la table avec les valeurs correspondantes du DataFrame.
# 
# L'op√©ration `merge` est un moyen efficace de maintenir la table de dimension √† jour avec de nouvelles donn√©es tout en √©vitant les duplications. Cela garantit que la table contient les informations les plus r√©centes et √©vite d'avoir √† r√©√©crire toutes les donn√©es.
# 
# #### Fonctionnement du code :
# 
# - La table Delta est r√©f√©renc√©e avec `DeltaTable.forPath()`.
# - Une op√©ration `merge` est ensuite effectu√©e entre la table existante (`gold`) et les nouvelles donn√©es (`updates`).
# - Le match est effectu√© sur la colonne `Ouvert` (date d'ouverture).
# - En cas de correspondance (`whenMatchedUpdate`), les colonnes de la table `dim_date_ouverture` sont mises √† jour.
# - En cas de non-correspondance (`whenNotMatchedInsert`), de nouvelles lignes sont ins√©r√©es.
# 
# Cette op√©ration garantit que la table de dimension reste synchronis√©e avec les donn√©es entrantes et est pr√™te √† √™tre utilis√©e dans le mod√®le en √©toile pour les jointures avec d'autres tables de faits.
# 

# In[36]:


from delta.tables import *

# R√©f√©rence √† la table Delta existante 
deltaTable = DeltaTable.forPath(spark, 'Tables/dim_date_ouverture')

# Pr√©paration des donn√©es √† ins√©rer   
dfUpdates = dfdimDate_Ouverture

# Op√©ration MERGE : mise √† jour si existant, sinon insertion
deltaTable.alias('gold') \
    .merge(
        dfUpdates.alias('updates'),
        'gold.Ouvert = updates.Ouvert'  # Matching on the 'Ouvert' timestamp column
    ) \
    .whenMatchedUpdate(set =
        {
            
            'gold.Jour': 'updates.Jour',
            'gold.Mois': 'updates.Mois',
            'gold.Annee': 'updates.Annee',
            'gold.Heure': 'updates.Heure',
            'gold.Minute': 'updates.Minute',
            'gold.Seconde': 'updates.Seconde'
        }
    ) \
    .whenNotMatchedInsert(values =
        {
            "Ouvert": "updates.Ouvert",
            "Jour": "updates.Jour",
            "Mois": "updates.Mois",
            "Annee": "updates.Annee",
            "Heure": "updates.Heure",
            "Minute": "updates.Minute",
            "Seconde": "updates.Seconde"
        }
    ) \
    .execute()


# In[37]:


# Charger la table Delta dans un DataFrame
df_loaded = spark.read.format("delta").table("Inetum_Data.Dim_Date_Ouverture")

# Afficher les 10 premi√®res lignes de la table charg√©e
df_loaded.show(10)


# ### üìÖ Cr√©ation de la table de dimension `Dim_Date_MAJ`
# 
# Dans cette √©tape, nous cr√©ons la table de dimension `Dim_Date_MAJ` dans le lakehouse en utilisant Delta Lake. Cette table sera utilis√©e pour stocker des informations sur la date et l'heure de la mise √† jour des tickets, ce qui est essentiel pour les analyses temporelles dans le cadre du traitement des tickets ou d'autres processus d√©cisionnels.
# 
# #### Structure de la table :
# La table `Dim_Date_MAJ` contient plusieurs colonnes, chacune repr√©sentant une unit√© de temps, afin de faciliter les agr√©gations par date ou par heure :
# - **Mis_a_jour** : La colonne principale de type `TimestampType()`, qui contient la date et l'heure de la mise √† jour.
# - **Jour** : Le jour du mois extrait de la colonne `Mis_a_jour` (type `IntegerType()`).
# - **Mois** : Le mois extrait de la colonne `Mis_a_jour` (type `IntegerType()`).
# - **Annee** : L'ann√©e extraite de la colonne `Mis_a_jour` (type `IntegerType()`).
# - **Heure** : L'heure extraite de la colonne `Mis_a_jour` (type `IntegerType()`).
# - **Minute** : La minute extraite de la colonne `Mis_a_jour` (type `IntegerType()`).
# - **Seconde** : La seconde extraite de la colonne `Mis_a_jour` (type `IntegerType()`).
# 
# #### Fonctionnement du code :
# - La m√©thode `createIfNotExists()` est utilis√©e pour cr√©er la table uniquement si elle n'existe pas d√©j√†.
# - Les colonnes de la table sont d√©finies √† l'aide de la m√©thode `addColumn()`, o√π chaque colonne est sp√©cifi√©e avec son nom et son type.
# - La table est ensuite ex√©cut√©e avec `.execute()`, ce qui la cr√©e dans le sch√©ma sp√©cifi√© (`Inetum_Data`).
# 
# Cette table pourra √™tre utilis√©e pour effectuer des jointures avec d'autres tables dans le cadre de l'analyse des mises √† jour des tickets ou d'autres processus d√©cisionnels dans notre mod√®le en √©toile.
# 

# In[38]:


from pyspark.sql.types import *
from delta.tables import *

# Cr√©ation de la table de dimension 
DeltaTable.createIfNotExists(spark) \
    .tableName("Inetum_Data.Dim_Date_MAJ") \
    .addColumn("Mis_a_jour", TimestampType()) \
    .addColumn("Jour", IntegerType()) \
    .addColumn("Mois", IntegerType()) \
    .addColumn("Annee", IntegerType()) \
    .addColumn("Heure", IntegerType()) \
    .addColumn("Minute", IntegerType()) \
    .addColumn("Seconde", IntegerType()) \
    .execute()


# ### üïí Transformation des donn√©es pour la table de dimension `Dim_Date_MAJ`
# 
# Dans cette √©tape, nous cr√©ons le DataFrame `dfdimDate_MAJ` en appliquant diverses transformations √† la colonne `Mis_a_jour` de notre DataFrame source. Cette table de dimension contient des informations sur la date et l'heure de mise √† jour des tickets.
# 
# #### Objectifs :
# 1. **Suppression des doublons** : Nous supprimons les doublons dans la colonne `Mis_a_jour` afin de ne conserver qu'une seule entr√©e pour chaque timestamp unique de mise √† jour.
# 2. **Extraction des composantes temporelles** : 
#    - Nous extrayons le jour, le mois, l'ann√©e, l'heure, la minute et la seconde de la colonne `Mis_a_jour` pour faciliter les analyses temporelles.
# 3. **S√©lection et transformation des colonnes** : 
#    - Nous ajoutons les colonnes `Jour`, `Mois`, `Annee`, `Heure`, `Minute`, et `Seconde` √† notre DataFrame √† partir de la colonne `Mis_a_jour`.
# 4. **Tri des donn√©es** : Les donn√©es sont tri√©es par la colonne `Mis_a_jour` pour faciliter l'organisation temporelle.
# 
# #### Affichage des premi√®res lignes :
# Nous utilisons la m√©thode `show(10)` pour afficher les 10 premi√®res lignes du DataFrame transform√© et v√©rifier que les transformations ont √©t√© effectu√©es correctement.
# 
# #### Utilisation future :
# Cette table pourra √™tre utilis√©e pour effectuer des jointures avec d'autres tables dans le cadre de l'analyse des mises √† jour des tickets ou d'autres processus d√©cisionnels dans notre mod√®le en √©toile.
# 
# 

# In[39]:


from pyspark.sql.functions import col, dayofmonth, month, year, hour, minute, second
from pyspark.sql import SparkSession
from delta.tables import *


# Cr√©er le DataFrame pour Dim_Date_Maj avec les transformations
dfdimDate_MAJ = df.dropDuplicates(["Mis_a_jour"]).select(
    col("Mis_a_jour"),
    dayofmonth("Mis_a_jour").alias("Jour"),
    month("Mis_a_jour").alias("Mois"),
    year("Mis_a_jour").alias("Annee"),
    hour("Mis_a_jour").alias("Heure"),
    minute("Mis_a_jour").alias("Minute"),
    second("Mis_a_jour").alias("Seconde")
).orderBy("Mis_a_jour")

# Afficher les 10 premi√®res lignes de la table charg√©e
dfdimDate_MAJ.show(10)


# ### üîÑ Fusion des donn√©es pour la table de dimension `Dim_Date_MAJ`
# 
# Dans cette √©tape, nous utilisons la commande **MERGE** de Delta Lake pour mettre √† jour ou ins√©rer des donn√©es dans la table de dimension `Dim_Date_MAJ`.
# 
# #### Objectifs :
# 1. **Mise √† jour de la table Delta** : 
#    - Nous mettons √† jour les enregistrements existants dans la table `Dim_Date_MAJ` si les dates de mise √† jour (`Mis_a_jour`) correspondent. Cela permet d'assurer que les valeurs des colonnes `Jour`, `Mois`, `Annee`, `Heure`, `Minute`, et `Seconde` sont √† jour.
#    
# 2. **Insertion de nouvelles donn√©es** :
#    - Si un enregistrement n'existe pas dans la table, il sera ins√©r√© avec les valeurs appropri√©es pour `Mis_a_jour`, `Jour`, `Mois`, `Annee`, `Heure`, `Minute`, et `Seconde`.
# 
# #### D√©tails de la fusion (MERGE) :
# - **Condition de correspondance** : Nous avons bas√© la correspondance sur la colonne `Mis_a_jour` afin de v√©rifier les lignes existantes dans la table.
# - **Mise √† jour des lignes existantes** : Si une ligne existe d√©j√† avec la m√™me valeur dans `Mis_a_jour`, les autres colonnes seront mises √† jour avec les valeurs correspondantes des nouvelles donn√©es.
# - **Insertion des nouvelles lignes** : Si aucune ligne correspondante n'est trouv√©e, une nouvelle ligne est ins√©r√©e dans la table avec les nouvelles valeurs.
# 
# #### R√©sultat :
# Cette op√©ration assure que la table `Dim_Date_MAJ` reste √† jour avec les derni√®res informations de mise √† jour et peut √™tre utilis√©e dans des jointures avec d'autres tables de faits pour l'analyse.
# 
# 

# In[40]:


from delta.tables import *

# R√©f√©rence √† la table Delta existante 
deltaTable = DeltaTable.forPath(spark, 'Tables/dim_date_maj')

# Pr√©paration des donn√©es √† ins√©rer   
dfUpdates = dfdimDate_MAJ

# Op√©ration MERGE : mise √† jour si existant, sinon insertion
deltaTable.alias('gold') \
    .merge(
        dfUpdates.alias('updates'),
        'gold.Mis_a_jour = updates.Mis_a_jour'  # Matching on the 'Ouvert' timestamp column
    ) \
    .whenMatchedUpdate(set =
        {
            
            'gold.Jour': 'updates.Jour',
            'gold.Mois': 'updates.Mois',
            'gold.Annee': 'updates.Annee',
            'gold.Heure': 'updates.Heure',
            'gold.Minute': 'updates.Minute',
            'gold.Seconde': 'updates.Seconde'
        }
    ) \
    .whenNotMatchedInsert(values =
        {
            "Mis_a_jour": "updates.Mis_a_jour",
            "Jour": "updates.Jour",
            "Mois": "updates.Mois",
            "Annee": "updates.Annee",
            "Heure": "updates.Heure",
            "Minute": "updates.Minute",
            "Seconde": "updates.Seconde"
        }
    ) \
    .execute()


# In[41]:


# Charger la table Delta dans un DataFrame
df_loaded = spark.read.format("delta").table("Inetum_Data.Dim_Date_MAJ")

# Afficher les 10 premi√®res lignes de la table charg√©e
df_loaded.show(10)


# ### üî® Cr√©ation de la table de dimension `Dim_Societe`
# 
# Dans cette √©tape, nous cr√©ons la table de dimension **Dim_Societe** dans notre **Lakehouse**. Cette table contiendra des informations sur les soci√©t√©s, telles que leur identifiant unique et le nom de la soci√©t√©.
# 
# #### Objectifs :
# 1. **Cr√©ation de la table** : 
#    - Si la table **Dim_Societe** n'existe pas d√©j√†, elle est cr√©√©e automatiquement.
#    
# 2. **Structure de la table** :
#    - **ID_Societe** : Identifiant unique de la soci√©t√©, de type **LongType**.
#    - **Societe** : Nom de la soci√©t√©, de type **StringType**.
#    
# Cette table sera utilis√©e pour stocker des informations sur les soci√©t√©s, ce qui permettra de les lier √† d'autres tables de faits lors des analyses d√©cisionnelles et de l'exploration des donn√©es.
# 
# #### D√©tails techniques :
# - **`DeltaTable.createIfNotExists()`** : Cette fonction v√©rifie si la table existe d√©j√† et la cr√©e si elle ne le fait pas. Elle assure √©galement que la structure de la table est d√©finie avec les bonnes colonnes et types de donn√©es.
# - **Colonnes** :
#    - `ID_Societe` : Utilis√© pour identifier de mani√®re unique chaque soci√©t√©.
#    - `Societe` : Stocke le nom de la soci√©t√©.
# 
# Une fois cette table cr√©√©e, elle pourra √™tre utilis√©e dans des processus de transformation des donn√©es, notamment pour lier des informations de soci√©t√©s avec des donn√©es de tickets ou d'autres entit√©s de notre mod√®le.
# 

# In[42]:


from pyspark.sql.types import *
from delta.tables import *

# Cr√©ation de la table de dimension Dim_Date_Ouverture
DeltaTable.createIfNotExists(spark) \
    .tableName("Inetum_Data.Dim_Societe") \
    .addColumn("ID_Societe", LongType()) \
    .addColumn("Societe", StringType()) \
    .execute()


# ### üîÑ Suppression des doublons pour la table de dimension `Dim_Societe`
# 
# Dans cette √©tape, nous nous concentrons sur la suppression des doublons dans la colonne **`Societe`** de notre DataFrame. L'objectif est de garantir qu'il n'y ait pas de valeurs dupliqu√©es avant d'utiliser ces donn√©es pour construire la table de dimension **`Dim_Societe`**. Nous utilisons la m√©thode **`dropDuplicates()`** pour supprimer les enregistrements redondants et nous nous assurons que chaque soci√©t√© est unique dans la table.

# In[43]:


from pyspark.sql.functions import col, dayofmonth, month, year, hour, minute, second
from pyspark.sql import SparkSession
from delta.tables import *
# Suppression des doublons sur la colonne 'Societe'
dfdimSociete= df.dropDuplicates(["Societe"]).select(col("Societe"))
# Affichage des premi√®res lignes du DataFrame
dfdimSociete.show(10)


# ### üîÑ Identification et ajout de nouvelles soci√©t√©s √† la table de dimension `Dim_Societe`
# 
# Ce code permet de traiter les soci√©t√©s existantes et d'ajouter les nouvelles soci√©t√©s qui ne figurent pas encore dans la table de dimension **`Dim_Societe`**. Voici les principales √©tapes effectu√©es :
# 
# 1. **Chargement de la table existante** : Nous commen√ßons par charger la table **`Dim_Societe`** existante √† partir de Delta Lake pour r√©cup√©rer les soci√©t√©s d√©j√† pr√©sentes.
# 
# 2. **R√©cup√©ration du dernier ID** : Nous r√©cup√©rons l'ID le plus √©lev√© dans la table existante **`Dim_Societe`**, afin d'ajouter de nouveaux identifiants √† la suite.
# 
# 3. **Extraction des soci√©t√©s uniques** : Nous extrayons les soci√©t√©s uniques √† partir du DataFrame initial **`df`**, en supprimant les doublons sur la colonne **`Societe`**.
# 
# 4. **Identification des nouvelles soci√©t√©s** : Nous comparons ces soci√©t√©s extraites avec celles d√©j√† pr√©sentes dans la table **`Dim_Societe`** pour identifier les soci√©t√©s qui n'existent pas encore.
# 
# 5. **Ajout d'un ID unique** : √Ä ces nouvelles soci√©t√©s, nous attribuons un identifiant **`ID_Societe`** unique, en l'incr√©mentant par rapport au dernier identifiant existant.
# 
# 6. **Affichage des r√©sultats** : Enfin, nous affichons les 10 premi√®res lignes de la table mise √† jour.
# 

# In[44]:


from pyspark.sql.functions import monotonically_increasing_id, col, coalesce, max, lit

# Charger la table existante Dim_Societe
dfdimSociete_temp = spark.read.table("Inetum_Data.Dim_Societe")

# R√©cup√©rer le dernier ID_Societe existant
MAXSocieteID = dfdimSociete_temp.select(coalesce(max(col("ID_Societe")), lit(0)).alias("MAXSocieteID")).first()[0]

# Extraire les soci√©t√©s uniques de df et les comparer avec la table existante
dfdimSociete_silver = df.dropDuplicates(["Societe"]).select(col("Societe"))

# Identifier les nouvelles soci√©t√©s qui n'existent pas encore dans Dim_Societe
dfdimSociete_gold = dfdimSociete_silver.join(dfdimSociete_temp, dfdimSociete_silver.Societe == dfdimSociete_temp.Societe, "left_anti")

# Ajouter un ID_Societe unique aux nouvelles soci√©t√©s
dfdimSociete_gold = dfdimSociete_gold.withColumn("ID_Societe", monotonically_increasing_id() + MAXSocieteID + 1)

# Afficher les 10 premi√®res lignes
dfdimSociete_gold.show(10)


# ### üîÑ Insertion des nouvelles soci√©t√©s dans la table `Dim_Societe`
# 
# Cette √©tape permet d'ajouter les nouvelles soci√©t√©s √† la table de dimension **`Dim_Societe`** en utilisant une op√©ration **MERGE** dans Delta Lake. Voici les √©tapes r√©alis√©es :
# 
# 1. **R√©f√©rence √† la table Delta existante** : Nous commen√ßons par acc√©der √† la table Delta existante **`Dim_Societe`** √† partir de son chemin sur le syst√®me de fichiers.
# 
# 2. **Pr√©paration des donn√©es √† ins√©rer** : Nous pr√©parons un DataFrame **`dfdimSociete_gold`** contenant les nouvelles soci√©t√©s √† ajouter, avec un identifiant unique **`ID_Societe`** pour chaque nouvelle soci√©t√©.
# 
# 3. **Op√©ration MERGE** : Nous utilisons la m√©thode **`merge()`** pour effectuer une mise √† jour ou une insertion selon que la soci√©t√© existe d√©j√† dans la table ou non. Si une soci√©t√© existe d√©j√†, elle ne sera pas modifi√©e (aucune mise √† jour dans cette op√©ration). Si la soci√©t√© n'existe pas, elle sera ins√©r√©e dans la table avec son identifiant et son nom.
# 
# 4. **Insertion des nouvelles soci√©t√©s** : Les soci√©t√©s qui n'existent pas encore dans la table seront ins√©r√©es avec les colonnes **`Societe`** et **`ID_Societe`**.
# 

# In[45]:


from delta.tables import *
#### Code de mise √† jour et insertion :

# R√©f√©rence √† la table Delta existante Dim_Societe
deltaTable = DeltaTable.forPath(spark, 'Tables/dim_societe')

# Pr√©paration des donn√©es √† ins√©rer   
dfUpdates = dfdimSociete_gold

# Op√©ration MERGE : mise √† jour si existant, sinon insertion
deltaTable.alias('gold') \
  .merge(
    dfUpdates.alias('updates'),
    'gold.Societe = updates.Societe'
  ) \
   .whenMatchedUpdate(set =
    {
          
    }
  ) \
 .whenNotMatchedInsert(values =
    {
      "Societe": "updates.Societe",
      "ID_Societe": "updates.ID_Societe"
    }
  ) \
  .execute()


# In[46]:


spark.read.table("Inetum_Data.dim_societe").show()


# ### üîÑ Cr√©ation de la table de dimension `Dim_Categorie`
# 
# Cette √©tape consiste √† cr√©er la table **`Dim_Categorie`**, une table de dimension utilis√©e pour stocker les informations relatives aux cat√©gories. Voici les op√©rations effectu√©es :
# 
# 1. **Cr√©ation de la table si elle n'existe pas d√©j√†** : Nous utilisons la m√©thode **`createIfNotExists()`** pour cr√©er la table **`Dim_Categorie`** dans le syst√®me Delta Lake si elle n'existe pas encore. Cette m√©thode garantit que la table sera cr√©√©e uniquement si elle est absente.
# 
# 2. **Ajout des colonnes** :
#    - **`ID_Categorie`** : Un identifiant unique pour chaque cat√©gorie, de type **Long**.
#    - **`Categorie`** : Le nom ou la description de la cat√©gorie, de type **String**.
# 
# 3. **Ex√©cution de la cr√©ation de la table** : Enfin, la m√©thode **`execute()`** permet de cr√©er la table avec les colonnes sp√©cifi√©es dans le sch√©ma.

# In[47]:


from pyspark.sql.types import *
from delta.tables import *

# Cr√©ation de la table de dimension Dim_Date_Categorie
DeltaTable.createIfNotExists(spark) \
    .tableName("Inetum_Data.Dim_Categorie") \
    .addColumn("ID_Categorie", LongType()) \
    .addColumn("Categorie", StringType()) \
    .execute()


# ### üîÑ Suppression des doublons pour la table de dimension `Dim_Categorie`
# 
# Dans cette √©tape, nous nous concentrons sur la suppression des doublons dans la colonne **`Categorie`** de notre DataFrame. L'objectif est de garantir qu'il n'y ait pas de valeurs dupliqu√©es avant d'utiliser ces donn√©es pour construire la table de dimension **`Dim_Categorie`**. Nous utilisons la m√©thode **`dropDuplicates()`** pour supprimer les enregistrements redondants et nous nous assurons que chaque soci√©t√© est unique dans la table.

# In[48]:


from pyspark.sql.functions import col, dayofmonth, month, year, hour, minute, second
from pyspark.sql import SparkSession
from delta.tables import *

# Suppression des doublons sur la colonne 'Categorie'
dfdimCategorie= df.dropDuplicates(["Categorie"]).select(col("Categorie"))
# Afficher les 10 premi√®res lignes
dfdimCategorie.show(10)


# ### üîÑ Transformation et pr√©paration des donn√©es pour la table de dimension `Dim_Categorie`
# 
# Dans cette √©tape, nous effectuons les transformations n√©cessaires pour pr√©parer les donn√©es destin√©es √† la table de dimension **`Dim_Categorie`**. Voici les principales op√©rations effectu√©es :
# 
# 1. **Chargement de la table existante** :
#    - Nous chargeons la table de dimension **`Dim_Categorie`** existante √† partir de Delta Lake afin de pouvoir la comparer avec les nouvelles donn√©es.
# 
# 2. **R√©cup√©ration du dernier `ID_Categorie`** :
#    - Nous r√©cup√©rons le dernier identifiant **`ID_Categorie`** existant dans la table **`Dim_Categorie`**. Cela nous permet de g√©n√©rer de nouveaux identifiants uniques pour les nouvelles cat√©gories.
# 
# 3. **Extraction des cat√©gories uniques** :
#    - Nous utilisons **`dropDuplicates()`** pour extraire les cat√©gories uniques de la DataFrame initiale **`df`**, puis nous s√©lectionnons uniquement la colonne **`Categorie`** pour la pr√©parer √† l'insertion dans la table de dimension.
# 
# 4. **Identification des nouvelles cat√©gories** :
#    - Nous effectuons une jointure **`left_anti`** pour identifier les cat√©gories pr√©sentes dans **`df`** mais pas encore dans la table **`Dim_Categorie`**. Cela permet de s'assurer que seules les nouvelles cat√©gories sont ins√©r√©es.
# 
# 5. **Attribution d'un ID unique** :
#    - Nous g√©n√©rons un nouvel **`ID_Categorie`** unique pour chaque nouvelle cat√©gorie en utilisant **`monotonically_increasing_id()`** et en l'ajustant avec l'ID maximum existant pour √©viter les conflits.
# 
# 6. **Affichage des donn√©es** :
#    - Enfin, nous affichons les 10 premi√®res lignes du DataFrame pour v√©rifier les r√©sultats avant l'insertion dans la table.
# 

# In[49]:


from pyspark.sql.functions import monotonically_increasing_id, col, coalesce, max, lit

# Charger la table existante Dim_Categorie
dfdimCategorie_temp = spark.read.table("Inetum_Data.Dim_Categorie")

# R√©cup√©rer le dernier ID_Categorie existant
MAXCategorieID = dfdimCategorie_temp.select(coalesce(max(col("ID_Categorie")), lit(0)).alias("MAXCategorieID")).first()[0]

# Extraire les Categories uniques de df et les comparer avec la table existante
dfdimCategorie_silver = df.dropDuplicates(["Categorie"]).select(col("Categorie"))

# Identifier les nouvelles Categories qui n'existent pas encore dans Dim_Categorie
dfdimCategorie_gold = dfdimCategorie_silver.join(dfdimCategorie_temp, dfdimCategorie_silver.Categorie == dfdimCategorie_temp.Categorie, "left_anti")

# Ajouter un ID_Categorie unique aux nouvelles Categories
dfdimCategorie_gold = dfdimCategorie_gold.withColumn("ID_Categorie", monotonically_increasing_id() + MAXCategorieID + 1)

# Afficher les 10 premi√®res lignes
dfdimCategorie_gold.show(10)


# ### üîÑ Insertion des nouvelles cat√©gories dans la table de dimension `Dim_Categorie`
# 
# Dans cette √©tape, nous effectuons l'insertion des nouvelles cat√©gories dans la table de dimension **`Dim_Categorie`** √† l'aide de la commande **`MERGE`** de Delta Lake. Voici les op√©rations effectu√©es :
# 
# 1. **Chargement de la table Delta existante** :
#    - Nous chargeons la table **`Dim_Categorie`** existante √† partir de Delta Lake pour pouvoir effectuer les mises √† jour ou insertions n√©cessaires.
# 
# 2. **Fusion des donn√©es** :
#    - Nous utilisons la m√©thode **`merge`** pour comparer les donn√©es existantes dans **`Dim_Categorie`** avec les nouvelles donn√©es dans **`dfdimCategorie_gold`**. Cela nous permet d'effectuer les mises √† jour ou les insertions de mani√®re efficace.
# 
# 3. **Mise √† jour des donn√©es existantes** :
#    - La clause **`whenMatchedUpdate`** est utilis√©e pour mettre √† jour les lignes existantes lorsque la cat√©gorie correspondante est trouv√©e. Cependant, dans ce cas particulier, aucune mise √† jour sp√©cifique n'est effectu√©e, car la structure de la table ne n√©cessite pas de modifications suppl√©mentaires.
# 
# 4. **Insertion des nouvelles cat√©gories** :
#    - La clause **`whenNotMatchedInsert`** est utilis√©e pour ins√©rer les nouvelles cat√©gories dans la table **`Dim_Categorie`**. Si une cat√©gorie n'existe pas encore dans la table, elle sera ajout√©e avec son identifiant unique (**`ID_Categorie`**).
# 

# In[50]:


from delta.tables import *
# Charger la table existante Dim_Categorie

deltaTable = DeltaTable.forPath(spark, 'Tables/dim_categorie')

# DataFrame contenant les mises √† jour
   
dfUpdates = dfdimCategorie_gold

# Ex√©cution du MERGE pour ins√©rer les nouvelles cat√©gories
   
deltaTable.alias('gold') \
  .merge(
    dfUpdates.alias('updates'),
    'gold.Categorie = updates.Categorie'
  ) \
   .whenMatchedUpdate(set =
    {
          
    }
  ) \
 .whenNotMatchedInsert(values =
    {
      "Categorie": "updates.Categorie",
      "ID_Categorie": "updates.ID_Categorie"
    }
  ) \
  .execute()


# In[51]:


spark.read.table("Inetum_Data.dim_categorie").show()


# ### üîÑ Cr√©ation de la table de dimension `Dim_Etat`
# 
# Dans cette √©tape, nous cr√©ons la table de dimension **`Dim_Etat`** dans Delta Lake. Cette table contiendra les informations relatives aux diff√©rents √©tats, avec un identifiant unique pour chaque √©tat. Voici les √©tapes principales :
# 
# 1. **V√©rification de l'existence de la table** :
#    - Nous utilisons la m√©thode **`createIfNotExists`** de Delta Lake pour v√©rifier si la table **`Dim_Etat`** existe d√©j√†. Si elle n'existe pas, elle est cr√©√©e.
# 
# 2. **Ajout des colonnes** :
#    - Nous d√©finissons deux colonnes principales pour cette table :
#      - **`ID_Etat`** : un identifiant unique pour chaque √©tat, de type **`LongType`**.
#      - **`Etat`** : le nom de l'√©tat, de type **`StringType`**.

# In[52]:


from pyspark.sql.types import *
from delta.tables import *

# Cr√©ation de la table de dimension Dim_Etat
DeltaTable.createIfNotExists(spark) \
    .tableName("Inetum_Data.Dim_Etat") \
    .addColumn("ID_Etat", LongType()) \
    .addColumn("Etat", StringType()) \
    .execute()


# ### üîÑ Suppression des doublons pour la table de dimension `Dim_Etat`
# 
# Dans cette √©tape, nous nous concentrons sur la suppression des doublons dans la colonne **`Etat`** de notre DataFrame. L'objectif est de garantir qu'il n'y ait pas de valeurs dupliqu√©es avant d'utiliser ces donn√©es pour construire la table de dimension **`Dim_Etat`**. Nous utilisons la m√©thode **`dropDuplicates()`** pour supprimer les enregistrements redondants et nous nous assurons que chaque soci√©t√© est unique dans la table.

# In[53]:


from pyspark.sql.functions import col, dayofmonth, month, year, hour, minute, second
from pyspark.sql import SparkSession
from delta.tables import *

# Suppression des doublons sur la colonne 'Etat'
dfdimEtat= df.dropDuplicates(["Etat"]).select(col("Etat"))
# Afficher les 10 premi√®res lignes
dfdimEtat.show(10)


# ### üîÑ Traitement des donn√©es pour la dimension `Dim_Etat`
# 
# Dans cette section, nous g√©n√©rons les donn√©es de la table de dimension **`Dim_Etat`** √† partir des donn√©es sources, tout en √©vitant les doublons et en attribuant un identifiant unique √† chaque nouvel √©tat. Voici les √©tapes principales :
# 
# 1. **Chargement de la table existante** :
#    - La table **`Dim_Etat`** est charg√©e depuis le catalogue Delta afin de v√©rifier les √©tats d√©j√† pr√©sents.
# 
# 2. **R√©cup√©ration du dernier identifiant** :
#    - On extrait le dernier identifiant **`ID_Etat`** existant pour continuer la num√©rotation sans doublons.
# 
# 3. **D√©tection des nouveaux √©tats** :
#    - On identifie les √©tats pr√©sents dans le DataFrame source mais absents de la table actuelle gr√¢ce √† une jointure en mode **left anti**.
# 
# 4. **Attribution des identifiants** :
#    - Pour chaque nouvel √©tat, un identifiant unique est g√©n√©r√© dynamiquement en s'appuyant sur la fonction **`monotonically_increasing_id()`**, en l'incr√©mentant √† partir du dernier ID existant.
# 
# 5. **Aper√ßu des donn√©es** :
#    - On affiche les 10 premi√®res lignes pour valider visuellement les transformations r√©alis√©es.
# 
# Ce processus assure une int√©gration continue et sans doublons des nouveaux √©tats dans la table de dimension.
# 

# In[54]:


from pyspark.sql.functions import monotonically_increasing_id, col, coalesce, max, lit

# Charger la table existante Dim_Etat
dfdimEtat_temp = spark.read.table("Inetum_Data.Dim_Etat")

# R√©cup√©rer le dernier ID_Etat existant
MAXEtatID = dfdimEtat_temp.select(coalesce(max(col("ID_Etat")), lit(0)).alias("MAXEtatID")).first()[0]

# Extraire les etats uniques de df et les comparer avec la table existante
dfdimEtat_silver = df.dropDuplicates(["Etat"]).select(col("Etat"))

# Identifier les nouvelles etats qui n'existent pas encore dans Dim_Etat
dfdimEtat_gold = dfdimEtat_silver.join(dfdimEtat_temp, dfdimEtat_silver.Etat == dfdimEtat_temp.Etat, "left_anti")

# Ajouter un ID_Etat unique aux nouvelles Etat
dfdimEtat_gold = dfdimEtat_gold.withColumn("ID_Etat", monotonically_increasing_id() + MAXEtatID + 1)

# Afficher les 10 premi√®res lignes
dfdimEtat_gold.show(10)


# ### üîÑ Insertion des nouvelles valeurs dans la table de dimension `Dim_Etat`
# 
# Dans cette section, nous mettons √† jour la table de dimension **`Dim_Etat`** avec les nouvelles valeurs identifi√©es pr√©c√©demment.
# 
# - Nous utilisons la m√©thode **`merge()`** de Delta Lake pour ins√©rer uniquement les nouveaux enregistrements dans la table Delta.
# - La condition de correspondance est bas√©e sur la colonne **`Etat`**, afin de v√©rifier si une valeur existe d√©j√† dans la table cible.
# - Les lignes **non pr√©sentes** dans la table (`whenNotMatchedInsert`) sont ins√©r√©es avec les colonnes **`Etat`** et **`ID_Etat`**.
# - Aucun traitement n'est effectu√© pour les lignes correspondantes d√©j√† existantes (`whenMatchedUpdate` vide), ce qui garantit que seules les nouvelles valeurs sont prises en compte.
# 
# Cette approche assure une alimentation incr√©mentale de la dimension **`Etat`**, tout en √©vitant les doublons.
# 

# In[55]:


from delta.tables import *
# Charger la table existante Dim_etat

deltaTable = DeltaTable.forPath(spark, 'Tables/dim_etat')

# DataFrame contenant les mises √† jour
  
dfUpdates = dfdimEtat_gold

# Ex√©cution du MERGE pour ins√©rer les nouvelles etats
    
deltaTable.alias('gold') \
  .merge(
    dfUpdates.alias('updates'),
    'gold.Etat = updates.Etat'
  ) \
   .whenMatchedUpdate(set =
    {
          
    }
  ) \
 .whenNotMatchedInsert(values =
    {
      "Etat": "updates.Etat",
      "ID_Etat": "updates.ID_Etat"
    }
  ) \
  .execute()


# In[56]:


spark.read.table("Inetum_Data.dim_etat").show()


# ### üóÇÔ∏è Cr√©ation de la table de dimension `Dim_Priorite`
# 
# Dans cette section, nous cr√©ons la table de dimension **`Dim_Priorite`** √† l'aide de la m√©thode **`createIfNotExists()`** de Delta Lake. Cette table contiendra les diff√©rentes valeurs de priorit√© pr√©sentes dans les tickets, ainsi qu'un identifiant unique associ√© √† chacune d'elles.
# 
# Les colonnes d√©finies sont :
# - `ID_Priorite` : identifiant unique de la priorit√© (type `Long`),
# - `Priorite` : libell√© de la priorit√© (type `String`).
# 
# Cette table s'inscrit dans la construction du sch√©ma en √©toile et pourra √™tre utilis√©e dans les analyses pour cat√©goriser les tickets selon leur niveau de priorit√©.
# 

# In[57]:


from pyspark.sql.types import *
from delta.tables import *

# Cr√©ation de la table de dimension Dim_Date_Ouverture
DeltaTable.createIfNotExists(spark) \
    .tableName("Inetum_Data.Dim_Priorite") \
    .addColumn("ID_Priorite", LongType()) \
    .addColumn("Priorite", StringType()) \
    .execute()


# ### üîÑ Suppression des doublons pour la table de dimension `Dim_Priorite`
# 
# Dans cette √©tape, nous nous concentrons sur la suppression des doublons dans la colonne **`Priorite`** de notre DataFrame. L'objectif est de garantir qu'il n'y ait pas de valeurs dupliqu√©es avant d'utiliser ces donn√©es pour construire la table de dimension **`Dim_Priorite`**. Nous utilisons la m√©thode **`dropDuplicates()`** pour supprimer les enregistrements redondants et nous nous assurons que chaque soci√©t√© est unique dans la table.

# In[58]:


from pyspark.sql.functions import col, dayofmonth, month, year, hour, minute, second
from pyspark.sql import SparkSession
from delta.tables import *
# Suppression des doublons sur la colonne 'Priorite'
dfdimPriorite= df.dropDuplicates(["Priorite"]).select(col("Priorite"))
# Afficher les 10 premi√®res lignes
dfdimPriorite.show(10)


# In[59]:


dfdimPriorite.show(10)


# ### üîÑ Transformation des donn√©es pour la table `Dim_Priorite`
# 
# Dans cette section, nous mettons √† jour la table de dimension **`Dim_Priorite`** en identifiant les nouvelles priorit√©s issues des donn√©es sources. Le processus se d√©roule comme suit :
# 
# 1. Chargement de la table existante `Dim_Priorite`.
# 2. R√©cup√©ration du dernier identifiant utilis√© (`ID_Priorite`).
# 3. Extraction des priorit√©s uniques depuis la source.
# 4. Identification des nouvelles priorit√©s absentes de la table actuelle.
# 5. Attribution d‚Äôun identifiant unique √† chaque nouvelle priorit√© √† l‚Äôaide de `monotonically_increasing_id`.
# 
# Ce traitement garantit que seules les nouvelles valeurs sont ajout√©es √† la table de dimension, √©vitant ainsi les doublons tout en maintenant l‚Äôint√©grit√© des identifiants.
# 

# In[60]:


from pyspark.sql.functions import monotonically_increasing_id, col, coalesce, max, lit

# Charger la table existante Dim_Priorite
dfdimPriorite_temp = spark.read.table("Inetum_Data.Dim_Priorite")

# R√©cup√©rer le dernier ID_Priorite existant
MAXPrioriteID = dfdimPriorite_temp.select(coalesce(max(col("ID_Priorite")), lit(0)).alias("MAXPrioriteID")).first()[0]

# Extraire les priorites uniques de df et les comparer avec la table existante
dfdimPriorite_silver = df.dropDuplicates(["Priorite"]).select(col("Priorite"))

# Identifier les nouvelles priorites qui n'existent pas encore dans Dim_Priorite
dfdimPriorite_gold = dfdimPriorite_silver.join(dfdimPriorite_temp, dfdimPriorite_silver.Priorite == dfdimPriorite_temp.Priorite, "left_anti")

# Ajouter un ID_Priorite unique aux nouvelles priorites
dfdimPriorite_gold = dfdimPriorite_gold.withColumn("ID_Priorite", monotonically_increasing_id() + MAXPrioriteID + 1)

# Afficher les 10 premi√®res lignes
dfdimPriorite_gold.show(10)


# ### üß© Mise √† jour incr√©mentale de la table de dimension `Dim_Priorite`
# 
# Dans cette section, nous mettons √† jour la table de dimension **`Dim_Priorite`** en utilisant l‚Äôinstruction **`merge()`** de Delta Lake. L‚Äôobjectif est d‚Äôins√©rer uniquement les nouvelles valeurs de priorit√© qui ne sont pas encore pr√©sentes dans la table existante :
# 
# - La condition de jointure se base sur la colonne `Priorite`.
# - Si une priorit√© existe d√©j√†, aucune mise √† jour n‚Äôest effectu√©e.
# - Si une priorit√© est absente, elle est ins√©r√©e avec un identifiant unique (`ID_Priorite`).
# 
# Ce m√©canisme garantit l‚Äôunicit√© des entr√©es dans la table de dimension.
# 

# In[61]:


from delta.tables import *

# Charger la table existante Dim_priorite

deltaTable = DeltaTable.forPath(spark, 'Tables/dim_priorite')

# DataFrame contenant les mises √† jour
 
dfUpdates = dfdimPriorite_gold

# Ex√©cution du MERGE pour ins√©rer les nouvelles Priorite

deltaTable.alias('gold') \
  .merge(
    dfUpdates.alias('updates'),
    'gold.Priorite = updates.Priorite'
  ) \
   .whenMatchedUpdate(set =
    {
          
    }
  ) \
 .whenNotMatchedInsert(values =
    {
      "Priorite": "updates.Priorite",
      "ID_Priorite": "updates.ID_Priorite"
    }
  ) \
  .execute()


# In[62]:


# Charger la table Delta dans un DataFrame
df_priorite=spark.read.table("Inetum_Data.dim_priorite")
# Afficher les 10 premi√®res lignes
df_priorite.show()


# In[63]:


from pyspark.sql.types import *
from delta.tables import *

# Cr√©ation de la table de dimension Dim_Date_Ouverture
DeltaTable.createIfNotExists(spark) \
    .tableName("Inetum_Data.Dim_Groupe_affectation") \
    .addColumn("ID_Groupe_affectation", LongType()) \
    .addColumn("Groupe_affectation", StringType()) \
    .execute()


# In[64]:


from pyspark.sql.functions import col, dayofmonth, month, year, hour, minute, second
from pyspark.sql import SparkSession
from delta.tables import *

dfdimGroupe_affectation= df.dropDuplicates(["Groupe_affectation"]).select(col("Groupe_affectation"))
dfdimGroupe_affectation.show(10)


# In[65]:


from pyspark.sql.functions import monotonically_increasing_id, col, coalesce, max, lit

# Charger la table existante Dim_Societe
dfdimGroupe_affectation_temp = spark.read.table("Inetum_Data.Dim_Groupe_affectation")

# R√©cup√©rer le dernier ID_Societe existant
MAXGroupe_affectationID = dfdimGroupe_affectation_temp.select(coalesce(max(col("ID_Groupe_affectation")), lit(0)).alias("MAXGroupe_affectationID")).first()[0]

# Extraire les soci√©t√©s uniques de df et les comparer avec la table existante
dfdimGroupe_affectation_silver = df.dropDuplicates(["Groupe_affectation"]).select(col("Groupe_affectation"))

# Identifier les nouvelles soci√©t√©s qui n'existent pas encore dans Dim_Societe
dfdimGroupe_affectation_gold = dfdimGroupe_affectation_silver.join(dfdimGroupe_affectation_temp, dfdimGroupe_affectation_silver.Groupe_affectation == dfdimGroupe_affectation_temp.Groupe_affectation, "left_anti")

# Ajouter un ID_Societe unique aux nouvelles soci√©t√©s
dfdimGroupe_affectation_gold = dfdimGroupe_affectation_gold.withColumn("ID_Groupe_affectation", monotonically_increasing_id() + MAXGroupe_affectationID + 1)

# Afficher les 10 premi√®res lignes
dfdimGroupe_affectation_gold.show(10)


# In[66]:


# Charger la table Delta existante
deltaTable = DeltaTable.forPath(spark, 'Tables/dim_groupe_affectation')

# Effectuer le merge des donn√©es
dfUpdates = dfdimGroupe_affectation_gold

deltaTable.alias('gold') \
  .merge(
    dfUpdates.alias('updates'),
    'gold.Groupe_affectation = updates.Groupe_affectation'
  ) \
  .whenMatchedUpdate(set =
    {
    }
  ) \
  .whenNotMatchedInsert(values =
    {
      "Groupe_affectation": "updates.Groupe_affectation",
      "ID_Groupe_affectation": "updates.ID_Groupe_affectation",
    }
  ) \
  .execute()


# In[67]:


df_check = spark.read.format("delta").table("Inetum_Data.dim_groupe_affectation")
df_check.show(10)


# ### üóÇÔ∏è Cr√©ation de la table de dimension `Dim_Groupe_aff`
# 
# Dans cette section, nous cr√©ons la table de dimension **`Dim_Groupe_aff`** √† l'aide de la m√©thode **`createIfNotExists()`** de Delta Lake. Cette table contiendra les diff√©rents groupes d'affectation et leur type, ainsi qu'un identifiant unique associ√© √† chacun d'eux.
# 
# Les colonnes d√©finies sont :
# - `ID_Groupe_affectation` : identifiant unique du groupe d'affectation (type `Long`),
# - `Groupe_affectation` : libell√© du groupe d'affectation (type `String`),
# - `Type_Groupe` : type du groupe d'affectation (type `String`).
# 
# Cette table s'inscrit dans la construction du sch√©ma en √©toile et pourra √™tre utilis√©e dans les analyses pour cat√©goriser les tickets en fonction de leur groupe d'affectation.
# 

# In[68]:


from pyspark.sql.types import *
from delta.tables import *

# Cr√©ation de la table de dimension Dim_Groupe_aff
DeltaTable.createIfNotExists(spark) \
    .tableName("Inetum_Data.Dim_Groupe_aff") \
    .addColumn("ID_Groupe_affectation", LongType()) \
    .addColumn("Groupe_affectation", StringType()) \
    .addColumn("Type_Groupe", StringType()) \
    .execute()


# ### üîÑ Suppression des doublons pour la table de dimension `Dim_Groupe_aff`
# 
# Dans cette √©tape, nous nous concentrons sur la suppression des doublons dans la colonne **`Groupe_affectation`** de notre DataFrame. L'objectif est de garantir qu'il n'y ait pas de valeurs dupliqu√©es avant d'utiliser ces donn√©es pour construire la table de dimension **`Dim_Groupe_aff`**. Nous utilisons la m√©thode **`dropDuplicates()`** pour supprimer les enregistrements redondants et nous nous assurons que chaque soci√©t√© est unique dans la table.

# In[69]:


from pyspark.sql.functions import col, dayofmonth, month, year, hour, minute, second
from pyspark.sql import SparkSession
from delta.tables import *
# Suppression des doublons sur la colonne 'Priorite'
dfdimGroupe_affectation= df.dropDuplicates(["Groupe_affectation"]).select(col("Groupe_affectation"))
# Afficher les 10 premi√®res lignes de la table charg√©e
dfdimGroupe_affectation.show(10)


# ### üßë‚Äçüíº D√©finition et classification des groupes d'affectation
# 
# Dans cette section, nous proc√©dons √† la classification des groupes d'affectation en fonction de leur domaine d'activit√© : **Exploitation**, **Supervision**, ou **Autre**.
# 
# #### √âtapes suivies :
# 1. **D√©finition des groupes** :
#    - Nous avons d'abord d√©fini deux listes de groupes :
#      - **`groupes_exploitation`** : comprend tous les groupes relatifs √† l'exploitation des syst√®mes et applications.
#      - **`groupes_supervision`** : comprend les groupes associ√©s √† la supervision des op√©rations.
# 
# 2. **Suppression des doublons** :
#    - Nous avons utilis√© la m√©thode **`dropDuplicates()`** pour √©liminer les doublons dans la colonne **`Groupe_affectation`**, ne conservant qu'une occurrence unique de chaque groupe.
# 
# 3. **Ajout de la colonne `Type_Groupe`** :
#    - Une nouvelle colonne **`Type_Groupe`** a √©t√© ajout√©e √† notre DataFrame, classant chaque groupe en fonction de sa pr√©sence dans les listes de groupes d√©finies. La classification est effectu√©e avec la m√©thode **`when()`**, en attribuant la valeur :
#      - **`Exploitation`** pour les groupes pr√©sents dans **`groupes_exploitation`**,
#      - **`Supervision`** pour les groupes pr√©sents dans **`groupes_supervision`**,
#      - **`Autre`** pour les groupes qui ne figurent dans aucune des deux listes.
# 
# 

# In[70]:


from pyspark.sql.functions import col, when, lit

# D√©finition des groupes
groupes_exploitation = [
    "ASDF - Exploitation Applicative",
    "ASDF - Exploitation T√©l√©phonie",
    "IO - S√©curit√© Op√©rationnelle",
    "ASDF - Syst√®me, Stockage, SGBD",
    "ASDF - R√©seau et s√©curit√©",
    "ASDF - Cloud",
    "IO - Support TOURS",
    "IO - Run PEGA/KOFAX ASP",
    "IO - Marine Support BackOffice",
    "IO - Data Factory",
    "IO - Distribution Legacy",
    "IO - Sant√© Pr√©voyance Collective",
    "IO - Sant√© Pr√©voyance individuelle",
    "MOI APRIL Marine",
    "MOI APRIL International Care France",
    "TMA - BULL",
    "DD - MarketPlace",
    "DD - Distribution Digitale",
    "IO - Etudes - IARD WAF",
    "ASDF - Environnement Utilisateurs",
    "IO - AICF_DEV"
]

groupes_supervision = [
    "ASDF - Supervision SAM",
    "SCIS TOOLING SUP/HYP",
    "SCIS M&C MA1",
    "SCIS M&C TL",
    "SCIS RUNOPS MA"
]

# Nettoyage des doublons et ajout de la colonne Type_Groupe
dfdimGroupe_affectation = df.dropDuplicates(["Groupe_affectation"]).select(col("Groupe_affectation"))

dfdimGroupe_affectation = dfdimGroupe_affectation.withColumn(
    "Type_Groupe",
    when(col("Groupe_affectation").isin(groupes_exploitation), lit("Exploitation"))
    .when(col("Groupe_affectation").isin(groupes_supervision), lit("Supervision"))
    .otherwise(lit("Autre"))
)

dfdimGroupe_affectation.show(10)


# ### üîÑ Identification et ajout des nouvelles lignes pour la table de dimension `Dim_Groupe_aff`
# 
# Dans cette √©tape, nous identifions les nouvelles lignes √† ins√©rer dans la table de dimension **`Dim_Groupe_aff`** en utilisant une jointure avec la table existante et en g√©n√©rant un nouvel identifiant unique pour chaque entr√©e.
# 
# #### √âtapes suivies :
# 1. **Chargement de la table existante** :
#    - La table de dimension **`Dim_Groupe_aff`** est charg√©e √† partir de Delta Lake dans un DataFrame temporaire (**`dfdimGroupe_affectation_temp`**).
# 
# 2. **R√©cup√©ration du dernier identifiant existant** :
#    - Nous r√©cup√©rons le dernier **`ID_Groupe_affectation`** en utilisant **`max()`** et **`coalesce()`** pour g√©rer les valeurs nulles. Cette valeur est utilis√©e pour g√©n√©rer de nouveaux identifiants uniques.
# 
# 3. **Identification des nouvelles lignes** :
#    - Une jointure de type **`left_anti`** est effectu√©e entre le DataFrame des groupes d'affectation (**`dfdimGroupe_affectation`**) et la table existante **`Dim_Groupe_aff`** afin d'identifier les nouvelles lignes (les groupes qui ne figurent pas encore dans la table).
# 
# 4. **Ajout du nouvel identifiant unique** :
#    - Un identifiant unique est g√©n√©r√© pour chaque nouvelle ligne en utilisant la fonction **`monotonically_increasing_id()`** et en ajoutant le dernier **`ID_Groupe_affectation`** r√©cup√©r√©.
# 

# In[71]:


from pyspark.sql.functions import monotonically_increasing_id, coalesce, max

# Charger la table existante
dfdimGroupe_affectation_temp = spark.read.table("Inetum_Data.Dim_Groupe_aff")

# R√©cup√©rer le dernier ID
MAXGroupe_affectationID = dfdimGroupe_affectation_temp.select(
    coalesce(max(col("ID_Groupe_affectation")), lit(0)).alias("MAXGroupe_affectationID")
).first()[0]

# Identifier les nouvelles lignes
dfdimGroupe_affectation_gold = dfdimGroupe_affectation.join(
    dfdimGroupe_affectation_temp,
    "Groupe_affectation",
    "left_anti"
)

# Ajouter un nouvel ID
dfdimGroupe_affectation_gold = dfdimGroupe_affectation_gold.withColumn(
    "ID_Groupe_affectation",
    monotonically_increasing_id() + MAXGroupe_affectationID + 1
)

dfdimGroupe_affectation_gold.show(10)


# ### üîÑ Mise √† jour et insertion des donn√©es dans la table `Dim_Groupe_aff`
# 
# Dans cette √©tape, nous effectuons un **MERGE** sur la table de dimension **`Dim_Groupe_aff`** pour mettre √† jour les lignes existantes et ins√©rer les nouvelles lignes identifi√©es dans l'√©tape pr√©c√©dente.
# 
# #### √âtapes suivies :
# 1. **Chargement de la table Delta** :
#    - La table **`Dim_Groupe_aff`** est charg√©e depuis Delta Lake en utilisant la m√©thode **`forPath()`** de DeltaTable.
# 
# 2. **Ex√©cution du MERGE** :
#    - Une jointure est effectu√©e entre les donn√©es existantes dans la table **`gold`** et les donn√©es mises √† jour (**`updates`**) √† partir de **`dfdimGroupe_affectation_gold`**. 
#    - Si une correspondance est trouv√©e sur la colonne **`Groupe_affectation`**, la colonne **`Type_Groupe`** est mise √† jour avec la nouvelle valeur.
#    - Si aucune correspondance n'est trouv√©e (c'est-√†-dire si le groupe d'affectation est nouveau), une nouvelle ligne est ins√©r√©e avec les valeurs des colonnes **`Groupe_affectation`**, **`ID_Groupe_affectation`**, et **`Type_Groupe`**.
# 

# In[72]:


from delta.tables import *

# Charger la table Delta
deltaTable = DeltaTable.forPath(spark, 'Tables/dim_groupe_aff')

# Effectuer le merge
deltaTable.alias('gold') \
  .merge(
    dfdimGroupe_affectation_gold.alias('updates'),
    'gold.Groupe_affectation = updates.Groupe_affectation'
  ) \
  .whenMatchedUpdate(set =
    {
        "Type_Groupe": "updates.Type_Groupe"
    }
  ) \
  .whenNotMatchedInsert(values =
    {
        "Groupe_affectation": "updates.Groupe_affectation",
        "ID_Groupe_affectation": "updates.ID_Groupe_affectation",
        "Type_Groupe": "updates.Type_Groupe"
    }
  ) \
  .execute()


# In[73]:


# Lecture de la table Delta 'Dim_Groupe_aff' depuis la base de donn√©es et affichage de son contenu
df_groupe = spark.read.table("Inetum_Data.Dim_Groupe_aff").show()


# ### üóÇÔ∏è Cr√©ation de la table de dimension `Dim_Ouvert_par`
# 
# Dans cette section, nous cr√©ons la table de dimension **`Dim_Ouvert_par`** √† l'aide de la m√©thode **`createIfNotExists()`** de Delta Lake. Cette table contiendra les diff√©rentes valeurs de la colonne **`Ouvert_par`**, qui repr√©sente l'entit√© ou la personne ayant ouvert le ticket, ainsi qu'un identifiant unique associ√© √† chacune d'elles.
# 
# Les colonnes d√©finies sont :
# - `ID_Ouvert_par` : identifiant unique de l'entit√© ayant ouvert le ticket (type `Long`),
# - `Ouvert_par` : libell√© de l'entit√© ayant ouvert le ticket (type `String`).
# 
# Cette table sera utilis√©e dans le cadre du traitement des tickets pour identifier l'entit√© responsable de l'ouverture de chaque ticket.
# 

# In[74]:


from pyspark.sql.types import *
from delta.tables import *

# Cr√©ation de la table de dimension Dim_Date_Ouverture
DeltaTable.createIfNotExists(spark) \
    .tableName("Inetum_Data.Dim_Ouvert_par") \
    .addColumn("ID_Ouvert_par", LongType()) \
    .addColumn("Ouvert_par", StringType()) \
    .execute()


# ### üîÑ Suppression des doublons pour la table de dimension `Dim_Ouvert_par`
# 
# Dans cette √©tape, nous nous concentrons sur la suppression des doublons dans la colonne **`Ouvert_par`** de notre DataFrame. L'objectif est de garantir qu'il n'y ait pas de valeurs dupliqu√©es avant d'utiliser ces donn√©es pour construire la table de dimension **`Dim_Ouvert_par`**. Nous utilisons la m√©thode **`dropDuplicates()`** pour supprimer les enregistrements redondants et nous nous assurons que chaque soci√©t√© est unique dans la table.

# In[75]:


from pyspark.sql.functions import col, dayofmonth, month, year, hour, minute, second
from pyspark.sql import SparkSession
from delta.tables import *
# Suppression des doublons sur la colonne 'Priorite'
dfdimOuvert_par= df.dropDuplicates(["Ouvert_par"]).select(col("Ouvert_par"))
# Afficher les 10 premi√®res lignes de la table charg√©e
dfdimOuvert_par.show(10)


# ### üõ†Ô∏è Transformation des donn√©es et ajout de l'ID dans la table `Dim_Ouvert_par`
# 
# Dans cette section, nous pr√©parons les donn√©es pour la table de dimension **`Dim_Ouvert_par`** en effectuant plusieurs √©tapes de transformation et de manipulation :
# 
# 1. **Chargement de la table existante** :
#    - Nous chargeons la table **`Dim_Ouvert_par`** existante √† partir du DataFrame **`dfdimOuvert_par_temp`**.
# 
# 2. **R√©cup√©ration du dernier identifiant** :
#    - Nous r√©cup√©rons le dernier identifiant `ID_Ouvert_par` de la table existante, en prenant la valeur maximale ou en utilisant 0 si la table est vide.
# 
# 3. **Suppression des doublons** :
#    - Nous supprimons les doublons dans la colonne **`Ouvert_par`** et s√©lectionnons les soci√©t√©s uniques, ce qui permet de garder uniquement les entit√©s distinctes.
# 
# 4. **Identification des nouvelles valeurs** :
#    - Nous identifions les entit√©s **`Ouvert_par`** qui n'existent pas encore dans la table **`Dim_Ouvert_par`** en utilisant une jointure **`left_anti`**.
# 
# 5. **Ajout d'un identifiant unique** :
#    - Un nouvel identifiant unique **`ID_Ouvert_par`** est g√©n√©r√© pour les nouvelles valeurs **`Ouvert_par`** √† l'aide de la fonction **`monotonically_increasing_id()`**.
# 
# 6. **Affichage des premi√®res lignes** :
#    - Nous affichons les 10 premi√®res lignes du DataFrame r√©sultant pour v√©rifier les donn√©es apr√®s la transformation.
# 
# Le r√©sultat de cette √©tape est un DataFrame **`dfdimOuvert_par_gold`** contenant les nouvelles entit√©s **`Ouvert_par`** avec des identifiants uniques, pr√™tes √† √™tre ins√©r√©es ou mises √† jour dans la table de dimension **`Dim_Ouvert_par`**.
# 

# In[76]:


from pyspark.sql.functions import monotonically_increasing_id, col, coalesce, max, lit

# Charger la table existante Dim_Ouvert_par
dfdimOuvert_par_temp = spark.read.table("Inetum_Data.Dim_Ouvert_par")

# R√©cup√©rer le dernier ID_Ouvert_par existant
MAXOuvert_parID = dfdimOuvert_par_temp.select(coalesce(max(col("ID_Ouvert_par")), lit(0)).alias("MAXOuvert_par")).first()[0]

# Extraire les Ouvert_par uniques de df et les comparer avec la table existante
dfdimOuvert_par_silver = df.dropDuplicates(["Ouvert_par"]).select(col("Ouvert_par"))

# Identifier les nouvelles Ouvert_par qui n'existent pas encore dans Dim_Ouvert_par
dfdimOuvert_par_gold = dfdimOuvert_par_silver.join(dfdimOuvert_par_temp, dfdimOuvert_par_silver.Ouvert_par == dfdimOuvert_par_temp.Ouvert_par, "left_anti")

# Ajouter un ID_Ouvert_par unique aux nouvelles soci√©t√©s
dfdimOuvert_par_gold = dfdimOuvert_par_gold.withColumn("ID_Ouvert_par", monotonically_increasing_id() + MAXOuvert_parID + 1)

# Afficher les 10 premi√®res lignes
dfdimOuvert_par_gold.show(10)


# ### üîÑ Mise √† jour et insertion des donn√©es dans la table `Dim_Ouvert_par`
# 
# Dans cette section, nous mettons √† jour ou ins√©rons les donn√©es dans la table **`Dim_Ouvert_par`** en utilisant la commande **`MERGE`** de Delta Lake pour g√©rer les nouvelles valeurs. Voici les √©tapes cl√©s :
# 
# 1. **Chargement de la table Delta** :
#    - Nous chargeons la table Delta **`dim_ouvert_par`** en utilisant **`DeltaTable.forPath()`** pour manipuler les donn√©es dans la table existante.
# 
# 2. **Pr√©paration des donn√©es √† mettre √† jour** :
#    - Nous pr√©parons le DataFrame **`dfdimOuvert_par_gold`** (les nouvelles lignes √† ins√©rer) pour le **`MERGE`** avec la table existante **`dim_ouvert_par`**.
# 
# 3. **Merge - Mise √† jour ou insertion** :
#    - **Quand une correspondance est trouv√©e** : Actuellement, aucun champ n'est mis √† jour lors de la correspondance (la section `whenMatchedUpdate` est vide). Cela pourrait √™tre utilis√© pour mettre √† jour des colonnes sp√©cifiques si n√©cessaire.
#    - **Quand aucune correspondance n'est trouv√©e** : Si la valeur **`Ouvert_par`** n'existe pas encore dans la table, elle est ins√©r√©e dans la table avec son **`ID_Ouvert_par`**.
# 
# Le r√©sultat de cette op√©ration est la mise √† jour de la table **`Dim_Ouvert_par`** avec les nouvelles entit√©s **`Ouvert_par`** qui ne sont pas encore pr√©sentes dans la table.
# 

# In[77]:


from delta.tables import *
# Charger la table Delta
deltaTable = DeltaTable.forPath(spark, 'Tables/dim_ouvert_par')
# DataFrame contenant les mises √† jour    
dfUpdates = dfdimOuvert_par_gold
# Effectuer le merge    
deltaTable.alias('gold') \
  .merge(
    dfUpdates.alias('updates'),
    'gold.Ouvert_par = updates.Ouvert_par'
  ) \
   .whenMatchedUpdate(set =
    {
          
    }
  ) \
 .whenNotMatchedInsert(values =
    {
      "Ouvert_par": "updates.Ouvert_par",
      "ID_Ouvert_par": "updates.ID_Ouvert_par"
    }
  ) \
  .execute()


# In[78]:


# Lecture de la table Delta 'Dim_Groupe_aff' depuis la base de donn√©es et affichage de son contenu
df_Ouvert_par = spark.read.table("Inetum_Data.dim_ouvert_par").show()


# ### üóÇÔ∏è Cr√©ation de la table de fait `factSupervision_gold`
# 
# Dans cette section, nous cr√©ons la table de fait **`factSupervision_gold`** √† l'aide de la m√©thode **`createIfNotExists()`** de Delta Lake. Cette table de fait contient les informations relatives aux tickets supervis√©s, avec un identifiant pour chaque dimension associ√©e √† un ticket.
# 
# Les colonnes d√©finies sont :
# 
# - `Numero` : le num√©ro du ticket (type `String`),
# - `N_du_ticket_externe` : le num√©ro externe du ticket (type `String`),
# - `Ouvert` : la date et l'heure d'ouverture du ticket (type `Timestamp`),
# - `Mis_a_jour` : la date et l'heure de la derni√®re mise √† jour du ticket (type `Timestamp`),
# - `ID_Societe` : identifiant de la soci√©t√© associ√©e au ticket (type `Long`),
# - `ID_Priorite` : identifiant de la priorit√© du ticket (type `Long`),
# - `ID_Groupe_affectation` : identifiant du groupe d'affectation (type `Long`),
# - `ID_Etat` : identifiant de l'√©tat du ticket (type `Long`),
# - `ID_Ouvert_par` : identifiant de l'entit√© ayant ouvert le ticket (type `Long`),
# - `ID_Categorie` : identifiant de la cat√©gorie du ticket (type `Long`).
# 
# Cette table repr√©sente une vue d'ensemble des tickets supervis√©s, avec des cl√©s √©trang√®res pointant vers les tables de dimensions correspondantes. Elle sera utilis√©e pour les analyses permettant de suivre l'√©tat des tickets, leur priorit√©, leur cat√©gorie, et d'autres attributs associ√©s. Elle fait partie du processus de construction du sch√©ma en √©toile et est essentielle pour les rapports de supervision et les analyses op√©rationnelles.
# 

# In[79]:


from pyspark.sql.types import *
from delta.tables import *
# Cr√©ation de la table de fait factSupervision_gold    
DeltaTable.createIfNotExists(spark) \
    .tableName("Inetum_Data.factSupervision_gold") \
    .addColumn("Numero", StringType()) \
    .addColumn("N_du_ticket_externe", StringType()) \
    .addColumn("Ouvert", TimestampType()) \
    .addColumn("Mis_a_jour", TimestampType()) \
    .addColumn("ID_Societe", LongType()) \
    .addColumn("ID_Priorite", LongType()) \
    .addColumn("ID_Groupe_affectation", LongType()) \
    .addColumn("ID_Etat", LongType()) \
    .addColumn("ID_Ouvert_par", LongType()) \
    .addColumn("ID_Categorie", LongType()) \
    .execute()


# ### üóÇÔ∏è Jointure des tables de dimensions dans `factSupervision_gold`
# 
# Dans cette section, nous effectuons la jointure des tables de dimensions avec les cl√©s correspondantes pour cr√©er une table de faits **`factSupervision_gold`**. Nous chargeons les tables de dimensions existantes, puis nous les joignons avec les donn√©es de faits (ici repr√©sent√©es par le DataFrame `df`). Chaque table de dimension est associ√©e √† une cl√© correspondante pour enrichir les informations sur chaque ticket.
# 
# Les √©tapes sont les suivantes :
# 1. Chargement des tables de dimensions depuis **`Inetum_Data`** : 
#    - `dim_societe`
#    - `dim_priorite`
#    - `dim_groupe_aff`
#    - `dim_etat`
#    - `dim_ouvert_par`
#    - `dim_categorie`
# 
# 2. Jointure des donn√©es de faits avec ces dimensions sur les cl√©s correspondantes, telles que la soci√©t√©, la priorit√©, l'affectation, l'√©tat, l'entit√© ayant ouvert le ticket, et la cat√©gorie.
# 
# 3. S√©lection des colonnes n√©cessaires dans la table de faits et organisation des r√©sultats.
# 
# 4. Affichage des 10 premi√®res lignes du DataFrame r√©sultant.
# 

# In[80]:


from pyspark.sql.functions import col
from delta.tables import *

# Charger les tables de dimensions
dfdimSociete = spark.read.table("Inetum_Data.dim_societe")
dfdimPriorite = spark.read.table("Inetum_Data.dim_priorite")
dfdimGroupeAffectation = spark.read.table("Inetum_Data.dim_groupe_affectation")
dfdimEtat = spark.read.table("Inetum_Data.dim_etat")
dfdimOuvertPar = spark.read.table("Inetum_Data.dim_ouvert_par")
dfdimCategorie = spark.read.table("Inetum_Data.dim_categorie")

# Joindre les tables de dimensions avec les cl√©s correspondantes
dffactSupervision_gold = df.alias("df1") \
    .join(dfdimSociete.alias("df2"), col("df1.Societe") == col("df2.Societe"), "left") \
    .join(dfdimPriorite.alias("df3"), col("df1.Priorite") == col("df3.Priorite"), "left") \
    .join(dfdimGroupeAffectation.alias("df4"), col("df1.Groupe_affectation") == col("df4.Groupe_affectation"), "left") \
    .join(dfdimEtat.alias("df5"), col("df1.Etat") == col("df5.Etat"), "left") \
    .join(dfdimOuvertPar.alias("df6"), col("df1.Ouvert_par") == col("df6.Ouvert_par"), "left") \
    .join(dfdimCategorie.alias("df7"), col("df1.Categorie") == col("df7.Categorie"), "left") \
    .select(
        col("df1.Numero"),
        col("df1.N_du_ticket_externe"),
        col("df1.Ouvert"),
        col("df1.Mis_a_jour"),
        col("df2.ID_Societe"),
        col("df3.ID_Priorite"),
        col("df4.ID_Groupe_affectation"),
        col("df5.ID_Etat"),
        col("df6.ID_Ouvert_par"),
        col("df7.ID_Categorie")
    ) \
    .orderBy(col("df1.Ouvert"), col("df1.Numero"))

# Affichage des 10 premi√®res lignes
display(dffactSupervision_gold.limit(10))


# ### üóÇÔ∏è Mise √† jour de la table de faits `factsupervision_gold` avec de nouvelles donn√©es
# 
# Dans cette section, nous effectuons la mise √† jour de la table de faits **`factsupervision_gold`** en utilisant la m√©thode **`merge()`** de Delta Lake. Cette op√©ration permet soit d'ins√©rer de nouvelles lignes, soit de mettre √† jour les lignes existantes dans la table de faits en fonction de la cl√© primaire `Numero` des tickets.
# 
# Les √©tapes sont les suivantes :
# 1. **Chargement de la table Delta** : Nous chargeons la table **`factsupervision_gold`** existante √† partir du chemin sp√©cifi√©.
# 2. **D√©finition des nouvelles donn√©es** : Nous d√©finissons le DataFrame **`dfUpdates`**, qui contient les nouvelles donn√©es que nous souhaitons ins√©rer ou mettre √† jour dans la table de faits.
# 3. **Mise √† jour ou insertion** :
#    - **Mise √† jour** : Si une ligne avec un `Numero` correspondant existe d√©j√† dans la table, les colonnes sont mises √† jour avec les nouvelles valeurs.
#    - **Insertion** : Si aucune ligne avec le m√™me `Numero` n'est trouv√©e, une nouvelle ligne est ins√©r√©e dans la table.
# 4. **Ex√©cution du `merge`** : L'op√©ration **`merge()`** est effectu√©e, ce qui garantit que les donn√©es dans **`factsupervision_gold`** sont soit mises √† jour, soit ajout√©es selon les conditions sp√©cifi√©es.
# 

# In[81]:


from delta.tables import *

# Charger la table Delta
deltaTable = DeltaTable.forPath(spark, 'Tables/factsupervision_gold')

# D√©finir les nouvelles donn√©es mises √† jour
dfUpdates = dffactSupervision_gold

# Effectuer le merge pour ins√©rer ou mettre √† jour les donn√©es
deltaTable.alias('gold') \
  .merge(
    dfUpdates.alias('updates'),
    'gold.Numero = updates.Numero'
  ) \
  .whenMatchedUpdate(set =
    {
      "N_du_ticket_externe": col("updates.N_du_ticket_externe"),
      "Ouvert": col("updates.Ouvert"),
      "Mis_a_jour": col("updates.Mis_a_jour"),
      "ID_Societe": col("updates.ID_Societe"),
      "ID_Priorite": col("updates.ID_Priorite"),
      "ID_Groupe_affectation": col("updates.ID_Groupe_affectation"),
      "ID_Etat": col("updates.ID_Etat"),
      "ID_Ouvert_par": col("updates.ID_Ouvert_par"),
      "ID_Categorie": col("updates.ID_Categorie")
    }
  ) \
  .whenNotMatchedInsert(values =
    {
      "Numero": col("updates.Numero"),
      "N_du_ticket_externe": col("updates.N_du_ticket_externe"),
      "Ouvert": col("updates.Ouvert"),
      "Mis_a_jour": col("updates.Mis_a_jour"),
      "ID_Societe": col("updates.ID_Societe"),
      "ID_Priorite": col("updates.ID_Priorite"),
      "ID_Groupe_affectation": col("updates.ID_Groupe_affectation"),
      "ID_Etat": col("updates.ID_Etat"),
      "ID_Ouvert_par": col("updates.ID_Ouvert_par"),
      "ID_Categorie": col("updates.ID_Categorie")
    }
  ) \
  .execute()


# ### üóÇÔ∏è S√©lectionner et afficher les 1000 premi√®res lignes de `factsupervision_gold`
# 
# Dans cette section, nous ex√©cutons une requ√™te SQL pour r√©cup√©rer les 1000 premi√®res lignes de la table **`factsupervision_gold`**. Ensuite, nous affichons les r√©sultats sous forme de DataFrame.

# In[82]:


df = spark.sql("SELECT * FROM Inetum_Data.factsupervision_gold LIMIT 1000")
display(df)


# In[83]:


df = spark.sql("SELECT * FROM Inetum_Data.dim_etat LIMIT 1000")
display(df)


# In[ ]:




