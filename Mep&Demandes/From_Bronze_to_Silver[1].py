#!/usr/bin/env python
# coding: utf-8

# ## From Bronze to Silver
# 
# New notebook

# ## Documentation du Notebook : Transformation des donn√©es de Bronze √† Silver
# ###  Contexte
# ###### Ce notebook s'inscrit dans une architecture Medallion mise en place dans Microsoft Fabric, et assure la transformation des donn√©es brutes (bronze) vers un format nettoy√© et structur√© (silver), en pr√©paration pour leur exploitation ult√©rieure (gold / rapport Power BI).
# 
# 

# ### üì¶ Importation des biblioth√®ques n√©cessaires
# 
# Dans cette section, nous importons l'ensemble des biblioth√®ques requises pour effectuer les traitements de donn√©es entre les couches Bronze et Silver :
# 
# - `pyspark.sql.types` : permet de d√©finir manuellement les sch√©mas des DataFrames (types de colonnes comme `StringType`, `TimestampType`, etc.).
# - `delta.tables` : fournit les outils pour manipuler les tables Delta (cr√©ation, mise √† jour, merge‚Ä¶), indispensables pour assurer la fiabilit√©, la tra√ßabilit√© et la gestion des versions dans les architectures Lakehouse.
# - `pyspark.sql` : utilis√© pour interagir avec l‚Äôenvironnement Spark, cr√©er des sessions ou manipuler des structures de donn√©es.
# - `pyspark.sql.functions` : contient les fonctions utiles pour transformer et enrichir les donn√©es (`col`, `when`, `lit`, `date_format`, etc.).
# 
# Ces biblioth√®ques sont essentielles pour la lecture des donn√©es, leur transformation et leur √©criture dans les diff√©rentes couches du Lakehouse (Silver, Gold).
# 

# In[7]:


from pyspark.sql.types import *
from delta.tables import *
from pyspark.sql import *
from pyspark.sql.functions import *


# ### üöÄ Initialisation de la session Spark
# 
# Avant d‚Äôeffectuer toute op√©ration sur les donn√©es, il est n√©cessaire d‚Äôinitialiser une session Spark.  
# Cette session constitue le point d‚Äôentr√©e principal pour interagir avec Spark, lire des fichiers, transformer des donn√©es et manipuler des tables Delta.
# 
# Dans cet exemple, nous nommons l'application `LakehouseData` pour faciliter son identification dans l'interface Spark UI.
# 

# In[8]:


# Initialize Spark Session
spark = SparkSession.builder.appName("LakehouseData").getOrCreate()


# ### üß± D√©finition du sch√©ma pour les incidents de mise en production
# 
# Dans cette √©tape, nous d√©finissons manuellement le sch√©ma (`StructType`) des incidents li√©s √† la **mise en production**.  
# Cela permet de structurer correctement les donn√©es brutes (couche Bronze) et d'√©viter que Spark inf√®re automatiquement les types de colonnes, ce qui pourrait g√©n√©rer des erreurs ou des types non coh√©rents.
# 
# Le sch√©ma contient les colonnes suivantes :
# 
# - `Num√©ro`, `Soci√©t√©`, `Priorit√©`, `√âtat`, `Cat√©gorie`, etc. ‚Üí informations descriptives des incidents (type `StringType`).
# - `Ouvert`, `Mis √† jour` ‚Üí dates importantes de suivi des incidents (type `DateType`).
# - `Description br√®ve`, `Groupe d'affectation`, `Ouvert par` ‚Üí √©l√©ments utiles au diagnostic et au suivi op√©rationnel.
# - `N¬∞ de ticket externe` ‚Üí lien √©ventuel avec d'autres outils de supervision.
# - `Ovrt_h`, `Mis_h` ‚Üí champs horaires sp√©cifiques, utiles pour calculer la dur√©e ou effectuer des analyses HO/HNO.
# 
# ‚û°Ô∏è Ce sch√©ma est essentiel pour assurer un chargement structur√© et fiable des donn√©es avant leur transformation dans la couche Silver.
# 

# In[9]:


from pyspark.sql.types import StructType, StructField, StringType

# Sch√©ma pour la table Incident
Incdtexplt_schema = StructType([
    StructField("Num√©ro", StringType(), True),
    StructField("Ouvert", DateType(), True),
    StructField("Mis √† jour", DateType(), True),
    StructField("Soci√©t√©", StringType(), True),
    StructField("Priorit√©", StringType(), True),
    StructField("Description br√®ve", StringType(), True),
    StructField("Groupe d'affectation", StringType(), True),
    StructField("√âtat", StringType(), True),
    StructField("Ouvert par", StringType(), True),
    StructField("Cat√©gorie", StringType(), True),
    StructField("N¬∞ de ticket externe", StringType(), True),
    StructField("Ovrt_h", StringType(), True),
    StructField("Mis_h", StringType(), True)
])


# ### üì• Chargement des donn√©es brutes depuis la couche Bronze (extrait ServiceNow)
# 
# Nous chargeons ici le fichier `incident_modified.csv` situ√© dans la couche `raw_bronze` du Lakehouse, via un chemin ABFS (Azure Blob File System).  
# Ce fichier est un **extrait export√© depuis ServiceNow**, contenant les incidents li√©s aux mises en production, sous format CSV.
# 
# üîç Points cl√©s :
# - Le format de lecture est `csv`.
# - L‚Äôoption `header` permet de prendre la premi√®re ligne comme en-t√™tes de colonnes.
# - Le s√©parateur est `,` (virgule).
# - Le sch√©ma `Incdtexplt_schema` est utilis√© pour imposer la structure correcte d√®s l‚Äôingestion, sans inf√©rence automatique.
# 
# Ce fichier brut constitue la **source initiale** des donn√©es avant traitement : il sera transform√©, nettoy√© et structur√© dans la couche Silver afin d‚Äô√™tre exploit√© dans les analyses et tableaux de bord.
# 

# In[10]:


# Chargement du fichier incident.csv depuis le bronze Lakehouse (ABFS path)
df_br = spark.read.format("csv") \
    .option("header", "true") \
    .option("delimiter", ",") \
    .schema(Incdtexplt_schema) \
    .load("abfss://e979a68f-0c82-481e-9cfa-3a24201ddb86@onelake.dfs.fabric.microsoft.com/69ba2d01-2315-4a5e-9ba4-67be20fe4a65/Files/raw/MEP.csv")


# ### üß™ V√©rification de la structure du DataFrame
# 
# Apr√®s le chargement des donn√©es brutes, nous utilisons la commande `printSchema()` pour afficher la structure (types et noms des colonnes) du DataFrame `df_br`.
# 
# Cela permet de :
# 
# - Valider que le sch√©ma appliqu√© (`Incdtexplt_schema`) a bien √©t√© pris en compte par Spark.
# - V√©rifier qu‚Äôaucune colonne ne pr√©sente de type incorrect (par exemple une date lue comme une cha√Æne).
# - Faciliter les √©tapes de transformation √† venir en ayant une vision claire de la structure des donn√©es.
# 
# Cette v√©rification est cruciale dans une d√©marche de fiabilit√© des pipelines de traitement.
# 

# In[11]:


# Affiche la structure (sch√©ma) du DataFrame pour v√©rifier que les colonnes et types ont bien √©t√© interpr√©t√©s selon le sch√©ma d√©fini
df_br.printSchema()


# ### ‚úèÔ∏è Nettoyage des noms de colonnes : suppression des espaces, accents et caract√®res sp√©ciaux
# 
# √Ä cette √©tape, nous renommons les colonnes du DataFrame `df_br` afin de garantir leur compatibilit√© avec les traitements ult√©rieurs (requ√™tes SQL, stockage Delta, dashboards, etc.).
# 
# Pourquoi faire cela ?
# 
# - üî§ **Enlever les accents** : certains moteurs ou outils ne les g√®rent pas correctement.
# - üö´ **Supprimer les espaces et caract√®res sp√©ciaux** : cela √©vite des erreurs lors de l'acc√®s aux colonnes, notamment dans les requ√™tes SQL ou lors des jointures.
# - üßº **Uniformiser le style** : utiliser un format clair, lisible et coh√©rent (souvent `snake_case` ou `camelCase` sans caract√®res sp√©ciaux).
# 
# Par exemple :
# - `"Mis √† jour"` devient `"Mis_a_jour"`
# - `"Groupe d'affectation"` devient `"Groupe_affectation"`
# - `"N¬∞ de ticket externe"` devient `"Numero_ticket_externe"`
# 
# ‚ö†Ô∏è Ce renommage est essentiel pour assurer la robustesse et la portabilit√© du pipeline dans les couches Silver et Gold.
# 

# In[12]:


# √âtape 3 : Renommer les colonnes pour enlever les espaces, caract√®res sp√©ciaux, accents
df_br = df_br.withColumnRenamed("Num√©ro", "Numero") \
       .withColumnRenamed("Ouvert", "Ouvert") \
       .withColumnRenamed("Mis √† jour", "Mis_a_jour") \
       .withColumnRenamed("Soci√©t√©", "Societe") \
       .withColumnRenamed("Priorit√©", "Priorite") \
       .withColumnRenamed("Description br√®ve", "Description_breve") \
       .withColumnRenamed("Groupe d'affectation", "Groupe_affectation") \
       .withColumnRenamed("√âtat", "Etat") \
       .withColumnRenamed("Ouvert par", "Ouvert_par") \
       .withColumnRenamed("Cat√©gorie", "Categorie") \
       .withColumnRenamed("Description", "Description") \
       .withColumnRenamed("N¬∞ de ticket externe", "Numero_ticket_externe")\
       .withColumnRenamed("Ovrt_h", "Ovrt_H") \
       .withColumnRenamed("Mis_h", "Mis_H") 


# ### üîç V√©rification de la structure apr√®s renommage des colonnes
# 
# Nous utilisons `df_br.printSchema()` une seconde fois pour **v√©rifier que le renommage des colonnes a bien √©t√© appliqu√©**.
# 
# Cette √©tape permet de :
# 
# - ‚úÖ S'assurer que **tous les noms de colonnes sont d√©sormais propres** : sans accents, sans espaces, ni caract√®res sp√©ciaux.
# - üß© Confirmer que le **sch√©ma reste conforme** avec les types de donn√©es attendus (`StringType`, `DateType`, etc.).
# - üõ†Ô∏è Pr√©parer sereinement les prochaines √©tapes de transformation dans la couche Silver, en √©vitant tout risque d‚Äôerreur li√© √† un nom de colonne mal format√©.
# 
# C'est une **bonne pratique syst√©matique** apr√®s toute op√©ration de modification de structure sur un DataFrame.
# 

# In[13]:


# Affiche la structure (sch√©ma) du DataFrame pour v√©rifier que les colonnes et types ont bien √©t√© interpr√©t√©s selon le sch√©ma d√©fini
df_br.printSchema()


# In[14]:


# Affichage des 10 premi√®res lignes
df_br.show(truncate=False)  # Par d√©faut coupe √† 20 caract√®res


# ### üïí Gestion de la compatibilit√© des dates : activation du mode LEGACY
# 
# Cette configuration permet d'√©viter des erreurs lors du **parsing (interpr√©tation)** de dates dans Spark, notamment lorsque les donn√©es d‚Äôentr√©e contiennent :
# 
# - des **formats de dates non standards** (ex : dates mal format√©es dans un export CSV),
# - ou des dates qui ne respectent pas strictement les r√®gles du format ISO.
# 
# üëâ En d√©finissant la propri√©t√© `spark.sql.legacy.timeParserPolicy` √† `"LEGACY"`, on active un **mode de compatibilit√©** avec les anciennes versions de Spark, plus tol√©rant vis-√†-vis des formats de dates h√©t√©rog√®nes.
# 
# Cela permet :
# 
# - d‚Äô√©viter des erreurs de type `DateTimeParseException`,
# - de **continuer le traitement sans blocage** m√™me si certaines valeurs ne sont pas parfaitement format√©es,
# - et d'effectuer un nettoyage ou une normalisation dans une √©tape ult√©rieure, si n√©cessaire.

# In[15]:


spark.conf.set("spark.sql.legacy.timeParserPolicy", "LEGACY")


# ### üßπ Suppression des colonnes non n√©cessaires √† l‚Äôanalyse
# 
# Dans cette √©tape, nous supprimons plusieurs colonnes du DataFrame `df_br` √† l‚Äôaide de la m√©thode `drop()`.  
# Ces colonnes, bien qu‚Äôexistantes dans la donn√©e brute, ne sont **pas n√©cessaires pour les traitements ou analyses** que nous souhaitons effectuer dans la couche Silver.
# 
# Colonnes supprim√©es :
# - `Priorite`  
# - `Societe`  
# - `Groupe_affectation`  
# - `Numero_ticket_externe`  
# - `Categorie`  
# 
# Pourquoi cette suppression ?
# - üîé All√©ger le jeu de donn√©es pour am√©liorer les performances.
# - üß† Se concentrer uniquement sur les colonnes utiles pour les KPI ou les analyses m√©tiers cibl√©es.
# - üìä Pr√©parer un sch√©ma plus simple et exploitable pour la cr√©ation de la table Silver.
# 
# La commande `df_br.show(30)` permet ensuite de **visualiser les premi√®res lignes** du DataFrame et de **v√©rifier que les colonnes ont bien √©t√© supprim√©es.**
# 

# In[16]:


# Suppression des colonnes non n√©cessaires √† l‚Äôanalyse
df_br = df_br.drop("Priorite", "Societe", "Groupe_affectation", "Numero_ticket_externe", "Categorie")
# Affichage des 30 premi√®res lignes
df_br.show(30)


# ### ‚ùì Analyse de la compl√©tude des donn√©es : d√©tection des valeurs nulles
# 
# Cette √©tape consiste √† v√©rifier la **pr√©sence de valeurs nulles** dans chaque colonne du DataFrame `df_br`.
# 
# Pourquoi c‚Äôest important ‚ùì
# - üîç Identifier les colonnes contenant des **donn√©es manquantes**, souvent synonymes de qualit√© de donn√©es √† surveiller.
# - ‚úÖ S‚Äôassurer que les champs critiques (comme `Num√©ro`, `Ouvert`, `Mis_a_jour`, etc.) sont bien renseign√©s.
# - üßº D√©terminer s‚Äôil est n√©cessaire de faire un **nettoyage, un filtrage ou un remplacement** (imputation) de ces valeurs avant de charger les donn√©es en Silver.
# 
# üìå La syntaxe utilis√©e :
# - `col(c).isNull()` : teste si la valeur est nulle.
# - `count(when(...))` : compte le nombre de lignes o√π la condition est vraie pour chaque colonne.
# - `.alias(c)` : donne √† chaque r√©sultat le nom de la colonne correspondante.
# 
# Cette v√©rification est essentielle pour garantir la **qualit√© et la fiabilit√©** des donn√©es en aval.
# 

# In[17]:


from pyspark.sql.functions import col

# V√©rifier les valeurs nulles dans chaque colonne
df_br.select([count(when(col(c).isNull(), c)).alias(c) for c in df_br.columns]).show()


# In[18]:


from pyspark.sql.functions import col, count, when

# Affiche le nombre de valeurs nulles par colonne
df_br.select([
    count(when(col(c).isNull(), c)).alias(c)
    for c in df_br.columns
]).show()


# ### üîç V√©rification des doublons dans le DataFrame
# 
# Avant de poursuivre le traitement, il est important de s'assurer que les donn√©es ne contiennent pas de doublons.
# 
# Dans cette √©tape, nous comparons :
# 
# - **Le nombre total de lignes** dans le DataFrame (`df_br`),
# - **Le nombre de lignes distinctes**, en supprimant les doublons.
# 
# L‚Äôobjectif est de d√©tecter si des enregistrements identiques sont pr√©sents plusieurs fois dans la source.
# 
# üëâ Si le nombre total est sup√©rieur au nombre de lignes distinctes, cela signifie que des **doublons sont pr√©sents** dans les donn√©es.
# 
# Ce diagnostic permet d'assurer la **qualit√©** et **l'unicit√©** des enregistrements avant de passer √† l'√©tape **Gold**.
# 

# In[19]:


from pyspark.sql.functions import count, col

# Compter les lignes totales
total_count = df_br.count()

# Compter les lignes distinctes (uniques)
distinct_count = df_br.distinct().count()

# Afficher les r√©sultats
print(f"Nombre total de lignes      : {total_count}")
print(f"Nombre de lignes distinctes : {distinct_count}")

if total_count == distinct_count:
    print("‚úÖ Aucun doublon d√©tect√©.")
else:
    print(f"‚ö†Ô∏è Il y a {total_count - distinct_count} doublons dans le DataFrame.")


# In[20]:


# from pyspark.sql import functions as F
# from pyspark.sql.types import StringType

# # Cr√©er une fonction pour attribuer 'HO' ou 'HNO' en fonction de l'heure
# def assign_shift(hour):
#     if hour is None:
#         return "Inconnu"
#     elif 8.5 <= hour < 18.5:
#         return "HO"
#     else:
#         return "HNO"

# # Enregistrer la fonction comme une fonction UDF (User Defined Function) dans PySpark
# assign_shift_udf = F.udf(assign_shift, StringType())

# # Ajouter la nouvelle colonne 'Plages_horaires' en fonction de l'heure de la colonne 'Ouvert'
# df_bronze = df_bronze.withColumn('Plages_horaires', 
#                                  assign_shift_udf(F.hour('Ouvert') + F.minute('Ouvert') / 60))

# # Afficher les r√©sultats pour v√©rifier si la nouvelle colonne a √©t√© ajout√©e correctement
# df_bronze.select('Ouvert', 'Plages_horaires').show(truncate=False)


# ### üïê Classification des horaires d‚Äôouverture : HO vs HNO avec gestion des jours f√©ri√©s
# 
# Dans cette √©tape, nous cr√©ons une nouvelle colonne `Plages_horaires` pour cat√©goriser chaque incident selon le **plage horaire de son ouverture** :
# 
# - **HO (Heures Ouvr√©es)** : entre 08h30 et 18h30, du lundi au vendredi, hors jours f√©ri√©s.
# - **HNO (Heures Non Ouvr√©es)** : toutes les autres p√©riodes (nuits, week-ends, jours f√©ri√©s).
# 
# #### üîß √âtapes cl√©s de cette logique :
# 1. **Liste des jours f√©ri√©s 2025** d√©finie manuellement.
# 2. Cr√©ation d‚Äôune fonction Python (`assign_shift_with_holidays`) :
#    - Combine la date (`Ouvert`) et l‚Äôheure (`Ovrt_h`).
#    - V√©rifie si la date est un **week-end ou un jour f√©ri√©**.
#    - Analyse l‚Äôheure d‚Äôouverture pour assigner la bonne cat√©gorie (`HO` ou `HNO`).
# 3. Transformation de cette fonction en **UDF** (`User Defined Function`) via `pyspark.sql.functions.udf`.
# 4. Application de l‚ÄôUDF √† chaque ligne du DataFrame pour cr√©er une nouvelle colonne `Plages_horaires`.
# 
# #### ‚úÖ Objectif :
# Faciliter les analyses sur la **distribution temporelle des incidents** : savoir combien sont ouverts en dehors des horaires standards.
# 
# üß™ L'affichage final permet de contr√¥ler que l'attribution HO/HNO est correcte selon les donn√©es source.
# 

# In[21]:


from pyspark.sql import functions as F
from pyspark.sql.types import StringType
from datetime import datetime

# Liste des jours f√©ri√©s
jours_feries = {
    "2025-01-01", "2025-04-21", "2025-05-01", "2025-05-08", "2025-05-29",
    "2025-06-09", "2025-07-14", "2025-08-15", "2025-11-01", "2025-11-11", "2025-12-25"
}

def assign_shift_with_holidays(date_obj, time_str):
    if date_obj is None or time_str is None:
        return "Inconnu"
    try:
        date_str = date_obj.strftime("%Y-%m-%d")  # conversion depuis DateType
        if date_str in jours_feries or date_obj.weekday() >= 5:
            return "HNO"

        h, m, s = map(int, time_str.split(":"))
        hour_decimal = h + m / 60
        if 8.5 <= hour_decimal < 18.5:
            return "HO"
        else:
            return "HNO"
    except Exception as e:
        return "Invalide"

# D√©claration UDF
assign_shift_udf = F.udf(assign_shift_with_holidays, StringType())

# Application
df_br = df_br.withColumn("Plages_horaires", assign_shift_udf(F.col("Ouvert"), F.col("Ovrt_h")))
df_br.select("Ouvert", "Ovrt_h", "Plages_horaires").show(truncate=False)


# In[22]:


# √âtape 1 : Ajouter la colonne Plages_horaires
df_br = df_br.withColumn("Plages_horaires", assign_shift_udf(F.col("Ouvert"), F.col("Ovrt_h")))

# √âtape 2 : Filtrer uniquement les lignes en HNO
df_hno = df_br.filter(F.col("Plages_horaires") == "HNO")

# √âtape 3 : Afficher le r√©sultat tri√©
df_hno.orderBy("Ouvert", "Ovrt_h").select("Ouvert", "Ovrt_h", "Plages_horaires").show(truncate=False)


# In[23]:


# from pyspark.sql import functions as F
# from pyspark.sql.types import StringType

# # Fonction Python pour assigner HO ou HNO √† partir de 'HH:mm:ss'
# def assign_shift_from_string(time_str):
#     if time_str is None:
#         return "Inconnu"
#     try:
#         h, m, s = map(int, time_str.split(":"))
#         hour_decimal = h + m / 60
#         if 8.5 <= hour_decimal < 18.5:
#             return "HO"
#         else:
#             return "HNO"
#     except:
#         return "Invalide"

# # D√©claration de la UDF
# assign_shift_udf = F.udf(assign_shift_from_string, StringType())

# # Application de la UDF sur la colonne 'Ovrt_H'
# df_br = df_br.withColumn("Plages_horaires", assign_shift_udf("Ovrt_H"))

# # Affichage de v√©rification
# df_br.select("Ovrt_H", "Plages_horaires").show(truncate=False)


# In[24]:


# Compter le nombre de lignes pour chaque valeur de 'Plages_horaires' (HO et HNO)
count_plages_horaires = df_br.groupBy('Plages_horaires').count()

# Afficher les r√©sultats
count_plages_horaires.show(truncate=False)


# In[25]:


# Compter le nombre de valeurs uniques dans la colonne 'Description_breve'
unique_count = df_br.select("Description_breve").distinct().count()

# Afficher le nombre de valeurs uniques
print("Nombre de valeurs uniques dans 'Description_breve':", unique_count)


# In[26]:


# Afficher les valeurs uniques dans la colonne 'Description_breve'
df_br.select("Description_breve").distinct().show(truncate=False)


# In[27]:


# Affichage des 20 premi√®res lignes
df_br.show()


# In[28]:


# Affiche la structure (sch√©ma) du DataFrame pour v√©rifier que les colonnes et types ont bien √©t√© interpr√©t√©s selon le sch√©ma d√©fini
df_br.printSchema()


# In[29]:


# Affichage des 10 premi√®res lignes

df_br.show(10)


# In[30]:


# Compter le nombre total de lignes dans le DataFrame
total_lignes = df_br.count()

# Compter le nombre de lignes distinctes (uniques)
lignes_distinctes = df_br.distinct().count()

# V√©rifier si le nombre de lignes distinctes est inf√©rieur au nombre total de lignes
if total_lignes > lignes_distinctes:
    print("Il y a des lignes dupliqu√©es dans le DataFrame.")
else:
    print("Aucune ligne dupliqu√©e dans le DataFrame.")


# In[31]:


from pyspark.sql.functions import count, when

# Compter les valeurs nulles ou vides pour chaque colonne
df_nulls_vides = df_br.select(
    [count(when(col(c).isNull() | (col(c) == ""), 1)).alias(c) for c in df_br.columns]
)

# Afficher les r√©sultats
df_nulls_vides.show(truncate=False)


# In[32]:


print(type(df_br))
df_br.limit(10).show()


# ### üè∑Ô∏è Ajout d'une colonne d'information contextuelle
# 
# Dans cette √©tape, nous ajoutons une nouvelle colonne nomm√©e **`Type`** au DataFrame.
# 
# - Cette colonne contient une **valeur constante** : `"Mise en production"`.
# - Elle permet d‚Äô**identifier clairement l‚Äôorigine ou le contexte des incidents** analys√©s (dans ce cas, ceux extraits de ServiceNow li√©s √† la mise en production).
# 
# Ce champ peut √™tre utile pour :
# - filtrer ou regrouper les donn√©es par type d‚Äôincident,
# - faciliter les analyses futures ou croisements avec d‚Äôautres jeux de donn√©es.
# 
# Enfin, nous affichons cette colonne pour confirmer son ajout correct dans le DataFrame.
# 

# In[33]:


from pyspark.sql.functions import lit

# Ajouter une colonne 'Type' avec la valeur constante 'Mise en production'
df_br = df_br.withColumn("Type", lit("Mise en production"))

# V√©rifier l'ajout de la colonne dans le DataFrame
df_br.select("Type").show(truncate=False)


# ### üóÇÔ∏è Cr√©ation (ou v√©rification) de la table Delta dans la couche Silver
# 
# Dans cette √©tape, nous cr√©ons une **table Delta** nomm√©e `cleansed_Silver.explt_silv` si elle n'existe pas encore.
# 
# - La table est con√ßue pour accueillir les donn√©es nettoy√©es provenant de la couche Bronze.
# - Elle contient les colonnes principales n√©cessaires √† l'analyse des incidents de mise en production.
# 
# Pourquoi une **table Delta** ?
# - ‚úÖ Permet de b√©n√©ficier des fonctionnalit√©s avanc√©es comme la gestion des versions (time travel), les mises √† jour en place (merge), la gestion des m√©tadonn√©es et la fiabilit√© des donn√©es.
# - üìä Repr√©sente la **couche Silver** dans l'architecture Medallion, o√π les donn√©es sont nettoy√©es, enrichies et pr√™tes pour l'analyse.
# 
# Cette table sera ensuite utilis√©e pour les op√©rations de mise √† jour et d'insertion de donn√©es.
# 

# In[34]:


# 1. Cr√©ation (ou v√©rification) de la table Delta
DeltaTable.createIfNotExists(spark) \
    .tableName("cleansed_Silver.MEP_silver") \
    .addColumn("Numero", StringType()) \
    .addColumn("Ouvert", DateType()) \
    .addColumn("Mis_a_jour", DateType()) \
    .addColumn("Description_breve", StringType()) \
    .addColumn("Etat", StringType()) \
    .addColumn("Ouvert_par", StringType()) \
    .addColumn("Ovrt_h", StringType()) \
    .addColumn("Mis_h", StringType()) \
    .addColumn("Plages_horaires", StringType()) \
    .addColumn("Type", StringType()) \
    .execute()


# ### üîÑ Fusion des donn√©es dans la table Delta (MERGE)
# 
# Cette √©tape consiste √† effectuer une **op√©ration de type MERGE** entre le DataFrame `df_br` (contenant les incidents de mise en production) et la table Delta `cleansed_Silver.explt_silv`.
# 
# Objectifs de cette op√©ration :
# - üîÑ **Mettre √† jour** les lignes existantes si un incident avec le m√™me `Numero` et la m√™me date `Ouvert` est d√©j√† pr√©sent.
# - ‚ûï **Ins√©rer** de nouvelles lignes si l'incident n'existe pas encore dans la table.
# 
# Ce m√©canisme permet d'assurer :
# - L'**int√©grit√©** des donn√©es (√©vite les doublons),
# - Une **synchronisation continue** entre la source (bronze) et la table nettoy√©e (silver),
# - Une gestion efficace de l'**historique et des mises √† jour**.
# 
# ‚ÑπÔ∏è Les colonnes ajout√©es r√©cemment (`Plages_horaires`, `Type`) sont √©galement prises en compte dans cette op√©ration de fusion.
# 

# In[35]:


from delta.tables import DeltaTable

# 3. MERGE dans la table Delta
deltaTable = DeltaTable.forName(spark, "cleansed_Silver.MEP_silver")

deltaTable.alias("silver") \
    .merge(
        df_br.alias("updates"),
        "silver.Numero = updates.Numero AND silver.Ouvert = updates.Ouvert"
    ) \
    .whenMatchedUpdate(set={
        "Mis_a_jour": "updates.Mis_a_jour",
        "Description_breve": "updates.Description_breve",
        "Etat": "updates.Etat",
        "Ouvert_par": "updates.Ouvert_par",
        "Ovrt_h": "updates.Ovrt_h",
        "Mis_h": "updates.Mis_h",
        "Plages_horaires": "updates.Plages_horaires",  # ajout ici
        "Type": "updates.Type"  # ajouter ici

        
    }) \
    .whenNotMatchedInsert(values={
        "Numero": "updates.Numero",
        "Ouvert": "updates.Ouvert",
        "Mis_a_jour": "updates.Mis_a_jour",
        "Description_breve": "updates.Description_breve",
        "Etat": "updates.Etat",
        "Ouvert_par": "updates.Ouvert_par",
        "Ovrt_h": "updates.Ovrt_h",
        "Mis_h": "updates.Mis_h",
        "Plages_horaires": "updates.Plages_horaires",  # ajout ici
        "Type": "updates.Type"  # ajouter ici

    }) \
    .execute()


# ### üì• Lecture des donn√©es depuis la table Delta Silver
# 
# √Ä cette √©tape, nous lisons les donn√©es stock√©es dans la table Delta `cleansed_Silver.explt_silv` afin de :
# 
# - ‚úÖ V√©rifier que les donn√©es ont bien √©t√© ins√©r√©es ou mises √† jour,
# - üîç Visualiser un √©chantillon (les 10 premi√®res lignes) du r√©sultat final,
# - üìä S'assurer que la transformation depuis la couche Bronze vers la Silver s'est d√©roul√©e correctement.
# 
# Cette op√©ration permet de **valider le pipeline de traitement** et de pr√©parer les donn√©es pour la suite du processus, notamment vers la couche Gold ou pour des analyses en dashboard.
# 

# In[36]:


df = spark.read.table("cleansed_Silver.MEP_silver")
display(df.head(10))


# In[37]:


# Charger le DataFrame
df = spark.read.table("cleansed_Silver.MEP_silver")

# Filtrer les lignes o√π 'Type' est 'Mise en production' et compter le nombre de ces lignes
count_mise_en_production = df.filter(df["Type"] == "Mise en production").count()

# Afficher le r√©sultat
print(f"Nombre de 'Mise en production' dans la colonne 'Type' : {count_mise_en_production}")


# In[38]:


# Charger le DataFrame
df = spark.read.table("cleansed_Silver.MEP_silver")

# Filtrer les lignes o√π 'Type' est 'HNO'
df_hno = df.filter(df["Plages_horaires"] == "HNO")

# S√©lectionner les colonnes √† afficher
df_hno.select("Plages_horaires", "Ouvert", "Ovrt_h").show(truncate=False)


# In[39]:


# abfss://Exploitation_PFE@onelake.dfs.fabric.microsoft.com/raw_bronze.Lakehouse/Files/raw/sctaskN.csv


# ### üóÇÔ∏è D√©finition du sch√©ma pour les demandes (Demande_schema)
# 
# Avant de charger le fichier contenant les demandes, nous d√©finissons explicitement le sch√©ma (`Demande_schema`). Cela permet √† Spark :
# 
# - üìå D'interpr√©ter correctement le type de chaque colonne (dates, cha√Ænes, entiers, etc.),
# - üö´ D'√©viter que Spark devine automatiquement les types, ce qui peut provoquer des erreurs ou incoh√©rences,
# - ‚úÖ D'assurer un chargement structur√© et fiable des donn√©es dans le DataFrame.
# 
# Voici les colonnes d√©finies :
# - `Cr√©√©` et `Ferm√©` : dates de cr√©ation et de cl√¥ture de la demande,
# - `Num√©ro` : identifiant unique de la demande,
# - `Description br√®ve` : courte description du besoin,
# - `Groupe d'affectation` et `Affect√© √†` : informations de responsabilit√©,
# - `Task Duration` : dur√©e de traitement en secondes,
# - `Cr√©√©_h` et `Ferm√©_h` : heures correspondantes (format texte) pour analyse horaire.
# 
# Cette √©tape est cruciale pour une ingestion propre dans la couche Silver.
# 

# In[40]:


from pyspark.sql.types import StructType, StructField, StringType

# Sch√©ma pour la table Incident
Demande_schema = StructType([
    StructField("Cr√©√©", DateType(), True),
    StructField("Ferm√©", DateType(), True),
    StructField("Num√©ro", StringType(), True),
    StructField("Description br√®ve", StringType(), True),
    StructField("Groupe d'affectation", StringType(), True),
    StructField("Affect√© √†", StringType(), True),
    StructField("Task Duration", LongType(), True),
    StructField("Cr√©√©_h", StringType(), True),
    StructField("Ferm√©_h", StringType(), True)
])


# ### üì• Chargement des donn√©es des demandes depuis la couche Bronze
# 
# Dans cette √©tape, nous chargeons le fichier **`sctaskN.csv`** depuis le **Bronze Lakehouse** en utilisant le chemin ABFS (Azure Blob File System). Ce fichier contient les demandes extraites de la plateforme ServiceNow.
# 
# Voici les options et param√®tres utilis√©s :
# 
# - `.format("csv")` : le fichier source est au format CSV,
# - `.option("header", "true")` : la premi√®re ligne du fichier contient les noms des colonnes,
# - `.option("delimiter", ",")` : les colonnes sont s√©par√©es par des virgules,
# - `.schema(Demande_schema)` : on applique un sch√©ma explicite d√©fini au pr√©alable pour garantir une structuration correcte des donn√©es,
# - `.load(...)` : chemin du fichier dans le Lakehouse, dans le dossier *raw/bronze*.
# 
# ‚úÖ Cette √©tape permet d‚Äôimporter proprement les donn√©es de demande dans un DataFrame Spark pour les futures transformations dans la couche Silver.
# 

# In[42]:


# Chargement du fichier incident.csv depuis le bronze Lakehouse (ABFS path)
df_De = spark.read.format("csv") \
    .option("header", "true") \
    .option("delimiter", ",") \
    .schema(Demande_schema) \
    .load("abfss://e979a68f-0c82-481e-9cfa-3a24201ddb86@onelake.dfs.fabric.microsoft.com/69ba2d01-2315-4a5e-9ba4-67be20fe4a65/Files/raw/demandes.csv")


# ### üßæ V√©rification du sch√©ma du DataFrame
# 
# Apr√®s le chargement du fichier **`sctaskN.csv`**, nous affichons la structure du DataFrame √† l'aide de `printSchema()`.
# 
# Cette commande permet de :
# 
# - ‚úÖ Confirmer que les colonnes ont bien √©t√© reconnues,
# - ‚úÖ V√©rifier que les types de donn√©es (`StringType`, `DateType`, `LongType`, etc.) correspondent bien √† ceux d√©finis dans le sch√©ma (`Demande_schema`),
# - üõ†Ô∏è Identifier d‚Äô√©ventuelles erreurs de typage ou de lecture avant d‚Äôappliquer les transformations en couche Silver.
# 
# C‚Äôest une √©tape de contr√¥le essentielle dans le processus de data cleansing, pour assurer la qualit√© et la fiabilit√© du pipeline.
# 

# In[43]:


# Affiche la structure (sch√©ma) du DataFrame pour v√©rifier que les colonnes et types ont bien √©t√© interpr√©t√©s selon le sch√©ma d√©fini
df_De.printSchema()


# ### üîÑ Modification des noms de colonnes dans le DataFrame
# 
# Dans cette √©tape, nous proc√©dons √† la modification des noms de colonnes pour √©liminer les espaces, caract√®res sp√©ciaux et accents. Cette transformation permet d'obtenir des noms de colonnes plus compatibles avec les pratiques standards en programmation, tout en am√©liorant la lisibilit√© du DataFrame.
# 
# Les modifications suivantes ont √©t√© appliqu√©es aux colonnes :
# - **Cr√©√©** ‚Üí `Cree`
# - **Ferm√©** ‚Üí `Ferme`
# - **Num√©ro** ‚Üí `Numero`
# - **Description br√®ve** ‚Üí `Description_breve`
# - **Groupe d'affectation** ‚Üí `Groupe_affectation`
# - **Affect√© √†** ‚Üí `Affecte_a`
# - **Task Duration** ‚Üí `Task_Duration`
# - **Cr√©√©_h** ‚Üí `Cree_h`
# - **Ferm√©_h** ‚Üí `Ferme_h`
# 
# Cela garantit une meilleure gestion des donn√©es et des op√©rations ult√©rieures sur les colonnes.
# 

# In[44]:


# √âtape 3 : Renommer les colonnes pour enlever les espaces, caract√®res sp√©ciaux, accents
df_De = df_De.withColumnRenamed("Cr√©√©", "Cree") \
       .withColumnRenamed("Ferm√©", "Ferme") \
       .withColumnRenamed("Num√©ro", "Numero") \
       .withColumnRenamed("Description br√®ve", "Description_breve") \
       .withColumnRenamed("Groupe d'affectation", "Groupe_affectation") \
       .withColumnRenamed("Affect√© √†", "Affecte_a") \
       .withColumnRenamed("Task Duration", "Task_Duration") \
       .withColumnRenamed("Cr√©√©_h", "Cree_h") \
       .withColumnRenamed("Ferm√©_h", "Ferme_h")


# ### üîç V√©rification de la structure apr√®s renommage des colonnes
# 
# Nous utilisons `df_De.printSchema()` une seconde fois pour **v√©rifier que le renommage des colonnes a bien √©t√© appliqu√©**.
# 
# Cette √©tape permet de :
# 
# - ‚úÖ S'assurer que **tous les noms de colonnes sont d√©sormais propres** : sans accents, sans espaces, ni caract√®res sp√©ciaux.
# - üß© Confirmer que le **sch√©ma reste conforme** avec les types de donn√©es attendus (`StringType`, `DateType`, etc.).
# - üõ†Ô∏è Pr√©parer sereinement les prochaines √©tapes de transformation dans la couche Silver, en √©vitant tout risque d‚Äôerreur li√© √† un nom de colonne mal format√©.
# 
# C'est une **bonne pratique syst√©matique** apr√®s toute op√©ration de modification de structure sur un DataFrame.
# 

# In[45]:


# Affiche la structure (sch√©ma) du DataFrame pour v√©rifier que les colonnes et types ont bien √©t√© interpr√©t√©s selon le sch√©ma d√©fini
df_De.printSchema()


# In[46]:


# Affichage des 20 premi√®res lignes
df_De.show(truncate=False)  # Par d√©faut coupe √† 20 caract√®res


# ### üïí Gestion de la compatibilit√© des dates : activation du mode LEGACY
# 
# Cette configuration permet d'√©viter des erreurs lors du **parsing (interpr√©tation)** de dates dans Spark, notamment lorsque les donn√©es d‚Äôentr√©e contiennent :
# 
# - des **formats de dates non standards** (ex : dates mal format√©es dans un export CSV),
# - ou des dates qui ne respectent pas strictement les r√®gles du format ISO.
# 
# üëâ En d√©finissant la propri√©t√© `spark.sql.legacy.timeParserPolicy` √† `"LEGACY"`, on active un **mode de compatibilit√©** avec les anciennes versions de Spark, plus tol√©rant vis-√†-vis des formats de dates h√©t√©rog√®nes.
# 
# Cela permet :
# 
# - d‚Äô√©viter des erreurs de type `DateTimeParseException`,
# - de **continuer le traitement sans blocage** m√™me si certaines valeurs ne sont pas parfaitement format√©es,
# - et d'effectuer un nettoyage ou une normalisation dans une √©tape ult√©rieure, si n√©cessaire.

# In[47]:


spark.conf.set("spark.sql.legacy.timeParserPolicy", "LEGACY")


# ### üßπ Suppression des colonnes non n√©cessaires √† l‚Äôanalyse
# Dans cette √©tape, nous supprimons la colonne **`Groupe_affectation`** du DataFrame `df_De` √† l'aide de la m√©thode **`drop()`**. Cette op√©ration permet de se d√©barrasser des informations non n√©cessaires √† l'analyse actuelle.
# 

# In[48]:


# Suppression des colonnes non n√©cessaires √† l‚Äôanalyse
df_De = df_De.drop("Groupe_affectation")
# Affichage des 30 premi√®res lignes
df_De.show(30)


# ### ‚ùì Analyse de la compl√©tude des donn√©es : d√©tection des valeurs nulles
# 
# Dans cette √©tape, nous v√©rifions la **pr√©sence de valeurs nulles** dans chaque colonne du DataFrame `df_De`.
# 
# #### Pourquoi c‚Äôest important ‚ùì
# - üîç Identifier les colonnes qui contiennent des **donn√©es manquantes**. Cela peut avoir un impact sur la qualit√© des analyses et des d√©cisions prises sur la base des donn√©es.
# - ‚úÖ S'assurer que les champs cruciaux, comme `Num√©ro`, `Description_breve`, `Affecte_a`, etc., sont correctement remplis et ne contiennent pas de valeurs nulles.
# - üßº D√©terminer les colonnes n√©cessitant un **nettoyage ou un traitement particulier** (comme le remplacement des valeurs nulles par des valeurs par d√©faut ou des moyennes).
# 
# #### üìå La syntaxe utilis√©e :
# - **`col(c).isNull()`** : Cette fonction teste si une valeur d'une colonne est nulle.
# - **`count(when(...))`** : Cette fonction permet de compter combien de lignes contiennent une valeur nulle pour chaque colonne.
# - **`.alias(c)`** : Utilis√© pour attribuer le nom de la colonne √† chaque r√©sultat.
# 
# Le r√©sultat de cette op√©ration est un tableau qui affiche pour chaque colonne le nombre de valeurs nulles qu‚Äôelle contient, permettant ainsi une √©valuation rapide de la **qualit√© des donn√©es** avant d‚Äôentamer d‚Äôautres traitements.
# 

# In[49]:


from pyspark.sql.functions import col

# V√©rifier les valeurs nulles dans chaque colonne
df_De.select([count(when(col(c).isNull(), c)).alias(c) for c in df_De.columns]).show()


# In[50]:


# Appliquer un filtre pour supprimer les lignes o√π les colonnes 'Ferme' et 'Ferme_h' contiennent des valeurs nulles
df_De = df_De.filter(col("Ferme").isNotNull() & col("Ferme_h").isNotNull())

# Afficher les premi√®res lignes du DataFrame apr√®s filtrage
df_De.show()


# In[51]:


from pyspark.sql.functions import col

# V√©rifier les valeurs nulles dans chaque colonne
df_De.select([count(when(col(c).isNull(), c)).alias(c) for c in df_De.columns]).show()


# In[52]:


from pyspark.sql.functions import col, count, when

# Affiche le nombre de valeurs nulles par colonne
df_De.select([
    count(when(col(c).isNull(), c)).alias(c)
    for c in df_De.columns
]).show()


# ### üîç V√©rification des doublons dans le DataFrame
# 
# Avant de poursuivre le traitement, il est important de s'assurer que les donn√©es ne contiennent pas de doublons.
# 
# Dans cette √©tape, nous comparons :
# 
# - **Le nombre total de lignes** dans le DataFrame (`df_De`),
# - **Le nombre de lignes distinctes**, en supprimant les doublons.
# 
# L‚Äôobjectif est de d√©tecter si des enregistrements identiques sont pr√©sents plusieurs fois dans la source.
# 
# üëâ Si le nombre total est sup√©rieur au nombre de lignes distinctes, cela signifie que des **doublons sont pr√©sents** dans les donn√©es.
# 
# Ce diagnostic permet d'assurer la **qualit√©** et **l'unicit√©** des enregistrements avant de passer √† l'√©tape **Gold**.
# 

# In[53]:


from pyspark.sql.functions import count, col

# Compter les lignes totales
total_count = df_De.count()

# Compter les lignes distinctes (uniques)
distinct_count = df_De.distinct().count()

# Afficher les r√©sultats
print(f"Nombre total de lignes      : {total_count}")
print(f"Nombre de lignes distinctes : {distinct_count}")

if total_count == distinct_count:
    print("‚úÖ Aucun doublon d√©tect√©.")
else:
    print(f"‚ö†Ô∏è Il y a {total_count - distinct_count} doublons dans le DataFrame.")


# ### üïí Classification des plages horaires : Assignation des shifts √† partir de l‚Äôheure d‚Äôouverture
# 
# Dans cette √©tape, nous cr√©ons une nouvelle colonne `Plages_horaires` pour cat√©goriser chaque incident selon son horaire de cr√©ation (`Cree_h`) :
# 
# - **Shift 1** : entre 07h00 et 14h59.
# - **Shift 2** : entre 15h00 et 22h59.
# - **Shift 3** : entre 23h00 et 06h59.
# 
# üîß **√âtapes cl√©s de cette logique** :
# 
# 1. **Fonction Python `assign_shift_from_string`** :
#     - La fonction prend en entr√©e une cha√Æne de caract√®res au format `HH:mm:ss`.
#     - Elle convertit l'heure en heure d√©cimale pour faciliter la comparaison.
#     - En fonction de l'heure d√©cimale, la fonction attribue un shift sp√©cifique :
#       - **Shift 1** : pour les heures entre 07:00 et 14:59.
#       - **Shift 2** : pour les heures entre 15:00 et 22:59.
#       - **Shift 3** : pour les heures entre 23:00 et 06:59.
#     - Si l'heure est invalide, la fonction retourne "Invalide" ou "Inconnu".
# 
# 2. **Cr√©ation de l‚ÄôUDF `assign_shift_udf`** :
#     - La fonction Python est transform√©e en **UDF** (User Defined Function) avec un type de retour `StringType()`.
# 
# 3. **Application de l‚ÄôUDF** :
#     - L‚ÄôUDF est appliqu√©e sur la colonne `Cree_h` pour cr√©er une nouvelle colonne `Plages_horaires` qui contient le shift attribu√© √† chaque ligne.
# 
# ‚úÖ **Objectif** :
# Permettre une analyse plus fine des incidents en fonction de l'heure de leur cr√©ation, ce qui est utile pour les √©tudes de charge de travail ou d'optimisation des ressources en fonction des horaires.
# 

# In[54]:


from pyspark.sql import functions as F
from pyspark.sql.types import StringType

# Fonction Python pour assigner les shifts en fonction de 'HH:mm:ss'
def assign_shift_from_string(time_str):
    if time_str is None:
        return "Inconnu"
    try:
        h, m, s = map(int, time_str.split(":"))
        hour_decimal = h + m / 60

        # Shift 1: entre 7h et 15h
        if 7 <= hour_decimal < 15:
            return "Shift 1"
        # Shift 2: entre 15h et 23h
        elif 15 <= hour_decimal < 23:
            return "Shift 2"
        # Shift 3: entre 23h et 7h
        else:
            return "Shift 3"
    except:
        return "Invalide"

# D√©claration de la UDF
assign_shift_udf = F.udf(assign_shift_from_string, StringType())

# Application de la UDF sur la colonne 'Cree_h' de df_De (et non 'Cr√©√©_h')
df_De = df_De.withColumn("Plages_horaires", assign_shift_udf("Cree_h"))

# Affichage de v√©rification
df_De.select("Cree_h", "Plages_horaires").show(truncate=False)


# In[55]:


# Compter le nombre de lignes pour chaque valeur de 'Plages_horaires' 
count_plages_horaires = df_De.groupBy('Plages_horaires').count()

# Afficher les r√©sultats
count_plages_horaires.show(truncate=False)


# In[56]:


# Compter le nombre de valeurs uniques dans la colonne 'Description_breve'
unique_count = df_De.select("Description_breve").distinct().count()

# Afficher le nombre de valeurs uniques
print("Nombre de valeurs uniques dans 'Description_breve':", unique_count)


# In[57]:


# Afficher les valeurs uniques dans la colonne 'Description_breve'
df_De.select("Description_breve").distinct().show(truncate=False)


# In[58]:


# Affichage des 20 premi√®res lignes
df_De.show()


# In[59]:


# Affiche la structure (sch√©ma) du DataFrame pour v√©rifier que les colonnes et types ont bien √©t√© interpr√©t√©s selon le sch√©ma d√©fini
df_De.printSchema()


# In[60]:


# Compter le nombre total de lignes dans le DataFrame
total_lignes = df_De.count()

# Compter le nombre de lignes distinctes (uniques)
lignes_distinctes = df_De.distinct().count()

# V√©rifier si le nombre de lignes distinctes est inf√©rieur au nombre total de lignes
if total_lignes > lignes_distinctes:
    print("Il y a des lignes dupliqu√©es dans le DataFrame.")
else:
    print("Aucune ligne dupliqu√©e dans le DataFrame.")


# In[61]:


from pyspark.sql.functions import count, when

# Compter les valeurs nulles ou vides pour chaque colonne
df_nulls_vides = df_De.select(
    [count(when(col(c).isNull() | (col(c) == ""), 1)).alias(c) for c in df_De.columns]
)

# Afficher les r√©sultats
df_nulls_vides.show(truncate=False)


# ### ‚ûï Ajout d'une colonne 'Type' avec une valeur constante
# 
# Dans cette √©tape, nous ajoutons une nouvelle colonne `Type` √† notre DataFrame `df_De`, et nous lui attribuons la valeur constante `"Demande"` pour toutes les lignes.
# 
# üîß **√âtapes cl√©s de cette logique** :
# 
# 1. **Ajout de la colonne** :
#     - Utilisation de la fonction `lit()` pour cr√©er une colonne contenant une valeur constante (ici, `"Demande"`).
#     - La colonne est ajout√©e au DataFrame avec `withColumn()`.
# 
# 2. **V√©rification de l'ajout** :
#     - L'ajout de la colonne est valid√© en affichant les premi√®res lignes de la colonne `Type`.
# 
# ‚úÖ **Objectif** :
# Ajouter une colonne suppl√©mentaire permettant de cat√©goriser ou d'annoter les lignes avec une valeur statique, utile pour les analyses futures ou pour distinguer les types de donn√©es dans le DataFrame.

# In[62]:


from pyspark.sql.functions import lit

# Ajouter une colonne 'Type' avec la valeur constante 'Mise en production'
df_De = df_De.withColumn("Type", lit("Demande"))

# V√©rifier l'ajout de la colonne dans le DataFrame
df_De.select("Type").show(truncate=False)


# In[63]:


# Affiche la structure (sch√©ma) du DataFrame pour v√©rifier que les colonnes et types ont bien √©t√© interpr√©t√©s selon le sch√©ma d√©fini
df_De.printSchema()


# ### üóÇÔ∏è Cr√©ation (ou v√©rification) de la table Delta dans la couche Silver
# 
# Dans cette √©tape, nous cr√©ons une **table Delta** nomm√©e `cleansed_Silver.demande_silv` si elle n'existe pas encore.
# 
# - La table est con√ßue pour accueillir les donn√©es nettoy√©es provenant de la couche Bronze.
# - Elle contient les colonnes principales n√©cessaires √† l'analyse des demande.
# 
# Pourquoi une **table Delta** ?
# - ‚úÖ Permet de b√©n√©ficier des fonctionnalit√©s avanc√©es comme la gestion des versions (time travel), les mises √† jour en place (merge), la gestion des m√©tadonn√©es et la fiabilit√© des donn√©es.
# - üìä Repr√©sente la **couche Silver** dans l'architecture Medallion, o√π les donn√©es sont nettoy√©es, enrichies et pr√™tes pour l'analyse.
# 
# Cette table sera ensuite utilis√©e pour les op√©rations de mise √† jour et d'insertion de donn√©es.
# 

# In[64]:


from pyspark.sql.types import StringType, DateType, LongType

# 1. Cr√©ation (ou v√©rification) de la table Delta
DeltaTable.createIfNotExists(spark) \
    .tableName("cleansed_Silver.demandes_silver") \
    .addColumn("Cree", DateType()) \
    .addColumn("Ferme", DateType()) \
    .addColumn("Numero", StringType()) \
    .addColumn("Description_breve", StringType()) \
    .addColumn("Affecte_a", StringType()) \
    .addColumn("Task_Duration", LongType()) \
    .addColumn("Cree_h", StringType()) \
    .addColumn("Ferme_h", StringType()) \
    .addColumn("Plages_horaires", StringType()) \
    .addColumn("Type", StringType(), nullable=False) \
    .execute()


# ### üîÑ Fusion des donn√©es dans la table Delta (MERGE)
# 
# Cette √©tape consiste √† effectuer une **op√©ration de type MERGE** entre le DataFrame `df_De` (contenant les demande) et la table Delta `cleansed_Silver.demande_silv`.
# 
# Objectifs de cette op√©ration :
# - üîÑ **Mettre √† jour** les lignes existantes si un incident avec le m√™me `Numero` et la m√™me date `Ouvert` est d√©j√† pr√©sent.
# - ‚ûï **Ins√©rer** de nouvelles lignes si l'incident n'existe pas encore dans la table.
# 
# Ce m√©canisme permet d'assurer :
# - L'**int√©grit√©** des donn√©es (√©vite les doublons),
# - Une **synchronisation continue** entre la source (bronze) et la table nettoy√©e (silver),
# - Une gestion efficace de l'**historique et des mises √† jour**.
# 
# ‚ÑπÔ∏è Les colonnes ajout√©es r√©cemment (`Plages_horaires`, `Type`) sont √©galement prises en compte dans cette op√©ration de fusion.
# 

# In[66]:


from delta.tables import DeltaTable
from pyspark.sql import functions as F

# 3. MERGE dans la table Delta
deltaTable = DeltaTable.forName(spark, "cleansed_Silver.demandes_silver")

deltaTable.alias("silver") \
    .merge(
        df_De.alias("updates"),
        "silver.Numero = updates.Numero "
    ) \
    .whenMatchedUpdate(set={
        "Ferme": "updates.Ferme",
        "Numero": "updates.Numero",
        "Description_breve": "updates.Description_breve",
        "Affecte_a": "updates.Affecte_a",
        "Task_Duration": "updates.Task_Duration",
        "Cree_h": "updates.Cree_h",
        "Ferme_h": "updates.Ferme_h",
        "Plages_horaires": "updates.Plages_horaires",  # ajout ici
        "Type": "updates.Type"  # ajouter ici
    }) \
    .whenNotMatchedInsert(values={
        "Cree": "updates.Cree",
        "Ferme": "updates.Ferme",
        "Numero": "updates.Numero",
        "Description_breve": "updates.Description_breve",
        "Affecte_a": "updates.Affecte_a",
        "Task_Duration": "updates.Task_Duration",
        "Cree_h": "updates.Cree_h",
        "Ferme_h": "updates.Ferme_h",
        "Plages_horaires": "updates.Plages_horaires",  # ajout ici
        "Type": "updates.Type"  # ajouter ici
    }) \
    .execute()


# ### üì• Lecture des donn√©es depuis la table Delta Silver
# 
# √Ä cette √©tape, nous lisons les donn√©es stock√©es dans la table Delta `cleansed_Silver.demande_silv` afin de :
# 
# - ‚úÖ V√©rifier que les donn√©es ont bien √©t√© ins√©r√©es ou mises √† jour,
# - üîç Visualiser un √©chantillon (les 10 premi√®res lignes) du r√©sultat final,
# - üìä S'assurer que la transformation depuis la couche Bronze vers la Silver s'est d√©roul√©e correctement.
# 
# Cette op√©ration permet de **valider le pipeline de traitement** et de pr√©parer les donn√©es pour la suite du processus, notamment vers la couche Gold ou pour des analyses en dashboard.
# 

# In[67]:


df = spark.read.table("cleansed_Silver.demandes_silver")
display(df.head(10))


# In[68]:


# Charger le DataFrame
df = spark.read.table("cleansed_Silver.demandes_silver")

# Filtrer les lignes o√π 'Type' est 'Mise en production' et compter le nombre de ces lignes
count_mise_Demande = df.filter(df["Type"] == "Demande").count()

# Afficher le r√©sultat
print(f"Nombre de 'Demande' dans la colonne 'Type' : {count_mise_Demande}")


# In[69]:


print(spark.read.table("cleansed_Silver.demandes_silver").count())

