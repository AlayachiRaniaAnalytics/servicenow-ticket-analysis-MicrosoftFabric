#!/usr/bin/env python
# coding: utf-8

# ## transform from silver to gold
# 
# New notebook

# ### üìä Transformation des donn√©es de la couche Silver √† la couche Gold
# 
# #### Objectif :
# Ce processus consiste √† effectuer la transformation des donn√©es depuis la **couche Silver** vers la **couche Gold** dans le Lakehouse. La couche Silver contient des donn√©es nettoy√©es et enrichies, tandis que la couche Gold repr√©sente les donn√©es finales pr√™tes pour l'analyse et la visualisation. 
# 
# Dans cette √©tape, nous mettons en place le **sch√©ma en √©toile** en cr√©ant les **tables de dimension** et la **table de fait** qui alimenteront nos rapports analytiques.

# ### üì¶ Importation des biblioth√®ques n√©cessaires
# 
# Dans cette section, nous importons les biblioth√®ques requises pour la suite du traitement :
# 
# - `pyspark.sql.types` : permet de d√©finir les sch√©mas (types de colonnes) pour structurer correctement les DataFrames.
# 
# - `delta.tables` : n√©cessaire pour manipuler les tables Delta (mise √† jour, suppression, merge, etc.), qui assurent la fiabilit√© et la tra√ßabilit√© des donn√©es dans la couche Silver.
# 
# - `pyspark.sql.functions` : contient un ensemble de fonctions permettant la transformation et la manipulation de colonnes dans un DataFrame. 
# 
# - `pyspark.sql.SparkSession` : permet d‚Äôinstancier une session Spark, qui constitue le point d‚Äôentr√©e principal pour la manipulation des donn√©es avec PySpark (lecture, √©criture, transformation, etc.).
# 

# In[1]:


from pyspark.sql.functions import col, split, dayofmonth, month, year
from pyspark.sql import SparkSession
from pyspark.sql.functions import split
from pyspark.sql.types import *
from delta.tables import *
from pyspark.sql.functions import monotonically_increasing_id, col, coalesce, max, lit


# # Les MEP

# #### √âtapes de la transformation
# 
# 1. **Chargement des donn√©es depuis la couche Silver :**
#    Les donn√©es sont lues depuis la table `Inetum_Data.supervision_silver`, qui contient les informations de niveau interm√©diaire, c'est-√†-dire des donn√©es nettoy√©es mais non encore totalement agr√©g√©es ou enrichies.

# In[2]:


df = spark.read.table("cleansed_Silver.MEP_silver")


# In[3]:


display(df.head(10))


# ### üõ†Ô∏è Cr√©ation de la table de dimension `Dim_Date_Ouverture`
# 
# Dans cette √©tape, nous cr√©ons une nouvelle table de dimension `Dim_Date_Ouverture` dans le lakehouse. Cette table sera utilis√©e pour stocker des informations sur les dates d'ouverture des tickets dans une structure dimensionnelle.
# 
# #### Processus de cr√©ation :
# 
# 1. **V√©rification de l'existence de la table** : Nous utilisons la m√©thode `createIfNotExists` pour garantir que la table est cr√©√©e uniquement si elle n'existe pas d√©j√†.
# 2. **D√©finition du sch√©ma** : La table contient plusieurs colonnes pour d√©tailler les informations relatives √† la date et √† l'heure d'ouverture des tickets :
#    - `Ouvert` : Le timestamp de l'ouverture du ticket.
#    - `Jour`, `Mois`, `Annee`, `Heure`, `Minute`, `Seconde` : Ces colonnes permettent de d√©composer la date d'ouverture pour une analyse plus pr√©cise.
# 3. **Ex√©cution de la cr√©ation** : La m√©thode `execute()` permet de finaliser la cr√©ation de la table.
# 
# Cette structure nous permettra de faire des jointures avec d'autres tables, comme les faits relatifs aux tickets, en utilisant des cl√©s dimensionnelles bas√©es sur la date et l'heure d'ouverture.
# 

# In[4]:


# Cr√©ation de la table de dimension Dim_Date_Ouverture
DeltaTable.createIfNotExists(spark) \
    .tableName("refined_gold.Dim_Date_Ouverture") \
    .addColumn("Ouvert", DateType()) \
    .addColumn("Jour", IntegerType()) \
    .addColumn("Mois", IntegerType()) \
    .addColumn("Annee", IntegerType()) \
    .addColumn("Heure", IntegerType()) \
    .addColumn("Minute", IntegerType()) \
    .addColumn("Seconde", IntegerType()) \
    .execute()
    


# ### üìÖ Cr√©ation et transformation de la table `Dim_Date_Ouverture`
# 
# Dans cette √©tape, nous pr√©parons les donn√©es pour cr√©er une table de dimension `Dim_Date_Ouverture`. Cette table sera utilis√©e pour stocker les informations d√©taill√©es sur la date et l'heure d'ouverture des tickets, et nous la pr√©parons √† l'aide de transformations sp√©cifiques sur la colonne `Ouvert` (timestamp).
# 
# #### Processus de transformation :
# 
# 1. **Suppression des doublons** : Nous utilisons la m√©thode `dropDuplicates` pour √©liminer les doublons dans la colonne `Ouvert`. Cela garantit qu'il n'y ait qu'une seule entr√©e par date d'ouverture.
# 2. **Cr√©ation de nouvelles colonnes** : Nous extrayons plusieurs √©l√©ments temporels de la colonne `Ouvert` :
#    - `Jour` : Le jour du mois de l'ouverture (`dayofmonth`).
#    - `Mois` : Le mois de l'ouverture (`month`).
#    - `Annee` : L'ann√©e de l'ouverture (`year`).
#    - `Heure`, `Minute`, `Seconde` : L'heure, la minute, et la seconde de l'ouverture, respectivement.
# 3. **Tri des r√©sultats** : Le DataFrame est ensuite tri√© par la colonne `Ouvert` pour assurer une organisation chronologique des donn√©es.
# #### Aper√ßu des donn√©es :
# 
# Le code utilise la m√©thode `show(10)` pour afficher les 10 premi√®res lignes du DataFrame `dfdimDate_Ouverture`, ce qui permet de pr√©visualiser les r√©sultats et v√©rifier que les transformations ont √©t√© appliqu√©es correctement.
# 
# Cette table de dimension `Dim_Date_Ouverture` sera utilis√©e dans le mod√®le de donn√©es en √©toile, o√π elle sera li√©e √† une table de faits.
# 

# In[5]:


# On part du principe que df contient les colonnes "Ouvert" et "Ovrt_h"
dfdimDate_Ouverture = df.dropDuplicates(["Ouvert"]).select(
    col("Ouvert"),
    dayofmonth("Ouvert").alias("Jour"),
    month("Ouvert").alias("Mois"),
    year("Ouvert").alias("Annee"),
    split(col("Ovrt_h"), ":")[0].cast("int").alias("Heure"),
    split(col("Ovrt_h"), ":")[1].cast("int").alias("Minute"),
    split(col("Ovrt_h"), ":")[2].cast("int").alias("Seconde")
).orderBy("Ouvert")

dfdimDate_Ouverture.show(10)


# ### üîÑ Mise √† jour ou insertion dans la table Delta `dim_date_ouverture`
# 
# Dans cette √©tape, nous effectuons une op√©ration de **merge** entre le DataFrame `dfdimDate_Ouverture` (contenant les nouvelles donn√©es) et la table Delta `dim_date_ouverture` (qui contient les donn√©es existantes). Cette op√©ration est effectu√©e dans deux sc√©narios :
# 
# 1. **Mise √† jour des lignes existantes** : Si une ligne dans la table `dim_date_ouverture` correspond √† une ligne du DataFrame sur la base de la colonne `Ouvert`, les colonnes `Jour`, `Mois`, `Annee`, `Heure`, `Minute`, et `Seconde` de la table Delta sont mises √† jour avec les nouvelles valeurs du DataFrame.
#    
# 2. **Insertion de nouvelles lignes** : Si une ligne n'existe pas d√©j√† dans la table `dim_date_ouverture` (bas√©e sur la colonne `Ouvert`), de nouvelles lignes sont ins√©r√©es dans la table avec les valeurs correspondantes du DataFrame.
# 
# L'op√©ration `merge` est un moyen efficace de maintenir la table de dimension √† jour avec de nouvelles donn√©es tout en √©vitant les duplications. Cela garantit que la table contient les informations les plus r√©centes et √©vite d'avoir √† r√©√©crire toutes les donn√©es.
# 
# #### Fonctionnement du code :
# 
# - La table Delta est r√©f√©renc√©e avec `DeltaTable.forPath()`.
# - Une op√©ration `merge` est ensuite effectu√©e entre la table existante (`gold`) et les nouvelles donn√©es (`updates`).
# - Le match est effectu√© sur la colonne `Ouvert` (date d'ouverture).
# - En cas de correspondance (`whenMatchedUpdate`), les colonnes de la table `dim_date_ouverture` sont mises √† jour.
# - En cas de non-correspondance (`whenNotMatchedInsert`), de nouvelles lignes sont ins√©r√©es.
# 
# Cette op√©ration garantit que la table de dimension reste synchronis√©e avec les donn√©es entrantes et est pr√™te √† √™tre utilis√©e dans le mod√®le en √©toile pour les jointures avec d'autres tables de faits.
# 

# In[6]:


# R√©f√©rence √† la table Delta existante
deltaTable = DeltaTable.forPath(spark,'Tables/dim_date_ouverture')

# Pr√©paration des donn√©es √† ins√©rer
dfUpdates = dfdimDate_Ouverture

# Op√©ration MERGE : mise √† jour si existant, sinon insertion
deltaTable.alias('gold') \
    .merge(
        dfUpdates.alias('updates'),
        'gold.Ouvert = updates.Ouvert'  # Correspondance sur la colonne timestamp
    ) \
    .whenMatchedUpdate(set={
        'Jour': 'updates.Jour',
        'Mois': 'updates.Mois',
        'Annee': 'updates.Annee',
        'Heure': 'updates.Heure',
        'Minute': 'updates.Minute',
        'Seconde': 'updates.Seconde'
    }) \
    .whenNotMatchedInsert(values={
        'Ouvert': 'updates.Ouvert',
        'Jour': 'updates.Jour',
        'Mois': 'updates.Mois',
        'Annee': 'updates.Annee',
        'Heure': 'updates.Heure',
        'Minute': 'updates.Minute',
        'Seconde': 'updates.Seconde'
    }) \
    .execute()


# In[7]:


# Charger la table Delta dans un DataFrame
df_loaded = spark.read.format("delta").table("refined_gold.Dim_Date_Ouverture")

# Afficher les 10 premi√®res lignes de la table charg√©e
df_loaded.show(10)


# ### üïí Transformation des donn√©es pour la table de dimension `Dim_Date_MAJ`
# 
# Dans cette √©tape, nous cr√©ons le DataFrame `dfdimDate_MAJ` en appliquant diverses transformations √† la colonne `Mis_a_jour` de notre DataFrame source. Cette table de dimension contient des informations sur la date et l'heure de mise √† jour des tickets.
# 
# #### Objectifs :
# 1. **Suppression des doublons** : Nous supprimons les doublons dans la colonne `Mis_a_jour` afin de ne conserver qu'une seule entr√©e pour chaque timestamp unique de mise √† jour.
# 2. **Extraction des composantes temporelles** : 
#    - Nous extrayons le jour, le mois, l'ann√©e, l'heure, la minute et la seconde de la colonne `Mis_a_jour` pour faciliter les analyses temporelles.
# 3. **S√©lection et transformation des colonnes** : 
#    - Nous ajoutons les colonnes `Jour`, `Mois`, `Annee`, `Heure`, `Minute`, et `Seconde` √† notre DataFrame √† partir de la colonne `Mis_a_jour`.
# 4. **Tri des donn√©es** : Les donn√©es sont tri√©es par la colonne `Mis_a_jour` pour faciliter l'organisation temporelle.
# 
# #### Affichage des premi√®res lignes :
# Nous utilisons la m√©thode `show(10)` pour afficher les 10 premi√®res lignes du DataFrame transform√© et v√©rifier que les transformations ont √©t√© effectu√©es correctement.
# 
# #### Utilisation future :
# Cette table pourra √™tre utilis√©e pour effectuer des jointures avec d'autres tables dans le cadre de l'analyse des mises √† jour des tickets ou d'autres processus d√©cisionnels dans notre mod√®le en √©toile.
# 
# 

# In[8]:


# Cr√©ation de la table de dimension Dim_Date_Ouverture
DeltaTable.createIfNotExists(spark) \
    .tableName("refined_gold.Dim_Date_maj") \
    .addColumn("Mis_a_jour", DateType()) \
    .addColumn("Jour", IntegerType()) \
    .addColumn("Mois", IntegerType()) \
    .addColumn("Annee", IntegerType()) \
    .addColumn("Heure", IntegerType()) \
    .addColumn("Minute", IntegerType()) \
    .addColumn("Seconde", IntegerType()) \
    .execute()


# In[9]:


# On part du principe que df contient les colonnes "Ouvert" et "Ovrt_h"
dfdimDate_maj = df.dropDuplicates(["Mis_a_jour"]).select(
    col("Mis_a_jour"),
    dayofmonth("Mis_a_jour").alias("Jour"),
    month("Mis_a_jour").alias("Mois"),
    year("Mis_a_jour").alias("Annee"),
    split(col("Mis_h"), ":")[0].cast("int").alias("Heure"),
    split(col("Mis_h"), ":")[1].cast("int").alias("Minute"),
    split(col("Mis_h"), ":")[2].cast("int").alias("Seconde")
).orderBy("Mis_a_jour")

dfdimDate_maj.show(10)


# ### üîÑ Fusion des donn√©es pour la table de dimension `Dim_Date_MAJ`
# 
# Dans cette √©tape, nous utilisons la commande **MERGE** de Delta Lake pour mettre √† jour ou ins√©rer des donn√©es dans la table de dimension `Dim_Date_MAJ`.
# 
# #### Objectifs :
# 1. **Mise √† jour de la table Delta** : 
#    - Nous mettons √† jour les enregistrements existants dans la table `Dim_Date_MAJ` si les dates de mise √† jour (`Mis_a_jour`) correspondent. Cela permet d'assurer que les valeurs des colonnes `Jour`, `Mois`, `Annee`, `Heure`, `Minute`, et `Seconde` sont √† jour.
#    
# 2. **Insertion de nouvelles donn√©es** :
#    - Si un enregistrement n'existe pas dans la table, il sera ins√©r√© avec les valeurs appropri√©es pour `Mis_a_jour`, `Jour`, `Mois`, `Annee`, `Heure`, `Minute`, et `Seconde`.
# 
# #### D√©tails de la fusion (MERGE) :
# - **Condition de correspondance** : Nous avons bas√© la correspondance sur la colonne `Mis_a_jour` afin de v√©rifier les lignes existantes dans la table.
# - **Mise √† jour des lignes existantes** : Si une ligne existe d√©j√† avec la m√™me valeur dans `Mis_a_jour`, les autres colonnes seront mises √† jour avec les valeurs correspondantes des nouvelles donn√©es.
# - **Insertion des nouvelles lignes** : Si aucune ligne correspondante n'est trouv√©e, une nouvelle ligne est ins√©r√©e dans la table avec les nouvelles valeurs.
# 
# #### R√©sultat :
# Cette op√©ration assure que la table `Dim_Date_MAJ` reste √† jour avec les derni√®res informations de mise √† jour et peut √™tre utilis√©e dans des jointures avec d'autres tables de faits pour l'analyse.
# 
# 

# In[10]:


# R√©f√©rence √† la table Delta existante
deltaTable = DeltaTable.forPath(spark,'Tables/dim_date_maj')

# Pr√©paration des donn√©es √† ins√©rer
dfUpdates = dfdimDate_maj

# Op√©ration MERGE : mise √† jour si existant, sinon insertion
deltaTable.alias('gold') \
    .merge(
        dfUpdates.alias('updates'),
        'gold.Mis_a_jour = updates.Mis_a_jour'  # Correspondance sur la colonne timestamp
    ) \
    .whenMatchedUpdate(set={
        'Jour': 'updates.Jour',
        'Mois': 'updates.Mois',
        'Annee': 'updates.Annee',
        'Heure': 'updates.Heure',
        'Minute': 'updates.Minute',
        'Seconde': 'updates.Seconde'
    }) \
    .whenNotMatchedInsert(values={
        'Mis_a_jour': 'updates.Mis_a_jour',
        'Jour': 'updates.Jour',
        'Mois': 'updates.Mois',
        'Annee': 'updates.Annee',
        'Heure': 'updates.Heure',
        'Minute': 'updates.Minute',
        'Seconde': 'updates.Seconde'
    }) \
    .execute()


# In[11]:


# Charger la table Delta dans un DataFrame
df_loaded = spark.read.format("delta").table("refined_gold.dim_date_maj")

# Afficher les 10 premi√®res lignes de la table charg√©e
df_loaded.show(10)


# ### üî® Cr√©ation de la table de dimension `Dim_Description`
# 
# Dans cette √©tape, nous cr√©ons la table de dimension **Dim_Description** dans notre **Lakehouse**. Cette table contiendra les informations li√©es aux mises en production (MEP).
# #### Objectifs :
# 1. **Cr√©ation de la table** : 
#    - Si la table **Dim_Description** n'existe pas d√©j√†, elle est cr√©√©e automatiquement.
# 
# 2. **Structure de la table** :
#    - **ID_Description** : Identifiant unique de la description, de type **LongType**.
#    - **Description** : D√©tail ou libell√© de la mise en production, de type **StringType**.
# 
# Cette table sera utilis√©e pour stocker les descriptions des MEP, facilitant ainsi l‚Äôanalyse, la cat√©gorisation, et l‚Äôenrichissement des donn√©es de faits lors de la construction du mod√®le d√©cisionnel.
# 
# #### D√©tails techniques :
# - **`DeltaTable.createIfNotExists()`** : Cette fonction v√©rifie si la table existe d√©j√† et la cr√©e dans le cas contraire. Elle permet de d√©finir la structure de la table avec pr√©cision et garantit sa coh√©rence au sein du Lakehouse.
# - **Colonnes** :
#    - `ID_Description` : Sert d‚Äôidentifiant technique unique pour chaque enregistrement.
#    - `Description` : Contient le libell√© explicatif de la MEP.
# 
# Une fois cette table cr√©√©e, elle pourra √™tre utilis√©e dans les processus de transformation et de mod√©lisation, notamment pour faire le lien avec les donn√©es op√©rationnelles ou analytiques li√©es aux d√©ploiements et √©volutions.
# 

# In[12]:


# Cr√©ation de la table de dimension Dim_Description
DeltaTable.createIfNotExists(spark) \
    .tableName("refined_gold.Dim_descriptions") \
    .addColumn("ID_description", LongType()) \
    .addColumn("Description_breve", StringType()) \
    .execute()


# ### üîÑ Suppression des doublons pour la table de dimension `Dim_Description`
# 
# Dans cette √©tape, nous nous concentrons sur la suppression des doublons dans la colonne **`Description`** de notre DataFrame. L'objectif est de garantir qu'il n'y ait pas de valeurs dupliqu√©es avant d'utiliser ces donn√©es pour construire la table de dimension **`Dim_Description`**. 
# 
# Nous utilisons la m√©thode **`dropDuplicates()`** afin d‚Äô√©liminer les enregistrements redondants et de s'assurer que chaque description de mise en production (MEP) est unique dans la table.
# 
# Cette √©tape est essentielle pour maintenir l'int√©grit√© des donn√©es et √©viter les anomalies lors des jointures ou des analyses ult√©rieures.
# 

# In[13]:


# Suppression des doublons sur la colonne 'Description_breve'
dfdimDescription= df.dropDuplicates(["Description_breve"]).select(col("Description_breve"))
# Affichage des premi√®res lignes du DataFrame
dfdimDescription.show(10)


# ### üîÑ Identification et ajout de nouvelles descriptions √† la table de dimension `Dim_Description`
# 
# Ce processus permet de traiter les descriptions existantes et d‚Äôajouter les nouvelles descriptions de mises en production (MEP) qui ne figurent pas encore dans la table de dimension **`Dim_Description`**. Voici les principales √©tapes r√©alis√©es :
# 
# #### üß© Chargement de la table existante :
# Nous commen√ßons par charger la table **`Dim_Description`** existante depuis le Delta Lake, afin de r√©cup√©rer les descriptions d√©j√† enregistr√©es.
# 
# #### üîç R√©cup√©ration du dernier ID :
# Nous r√©cup√©rons la valeur maximale de la colonne **`ID_Description`** dans la table existante, ce qui permettra d‚Äôattribuer de nouveaux identifiants uniques aux nouvelles descriptions √† ins√©rer.
# 
# #### üßπ Extraction des descriptions uniques :
# √Ä partir du DataFrame source (`df`), nous extrayons les valeurs distinctes de la colonne **`Description`**, en supprimant les doublons.
# 
# #### üß† Identification des nouvelles descriptions :
# Nous comparons les descriptions extraites avec celles d√©j√† pr√©sentes dans la table **`Dim_Description`**, afin d‚Äôidentifier uniquement celles qui n‚Äôont pas encore √©t√© enregistr√©es.
# 
# #### üÜî Attribution d‚Äôun identifiant unique :
# Les nouvelles descriptions identifi√©es se voient attribuer un
# 

# In[14]:


# Charger la table existante Dim_description
dfdimDescription_temp = spark.read.table("refined_gold.Dim_descriptions")

# R√©cup√©rer le dernier ID_description existant
MAXDescriptionID = dfdimDescription_temp.select(coalesce(max(col("ID_description")), lit(0)).alias("MAXDescriptionID")).first()[0]

# Extraire les descriptions uniques de df et les comparer avec la table existante
dfdimDescription_silver = df.dropDuplicates(["Description_breve"]).select(col("Description_breve"))

# Identifier les nouvelles descriptions qui n'existent pas encore dans Dim_description
dfdimDescription_gold = dfdimDescription_silver.join(dfdimDescription_temp, dfdimDescription_silver.Description_breve == dfdimDescription_temp.Description_breve, "left_anti")

# Ajouter un ID_description unique aux nouvelles descriptions
dfdimDescription_gold = dfdimDescription_gold.withColumn("ID_description", monotonically_increasing_id() + MAXDescriptionID + 1)

# Afficher les 10 premi√®res lignes
dfdimDescription_gold.show(10)


# ### üîÑ Insertion des nouvelles descriptions dans la table `Dim_Description`
# 
# Cette √©tape permet d‚Äôajouter les nouvelles descriptions √† la table de dimension **`Dim_Description`** en utilisant une op√©ration **MERGE** dans Delta Lake. Voici les √©tapes r√©alis√©es :
# 
# 1. **R√©f√©rence √† la table Delta existante** : Nous acc√©dons √† la table Delta existante **`Dim_Description`** via son chemin dans le syst√®me de fichiers (Delta Lake).
# 
# 2. **Pr√©paration des donn√©es √† ins√©rer** : Un DataFrame nomm√© **`dfdimDescription_gold`** est pr√©par√©, contenant les nouvelles descriptions √† ins√©rer, accompagn√©es de leur identifiant unique **`ID_Description`**.
# 
# 3. **Op√©ration MERGE** : Gr√¢ce √† la m√©thode **`merge()`**, nous effectuons une op√©ration de type *upsert* (mise √† jour ou insertion). Si la description existe d√©j√† dans la table, aucune modification n‚Äôest apport√©e. Si elle est absente, elle est ins√©r√©e.
# 
# 4. **Insertion des nouvelles descriptions** : Les descriptions absentes de la table seront ins√©r√©es avec les colonnes **`Description`** et **`ID_Description`**, garantissant ainsi l‚Äôunicit√© et la tra√ßabilit√© des entr√©es.
# 

# In[15]:


#### Code de mise √† jour et insertion :

# R√©f√©rence √† la table Delta existante Dim_Description
deltaTable = DeltaTable.forPath(spark, 'Tables/dim_descriptions')

# Pr√©paration des donn√©es √† ins√©rer   
dfUpdates = dfdimDescription_gold

# Op√©ration MERGE : mise √† jour si existant, sinon insertion
deltaTable.alias('gold') \
  .merge(
    dfUpdates.alias('updates'),
    'gold.Description_breve = updates.Description_breve'
  ) \
   .whenMatchedUpdate(set =
    {
          
    }
  ) \
 .whenNotMatchedInsert(values =
    {
      "Description_breve": "updates.Description_breve",
      "ID_description": "updates.ID_description"
    }
  ) \
  .execute()


# ### üî® Cr√©ation de la table de dimension `Dim_Collaborateur`
# 
# Dans cette √©tape, nous cr√©ons la table de dimension **Dim_Collaborateur** dans notre **Lakehouse**. Cette table contiendra les informations relatives aux collaborateurs impliqu√©s dans les diff√©rents processus (ex. : ouverture de tickets, ex√©cution des MEP, etc.).
# 
# #### Objectifs :
# 1. **Cr√©ation de la table** :  
#    - Si la table **Dim_Collaborateur** n‚Äôexiste pas encore, elle est automatiquement cr√©√©e.
# 
# 2. **Structure de la table** :
#    - **ID_Collaborateur** : Identifiant unique du collaborateur, de type **LongType**.
#    - **Ouvert par** : Nom complet ou identifiant du collaborateur, de type **StringType**.
# 
# Cette table facilitera l‚Äôanalyse des actions men√©es par les collaborateurs, permettra une meilleure tra√ßabilit√© des op√©rations et constituera une r√©f√©rence lors de la construction du mod√®le d√©cisionnel.
# 
# #### D√©tails techniques :
# - **`DeltaTable.createIfNotExists()`** : Cette m√©thode permet de v√©rifier si la table existe d√©j√† et la cr√©e si besoin. Elle d√©finit aussi la structure des colonnes et garantit leur int√©grit√© dans l‚Äôenvironnement Delta Lake.
# - **Colonnes** :
#    - `ID_Collaborateur` : Sert d‚Äôidentifiant technique unique pour chaque collaborateur.
#    - `Ouvert par` : Contient le nom du collaborateur ou son identifiant dans le syst√®me source.
# 
# Une fois cette table en place, elle pourra √™tre int√©gr√©e dans les pipelines de transformation et utilis√©e dans les jointures avec d‚Äôautres entit√©s (tickets, projets, t√¢ches, etc.) pour enrichir les analyses d√©cisionnelles.
# 

# In[16]:


# Cr√©ation de la table de dimension Dim_Collaborateurs
DeltaTable.createIfNotExists(spark) \
    .tableName("refined_gold.Dim_Collaborateur") \
    .addColumn("ID_Collaborateur", LongType()) \
    .addColumn("Ouvert_par", StringType()) \
    .execute()


# ### üîÑ Suppression des doublons pour la table de dimension `Dim_Collaborateur`
# 
# Dans cette √©tape, nous nous concentrons sur la suppression des doublons dans la colonne **`Ouvert par`** de notre DataFrame. L'objectif est de garantir qu'il n'y ait pas de valeurs dupliqu√©es avant d'utiliser ces donn√©es pour construire la table de dimension **`Dim_Collaborateur`**. 
# 
# Nous utilisons la m√©thode **`dropDuplicates()`** afin d‚Äô√©liminer les enregistrements redondants et de s'assurer que chaque collaborateur est unique dans la table.
# 
# Cette √©tape est essentielle pour maintenir l'int√©grit√© des donn√©es et √©viter les anomalies lors des jointures ou des analyses ult√©rieures.
# 

# In[17]:


# Suppression des doublons sur la colonne 'Ouvert_par'
dfdimCollaborateurs = df.dropDuplicates(["Ouvert_par"]).select(col("Ouvert_par"))
# Affichage des premi√®res lignes du DataFrame
dfdimCollaborateurs.show(10)


# ### üîÑ Identification et ajout de nouveaux collaborateurs √† la table de dimension `Dim_Collaborateur`
# 
# Ce processus permet de traiter les collaborateurs existants et d‚Äôajouter ceux qui ne figurent pas encore dans la table de dimension **`Dim_Collaborateur`**. Voici les principales √©tapes r√©alis√©es :
# 
# #### üß© Chargement de la table existante :
# Nous commen√ßons par charger la table **`Dim_Collaborateur`** existante depuis le Delta Lake, afin de r√©cup√©rer les collaborateurs d√©j√† enregistr√©s.
# 
# #### üîç R√©cup√©ration du dernier ID :
# Nous r√©cup√©rons la valeur maximale de la colonne **`ID_Collaborateur`** dans la table existante, ce qui permettra d‚Äôattribuer de nouveaux identifiants uniques aux nouveaux collaborateurs √† ins√©rer.
# 
# #### üßπ Extraction des collaborateurs uniques :
# √Ä partir du DataFrame source (`df`), nous extrayons les valeurs distinctes de la colonne **'Ouvert par'**, en supprimant les doublons.
# 
# #### üß† Identification des nouveaux collaborateurs :
# Nous comparons les collaborateurs extraits avec ceux d√©j√† pr√©sents dans la table **`Dim_Collaborateur`**, afin d‚Äôidentifier uniquement ceux qui n‚Äôont pas encore √©t√© enregistr√©s.
# 
# #### üÜî Attribution d‚Äôun identifiant unique :
# Les nouveaux collaborateurs identifi√©s se voient attribuer un **`ID_Collaborateur`** unique, en continuant la num√©rotation √† partir du dernier identifiant existant.
# 
# #### üìä Affichage des r√©sultats :
# Enfin, nous affichons les **10 premi√®res lignes** du DataFrame contenant les nouveaux collaborateurs pr√™ts √† √™tre ins√©r√©s dans la table de dimension.
# 

# In[18]:


# Charger la table existante Dim_Collaborateurs
dfdimCollaborateurs_temp = spark.read.table("refined_gold.Dim_Collaborateur")

# R√©cup√©rer le dernier ID_Collaborateur existant
MAXCollaborateurID = dfdimCollaborateurs_temp.select(coalesce(max(col("ID_Collaborateur")), lit(0)).alias("MAXCollaborateurID")).first()[0]

# Extraire les collaborateurs uniques de df et les comparer avec la table existante
dfdimCollaborateurs_silver = df.dropDuplicates(["Ouvert_par"]).select(col("Ouvert_par"))

# Identifier les nouveaux collaborateurs qui n'existent pas encore dans Dim_Collaborateurs
dfdimCollaborateurs_gold = dfdimCollaborateurs_silver.join(
    dfdimCollaborateurs_temp,
    dfdimCollaborateurs_silver["Ouvert_par"] == dfdimCollaborateurs_temp["Collaborateur"],
    "left_anti"
)

# Ajouter un ID_Collaborateur unique aux nouveaux collaborateurs
dfdimCollaborateurs_gold = dfdimCollaborateurs_gold.withColumn("ID_Collaborateur", monotonically_increasing_id() + MAXCollaborateurID + 1)

# Afficher les 10 premi√®res lignes
dfdimCollaborateurs_gold.show(10)


# ### üîÑ Insertion des nouveaux collaborateurs dans la table `Dim_Collaborateur`
# 
# Cette √©tape permet d‚Äôajouter les nouveaux collaborateurs √† la table de dimension **`Dim_Collaborateur`** en utilisant une op√©ration **MERGE** dans Delta Lake. Voici les √©tapes r√©alis√©es :
# 
# 1. **R√©f√©rence √† la table Delta existante** : Nous acc√©dons √† la table Delta existante **`Dim_Collaborateur`** via son chemin dans le syst√®me de fichiers (Delta Lake).
# 
# 2. **Pr√©paration des donn√©es √† ins√©rer** : Un DataFrame nomm√© **`dfdimCollaborateur_gold`** est pr√©par√©, contenant les nouveaux collaborateurs √† ins√©rer, accompagn√©s de leur identifiant unique **`ID_Collaborateur`**.
# 
# 3. **Op√©ration MERGE** : Gr√¢ce √† la m√©thode **`merge()`**, nous effectuons une op√©ration de type *upsert* (mise √† jour ou insertion). Si le collaborateur existe d√©j√† dans la table, aucune modification n‚Äôest apport√©e. Si elle est absente, elle est ins√©r√©e.
# 
# 4. **Insertion des nouveaux collaborateurs** : Les collaborateurs absents de la table seront ins√©r√©s avec les colonnes **`Ouvert par`** et **`ID_Collaborateur`**, garantissant ainsi l‚Äôunicit√© et la tra√ßabilit√© des entr√©es.
# 

# In[19]:


#### Code de mise √† jour et insertion :

# R√©f√©rence √† la table Delta existante Dim_Societe
deltaTable = DeltaTable.forPath(spark, 'Tables/dim_collaborateur')

# Pr√©paration des donn√©es √† ins√©rer   
dfUpdates = dfdimCollaborateurs_gold

# Op√©ration MERGE : mise √† jour si existant, sinon insertion
deltaTable.alias('gold') \
  .merge(
    dfUpdates.alias('updates'),
    'gold.Collaborateur = updates.Ouvert_par'
  ) \
   .whenMatchedUpdate(set =
    {
          
    }
  ) \
 .whenNotMatchedInsert(values =
    {
      "Collaborateur": "updates.Ouvert_par",
      "ID_Collaborateur": "updates.ID_Collaborateur"
    }
  ) \
  .execute()


# ### üõ†Ô∏è Renommage de colonne dans la table `Dim_Collaborateur`
# 
# Dans cette √©tape, nous effectuons une mise √† jour structurelle de la table **Dim_Collaborateur** en renommant la colonne **`Ouvert_par`** en **`Collaborateur`** pour une meilleure coh√©rence s√©mantique dans notre mod√®le de donn√©es.
# 
# #### √âtapes r√©alis√©es :
# 
# 1. **Lecture de la table Delta existante** :  
#    - Nous chargeons la table `Dim_Collaborateur` √† partir de son emplacement dans le Delta Lake, afin d‚Äôeffectuer les modifications n√©cessaires.
# 
# 2. **Renommage de la colonne** :  
#    - La colonne `Ouvert_par`, qui contient les informations sur les collaborateurs, est renomm√©e en `Collaborateur` pour am√©liorer la lisibilit√© et l'uniformit√© des noms de colonnes √† travers l‚Äôensemble du mod√®le.
# 
# 3. **√âcriture de la version mise √† jour** :  
#    - La table modifi√©e est ensuite enregistr√©e en √©crasant l‚Äôancienne version √† l‚Äôaide du mode **overwrite** avec l‚Äôoption **overwriteSchema** activ√©e. Cela permet de conserver l‚Äôint√©grit√© du sch√©ma tout en appliquant la modification.
# 
# Cette op√©ration garantit que la structure de la table reste √† jour avec les conventions de nommage adopt√©es, facilitant ainsi l'int√©gration avec d'autres tables du Lakehouse et l‚Äôanalyse dans Power BI ou d'autres outils.
# 

# In[20]:


# Lire la table actuelle
df_table = spark.read.format("delta").load("Tables/dim_collaborateur")

# Renommer la colonne
df_table_renamed = df_table.withColumnRenamed("Ouvert_par", "Collaborateur")

# √âcraser l‚Äôancienne table avec la nouvelle version
df_table_renamed.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save("Tables/dim_collaborateur")


# In[21]:


# Charger la table Delta dans un DataFrame
df_loaded = spark.read.format("delta").table("refined_gold.dim_collaborateur")

# Afficher les 10 premi√®res lignes de la table charg√©e
df_loaded.show(10)


# ### üî® Cr√©ation de la table de dimension `Dim_Plage_Horaires`
# 
# Dans cette √©tape, nous cr√©ons la table de dimension **Dim_Plage_Horaires** dans notre **Lakehouse**. Cette table contiendra les informations relatives aux plages horaires utilis√©es dans les diff√©rents processus (ex. : horaires d'ouverture de tickets, ex√©cution de MEP, etc.).
# 
# #### Objectifs :
# 1. **Cr√©ation de la table** :  
#    - Si la table **Dim_Plage_Horaires** n‚Äôexiste pas encore, elle est automatiquement cr√©√©e.
# 
# 2. **Structure de la table** :
#    - **ID_Plage_Horaires** : Identifiant unique de la plage horaire, de type **LongType**.
#    - **Plages_Horaires** : Libell√© ou description de la tranche horaire, de type **StringType**.
# 
# Cette table facilitera l‚Äôanalyse temporelle des activit√©s, permettra une meilleure organisation des √©v√©nements et constituera une r√©f√©rence solide pour la construction du mod√®le d√©cisionnel.
# 
# #### D√©tails techniques :
# - **`DeltaTable.createIfNotExists()`** : Cette m√©thode permet de v√©rifier si la table existe d√©j√† et la cr√©e si besoin. Elle d√©finit √©galement la structure des colonnes et garantit leur int√©grit√© dans l‚Äôenvironnement Delta Lake.
# - **Colonnes** :
#    - `ID_Plage_Horaire` : Sert d‚Äôidentifiant technique unique pour chaque plage horaire.
#    - `Plage_Horaire` : Contient le descriptif de la tranche horaire ou la p√©riode d‚Äôactivit√©.
# 
# Une fois cette table en place, elle pourra √™tre int√©gr√©e dans les pipelines de transformation et utilis√©e dans les jointures avec d‚Äôautres entit√©s (tickets, activit√©s, incidents, etc.) pour enrichir les analyses temporelles.
# 

# In[22]:


# Cr√©ation de la table de dimension Dim_Plage_Horaires
DeltaTable.createIfNotExists(spark) \
    .tableName("refined_gold.Dim_Plage_Horaires") \
    .addColumn("ID_Plage_Horaires", LongType()) \
    .addColumn("Plages_horaires", StringType()) \
    .execute()


# ### üîÑ Suppression des doublons pour la table de dimension `Dim_Plage_Horaires`
# 
# Dans cette √©tape, nous nous concentrons sur la suppression des doublons dans la colonne **`Plage_Horaire`** de notre DataFrame. L'objectif est de garantir qu'il n'y ait pas de valeurs dupliqu√©es avant d'utiliser ces donn√©es pour construire la table de dimension **`Dim_Plage_Horaires`**.
# 
# Nous utilisons la m√©thode **`dropDuplicates()`** afin d‚Äô√©liminer les enregistrements redondants et de s'assurer que chaque plage horaire est unique dans la table.
# 
# Cette √©tape est essentielle pour maintenir l'int√©grit√© des donn√©es et √©viter les anomalies lors des jointures ou des analyses temporelles ult√©rieures.
# 

# In[23]:


# Suppression des doublons sur la colonne 'Ouvert_par'
dfdimPlageHoraire_silver = df.dropDuplicates(["Plages_horaires"]).select(col("Plages_horaires"))
# Affichage des premi√®res lignes du DataFrame
dfdimPlageHoraire_silver.show(10)


# ### üîÑ Identification et ajout de nouvelles plages horaires √† la table de dimension `Dim_Plage_Horaires`
# 
# Ce processus permet de traiter les plages horaires existantes et d‚Äôajouter celles qui ne figurent pas encore dans la table de dimension **`Dim_Plage_Horaires`**. Voici les principales √©tapes r√©alis√©es :
# 
# #### üß© Chargement de la table existante :
# Nous commen√ßons par charger la table **`Dim_Plage_Horaires`** existante depuis le Delta Lake, afin de r√©cup√©rer les plages horaires d√©j√† enregistr√©es.
# 
# #### üîç R√©cup√©ration du dernier ID :
# Nous r√©cup√©rons la valeur maximale de la colonne **`ID_Plage_Horaire`** dans la table existante, ce qui permettra d‚Äôattribuer de nouveaux identifiants uniques aux nouvelles plages horaires √† ins√©rer.
# 
# #### üßπ Extraction des plages horaires uniques :
# √Ä partir du DataFrame source (`df`), nous extrayons les valeurs distinctes de la colonne **`Plage_Horaire`**, en supprimant les doublons.
# 
# #### üß† Identification des nouvelles plages horaires :
# Nous comparons les plages horaires extraites avec celles d√©j√† pr√©sentes dans la table **`Dim_Plage_Horaires`**, afin d‚Äôidentifier uniquement celles qui n‚Äôont pas encore √©t√© enregistr√©es.
# 
# #### üÜî Attribution d‚Äôun identifiant unique :
# Les nouvelles plages horaires identifi√©es se voient attribuer un **`ID_Plage_Horaire`** unique, en continuant la num√©rotation √† partir du dernier identifiant existant.
# 
# #### üìä Affichage des r√©sultats :
# Enfin, nous affichons les **10 premi√®res lignes** du DataFrame contenant les nouvelles plages horaires pr√™tes √† √™tre ins√©r√©es dans la table de dimension.
# 

# In[24]:


# Charger la table existante Dim_Collaborateurs
dfdimPlageHoraire_temp = spark.read.table("refined_gold.Dim_Plage_Horaires")

# R√©cup√©rer le dernier ID_Collaborateur existant
MAXPlageHoraireID = dfdimPlageHoraire_temp.select(coalesce(max(col("ID_Plage_Horaires")), lit(0)).alias("MAXPlageHoraireID")).first()[0]

# Extraire les collaborateurs uniques de df et les comparer avec la table existante
dfdimPlageHoraire_silver = df.dropDuplicates(["Plages_horaires"]).select(col("Plages_horaires"))

# Identifier les nouveaux collaborateurs qui n'existent pas encore dans Dim_Collaborateurs
dfdimPlageHoraire_gold = dfdimPlageHoraire_silver.join(
    dfdimPlageHoraire_temp,
    dfdimPlageHoraire_silver.Plages_horaires== dfdimPlageHoraire_temp.Plages_horaires,
    "left_anti"
)

# Ajouter un ID_Collaborateur unique aux nouveaux collaborateurs
dfdimPlageHoraire_gold = dfdimPlageHoraire_gold.withColumn("ID_Plage_Horaires", monotonically_increasing_id() + MAXPlageHoraireID + 1)

# Afficher les 10 premi√®res lignes
dfdimPlageHoraire_gold.show(10)


# ### üîÑ Insertion des nouvelles plages horaires dans la table `Dim_Plage_Horaires`
# 
# Cette √©tape permet d‚Äôajouter les nouvelles plages horaires √† la table de dimension **`Dim_Plage_Horaires`** en utilisant une op√©ration **MERGE** dans Delta Lake. Voici les √©tapes r√©alis√©es :
# 
# 1. **R√©f√©rence √† la table Delta existante** : Nous acc√©dons √† la table Delta existante **`Dim_Plage_Horaires`** via son chemin dans le syst√®me de fichiers (Delta Lake).
# 
# 2. **Pr√©paration des donn√©es √† ins√©rer** : Un DataFrame nomm√© **`dfdimPlageHoraires_gold`** est pr√©par√©, contenant les nouvelles plages horaires √† ins√©rer, accompagn√©es de leur identifiant unique **`ID_Plage_Horaire`**.
# 
# 3. **Op√©ration MERGE** : Gr√¢ce √† la m√©thode **`merge()`**, nous effectuons une op√©ration de type *upsert* (mise √† jour ou insertion). Si la plage horaire existe d√©j√† dans la table, aucune modification n‚Äôest apport√©e. Si elle est absente, elle est ins√©r√©e.
# 
# 4. **Insertion des nouvelles plages horaires** : Les plages horaires absentes de la table seront ins√©r√©es avec les colonnes **`Plage_Horaire`** et **`ID_Plage_Horaire`**, garantissant ainsi l‚Äôunicit√© et la tra√ßabilit√© des entr√©es.
# 

# In[25]:


#### Code de mise √† jour et insertion :

# R√©f√©rence √† la table Delta existante Dim_Description
deltaTable = DeltaTable.forPath(spark, 'Tables/dim_plage_horaires')

# Pr√©paration des donn√©es √† ins√©rer   
dfUpdates = dfdimPlageHoraire_gold

# Op√©ration MERGE : mise √† jour si existant, sinon insertion
deltaTable.alias('gold') \
  .merge(
    dfUpdates.alias('updates'),
    'gold.Plages_horaires = updates.Plages_horaires'
  ) \
   .whenMatchedUpdate(set =
    {
          
    }
  ) \
 .whenNotMatchedInsert(values =
    {
      "Plages_horaires": "updates.Plages_horaires",
      "ID_Plage_Horaires": "updates.ID_Plage_Horaires"
    }
  ) \
  .execute()


# In[26]:


# Charger la table Delta dans un DataFrame
df_loaded = spark.read.format("delta").table("refined_gold.dim_plage_horaires")

# Afficher les 10 premi√®res lignes de la table charg√©e
df_loaded.show(10)


# # Les Demandes

# #### √âtapes de la transformation
# 
# 1. **Chargement des donn√©es depuis la couche Silver :**  
#    Les donn√©es sont lues depuis la table `cleansed_Silver.demandes_silver`, qui contient les informations nettoy√©es issues du traitement de la couche Bronze. Cette table constitue une base fiable pour effectuer des enrichissements et transformations suppl√©mentaires dans les couches sup√©rieures du Lakehouse.
# 

# In[27]:


dfd = spark.read.table("cleansed_Silver.demandes_silver")


# In[28]:


display(dfd.head(10))


# ### üõ†Ô∏è Cr√©ation de la table de dimension `Dim_Date_Ouverture_d`
# 
# Dans cette √©tape, nous cr√©ons une nouvelle table de dimension `Dim_Date_Ouverture_d` dans le lakehouse. Cette table sera utilis√©e pour stocker des informations sur les dates d'ouverture des tickets dans une structure dimensionnelle.
# 
# #### Processus de cr√©ation :
# 
# 1. **V√©rification de l'existence de la table** : Nous utilisons la m√©thode `createIfNotExists` pour garantir que la table est cr√©√©e uniquement si elle n'existe pas d√©j√†.
# 2. **D√©finition du sch√©ma** : La table contient plusieurs colonnes pour d√©tailler les informations relatives √† la date et √† l'heure d'ouverture des tickets :
#    - `Ouvert` : Le timestamp de l'ouverture du ticket.
#    - `Jour`, `Mois`, `Annee`, `Heure`, `Minute`, `Seconde` : Ces colonnes permettent de d√©composer la date d'ouverture pour une analyse plus pr√©cise.
# 3. **Ex√©cution de la cr√©ation** : La m√©thode `execute()` permet de finaliser la cr√©ation de la table.
# 
# Cette structure nous permettra de faire des jointures avec d'autres tables, comme les faits relatifs aux tickets, en utilisant des cl√©s dimensionnelles bas√©es sur la date et l'heure d'ouverture.
# 

# In[29]:


# Cr√©ation de la table de dimension Dim_Date_Ouverture
DeltaTable.createIfNotExists(spark) \
    .tableName("refined_gold.Dim_Date_Ouverture_d") \
    .addColumn("Cree", DateType()) \
    .addColumn("Jour", IntegerType()) \
    .addColumn("Mois", IntegerType()) \
    .addColumn("Annee", IntegerType()) \
    .addColumn("Heure", IntegerType()) \
    .addColumn("Minute", IntegerType()) \
    .addColumn("Seconde", IntegerType()) \
    .execute()


# ### üìÖ Cr√©ation et transformation de la table `Dim_Date_Ouverture_d`
# 
# Dans cette √©tape, nous pr√©parons les donn√©es pour cr√©er une table de dimension `Dim_Date_Ouverture_d`. Cette table sera utilis√©e pour stocker les informations d√©taill√©es sur la date et l'heure d'ouverture des tickets, et nous la pr√©parons √† l'aide de transformations sp√©cifiques sur la colonne `Ouvert` (timestamp).
# 
# #### Processus de transformation :
# 
# 1. **Suppression des doublons** : Nous utilisons la m√©thode `dropDuplicates` pour √©liminer les doublons dans la colonne `Ouvert`. Cela garantit qu'il n'y ait qu'une seule entr√©e par date d'ouverture.
# 2. **Cr√©ation de nouvelles colonnes** : Nous extrayons plusieurs √©l√©ments temporels de la colonne `Ouvert` :
#    - `Jour` : Le jour du mois de l'ouverture (`dayofmonth`).
#    - `Mois` : Le mois de l'ouverture (`month`).
#    - `Annee` : L'ann√©e de l'ouverture (`year`).
#    - `Heure`, `Minute`, `Seconde` : L'heure, la minute, et la seconde de l'ouverture, respectivement.
# 3. **Tri des r√©sultats** : Le DataFrame est ensuite tri√© par la colonne `Ouvert` pour assurer une organisation chronologique des donn√©es.
# #### Aper√ßu des donn√©es :
# 
# Le code utilise la m√©thode `show(10)` pour afficher les 10 premi√®res lignes du DataFrame `dfdimDate_Ouverture`, ce qui permet de pr√©visualiser les r√©sultats et v√©rifier que les transformations ont √©t√© appliqu√©es correctement.
# 
# Cette table de dimension `Dim_Date_Ouverture_d` sera utilis√©e dans le mod√®le de donn√©es en √©toile, o√π elle sera li√©e √† une table de faits.
# 

# In[30]:


# On part du principe que df contient les colonnes "Ouvert" et "Ovrt_h"
dfdimDate_Ouverture_d = dfd.dropDuplicates(["Cree"]).select(
    col("Cree"),
    dayofmonth("Cree").alias("Jour"),
    month("Cree").alias("Mois"),
    year("Cree").alias("Annee"),
    split(col("Cree_h"), ":")[0].cast("int").alias("Heure"),
    split(col("Cree_h"), ":")[1].cast("int").alias("Minute"),
    split(col("Cree_h"), ":")[2].cast("int").alias("Seconde")
).orderBy("Cree")

dfdimDate_Ouverture_d.show(10)


# ### üîÑ Mise √† jour ou insertion dans la table Delta `dim_date_ouverture_d`
# 
# Dans cette √©tape, nous effectuons une op√©ration de **merge** entre le DataFrame `dfdimDate_Ouverture_d` (contenant les nouvelles donn√©es) et la table Delta `dim_date_ouverture_d` (qui contient les donn√©es existantes). Cette op√©ration est effectu√©e dans deux sc√©narios :
# 
# 1. **Mise √† jour des lignes existantes** : Si une ligne dans la table `dim_date_ouverture_d` correspond √† une ligne du DataFrame sur la base de la colonne `Ouvert`, les colonnes `Jour`, `Mois`, `Annee`, `Heure`, `Minute`, et `Seconde` de la table Delta sont mises √† jour avec les nouvelles valeurs du DataFrame.
#    
# 2. **Insertion de nouvelles lignes** : Si une ligne n'existe pas d√©j√† dans la table `dim_date_ouverture_d` (bas√©e sur la colonne `Ouvert`), de nouvelles lignes sont ins√©r√©es dans la table avec les valeurs correspondantes du DataFrame.
# 
# L'op√©ration `merge` est un moyen efficace de maintenir la table de dimension √† jour avec de nouvelles donn√©es tout en √©vitant les duplications. Cela garantit que la table contient les informations les plus r√©centes et √©vite d'avoir √† r√©√©crire toutes les donn√©es.
# 
# #### Fonctionnement du code :
# 
# - La table Delta est r√©f√©renc√©e avec `DeltaTable.forPath()`.
# - Une op√©ration `merge` est ensuite effectu√©e entre la table existante (`gold`) et les nouvelles donn√©es (`updates`).
# - Le match est effectu√© sur la colonne `Ouvert` (date d'ouverture).
# - En cas de correspondance (`whenMatchedUpdate`), les colonnes de la table `dim_date_ouverture_d` sont mises √† jour.
# - En cas de non-correspondance (`whenNotMatchedInsert`), de nouvelles lignes sont ins√©r√©es.
# 
# Cette op√©ration garantit que la table de dimension reste synchronis√©e avec les donn√©es entrantes et est pr√™te √† √™tre utilis√©e dans le mod√®le en √©toile pour les jointures avec d'autres tables de faits.
# 

# In[31]:


# R√©f√©rence √† la table Delta existante
deltaTable = DeltaTable.forPath(spark,'Tables/dim_date_ouverture_d')

# Pr√©paration des donn√©es √† ins√©rer
dfUpdates = dfdimDate_Ouverture_d

# Op√©ration MERGE : mise √† jour si existant, sinon insertion
deltaTable.alias('gold') \
    .merge(
        dfUpdates.alias('updates'),
        'gold.Cree = updates.Cree'  # Correspondance sur la colonne timestamp
    ) \
    .whenMatchedUpdate(set={
        'Jour': 'updates.Jour',
        'Mois': 'updates.Mois',
        'Annee': 'updates.Annee',
        'Heure': 'updates.Heure',
        'Minute': 'updates.Minute',
        'Seconde': 'updates.Seconde'
    }) \
    .whenNotMatchedInsert(values={
        'Cree': 'updates.Cree',
        'Jour': 'updates.Jour',
        'Mois': 'updates.Mois',
        'Annee': 'updates.Annee',
        'Heure': 'updates.Heure',
        'Minute': 'updates.Minute',
        'Seconde': 'updates.Seconde'
    }) \
    .execute()


# ### üïí Transformation des donn√©es pour la table de dimension `Dim_Fermeture_D`
# 
# Dans cette √©tape, nous cr√©ons le DataFrame `dfdimDate_Fermeture_D` en appliquant diverses transformations √† la colonne `Ferme` de notre DataFrame source. Cette table de dimension contient des informations sur la date et l'heure de fermeture des tickets.
# 
# #### Objectifs :
# 1. **Suppression des doublons** : Nous supprimons les doublons dans la colonne `Ferme` afin de ne conserver qu'une seule entr√©e pour chaque timestamp unique de fermeture.
# 2. **Extraction des composantes temporelles** : 
#    - Nous extrayons le jour, le mois, l'ann√©e, l'heure, la minute et la seconde de la colonne `Ferme` pour faciliter les analyses temporelles.
# 3. **S√©lection et transformation des colonnes** : 
#    - Nous ajoutons les colonnes `Jour`, `Mois`, `Annee`, `Heure`, `Minute`, et `Seconde` √† notre DataFrame √† partir de la colonne `Ferme`.
# 4. **Tri des donn√©es** : Les donn√©es sont tri√©es par la colonne `Ferme` pour faciliter l'organisation temporelle.
# 
# #### Affichage des premi√®res lignes :
# Nous utilisons la m√©thode `show(10)` pour afficher les 10 premi√®res lignes du DataFrame transform√© et v√©rifier que les transformations ont √©t√© effectu√©es correctement.
# 
# #### Utilisation future :
# Cette table pourra √™tre utilis√©e pour effectuer des jointures avec d'autres tables dans le cadre de l'analyse des fermeture des tickets ou d'autres processus d√©cisionnels dans notre mod√®le en √©toile.
# 

# In[32]:


# Cr√©ation de la table de dimension Dim_Date_Ouverture
DeltaTable.createIfNotExists(spark) \
    .tableName("refined_gold.Dim_Fermeture_D") \
    .addColumn("Ferme", DateType()) \
    .addColumn("Jour", IntegerType()) \
    .addColumn("Mois", IntegerType()) \
    .addColumn("Annee", IntegerType()) \
    .addColumn("Heure", IntegerType()) \
    .addColumn("Minute", IntegerType()) \
    .addColumn("Seconde", IntegerType()) \
    .execute()


# ### üîÑ Fusion des donn√©es pour la table de dimension `Dim_Date_Fermeture`
# 
# Dans cette √©tape, nous utilisons la commande **MERGE** de Delta Lake pour mettre √† jour ou ins√©rer des donn√©es dans la table de dimension `Dim_Date_fermeture`.
# 
# #### Objectifs :
# 1. **Mise √† jour de la table Delta** : 
#    - Nous mettons √† jour les enregistrements existants dans la table `Dim_Date_Fermeture` si les dates de Fermeture (`ferme`) correspondent. Cela permet d'assurer que les valeurs des colonnes `Jour`, `Mois`, `Annee`, `Heure`, `Minute`, et `Seconde` sont √† jour.
#    
# 2. **Insertion de nouvelles donn√©es** :
#    - Si un enregistrement n'existe pas dans la table, il sera ins√©r√© avec les valeurs appropri√©es pour `Ferme`, `Jour`, `Mois`, `Annee`, `Heure`, `Minute`, et `Seconde`.
# 
# #### D√©tails de la fusion (MERGE) :
# - **Condition de correspondance** : Nous avons bas√© la correspondance sur la colonne `Ferme` afin de v√©rifier les lignes existantes dans la table.
# - **Mise √† jour des lignes existantes** : Si une ligne existe d√©j√† avec la m√™me valeur dans `Mis_a_jour`, les autres colonnes seront mises √† jour avec les valeurs correspondantes des nouvelles donn√©es.
# - **Insertion des nouvelles lignes** : Si aucune ligne correspondante n'est trouv√©e, une nouvelle ligne est ins√©r√©e dans la table avec les nouvelles valeurs.
# 
# #### R√©sultat :
# Cette op√©ration assure que la table `Dim_Date_MAJ` reste √† jour avec les derni√®res informations de mise √† jour et peut √™tre utilis√©e dans des jointures avec d'autres tables de faits pour l'analyse.
# 

# In[33]:


# On part du principe que df contient les colonnes "Ouvert" et "Ovrt_h"
dfdimDate_Fermeture_d = dfd.dropDuplicates(["Ferme"]).select(
    col("Ferme"),
    dayofmonth("Ferme").alias("Jour"),
    month("Ferme").alias("Mois"),
    year("Ferme").alias("Annee"),
    split(col("Ferme_h"), ":")[0].cast("int").alias("Heure"),
    split(col("Ferme_h"), ":")[1].cast("int").alias("Minute"),
    split(col("Ferme_h"), ":")[2].cast("int").alias("Seconde")
).orderBy("Ferme")

dfdimDate_Fermeture_d.show(10)


# In[34]:


# R√©f√©rence √† la table Delta existante
deltaTable = DeltaTable.forPath(spark,'Tables/dim_fermeture_d')

# Pr√©paration des donn√©es √† ins√©rer
dfUpdates = dfdimDate_Fermeture_d

# Op√©ration MERGE : mise √† jour si existant, sinon insertion
deltaTable.alias('gold') \
    .merge(
        dfUpdates.alias('updates'),
        'gold.Ferme = updates.Ferme'  # Correspondance sur la colonne timestamp
    ) \
    .whenMatchedUpdate(set={
        'Jour': 'updates.Jour',
        'Mois': 'updates.Mois',
        'Annee': 'updates.Annee',
        'Heure': 'updates.Heure',
        'Minute': 'updates.Minute',
        'Seconde': 'updates.Seconde'
    }) \
    .whenNotMatchedInsert(values={
        'Ferme': 'updates.Ferme',
        'Jour': 'updates.Jour',
        'Mois': 'updates.Mois',
        'Annee': 'updates.Annee',
        'Heure': 'updates.Heure',
        'Minute': 'updates.Minute',
        'Seconde': 'updates.Seconde'
    }) \
    .execute()


# In[35]:


# Cr√©ation de la table de dimension Dim_Description
DeltaTable.createIfNotExists(spark) \
    .tableName("refined_gold.Dim_descriptions_d") \
    .addColumn("ID_description", LongType()) \
    .addColumn("Description_breve", StringType()) \
    .execute()


# In[36]:


# Suppression des doublons sur la colonne 'Description_breve'
dfdimDescription= dfd.dropDuplicates(["Description_breve"]).select(col("Description_breve"))
# Affichage des premi√®res lignes du DataFrame
dfdimDescription.show(10)


# In[37]:


# Charger la table existante Dim_description
dfdimDescription_temp = spark.read.table("refined_gold.Dim_descriptions_d")

# R√©cup√©rer le dernier ID_description existant
MAXDescriptionID = dfdimDescription_temp.select(coalesce(max(col("ID_description")), lit(0)).alias("MAXDescriptionID")).first()[0]

# Extraire les descriptions uniques de df et les comparer avec la table existante
dfdimDescription_silver = dfd.dropDuplicates(["Description_breve"]).select(col("Description_breve"))

# Identifier les nouvelles descriptions qui n'existent pas encore dans Dim_description
dfdimDescription_gold = dfdimDescription_silver.join(dfdimDescription_temp, dfdimDescription_silver.Description_breve == dfdimDescription_temp.Description_breve, "left_anti")

# Ajouter un ID_description unique aux nouvelles descriptions
dfdimDescription_gold = dfdimDescription_gold.withColumn("ID_description", monotonically_increasing_id() + MAXDescriptionID + 1)

# Afficher les 10 premi√®res lignes
dfdimDescription_gold.show(10)


# In[38]:


#### Code de mise √† jour et insertion :

# R√©f√©rence √† la table Delta existante Dim_Description
deltaTable = DeltaTable.forPath(spark, 'Tables/dim_descriptions_d')

# Pr√©paration des donn√©es √† ins√©rer   
dfUpdates = dfdimDescription_gold

# Op√©ration MERGE : mise √† jour si existant, sinon insertion
deltaTable.alias('gold') \
  .merge(
    dfUpdates.alias('updates'),
    'gold.Description_breve = updates.Description_breve'
  ) \
   .whenMatchedUpdate(set =
    {
          
    }
  ) \
 .whenNotMatchedInsert(values =
    {
      "Description_breve": "updates.Description_breve",
      "ID_description": "updates.ID_description"
    }
  ) \
  .execute()


# In[39]:


# Charger la table Delta dans un DataFrame
df_loaded = spark.read.format("delta").table("refined_gold.dim_descriptions_d")

# Afficher les 10 premi√®res lignes de la table charg√©e
df_loaded.show(10)


# In[40]:


# Cr√©ation de la table de dimension Dim_Collaborateurs
DeltaTable.createIfNotExists(spark) \
    .tableName("refined_gold.Dim_Collaborateur_d") \
    .addColumn("ID_Collaborateur", LongType()) \
    .addColumn("Affecte_a", StringType()) \
    .execute()


# In[41]:


# Suppression des doublons sur la colonne 'Ouvert_par'
dfdimCollaborateurs = dfd.dropDuplicates(["Affecte_a"]).select(col("Affecte_a"))
# Affichage des premi√®res lignes du DataFrame
dfdimCollaborateurs.show(10)


# In[42]:


# Charger la table existante Dim_Collaborateurs
dfdimCollaborateurs_temp = spark.read.table("refined_gold.Dim_Collaborateur_d")

# R√©cup√©rer le dernier ID_Collaborateur existant
MAXCollaborateurID = dfdimCollaborateurs_temp.select(coalesce(max(col("ID_Collaborateur")), lit(0)).alias("MAXCollaborateurID")).first()[0]

# Extraire les collaborateurs uniques de df et les comparer avec la table existante
dfdimCollaborateurs_silver = dfd.dropDuplicates(["Affecte_a"]).select(col("Affecte_a"))

# Identifier les nouveaux collaborateurs qui n'existent pas encore dans Dim_Collaborateurs
dfdimCollaborateurs_gold = dfdimCollaborateurs_silver.join(
    dfdimCollaborateurs_temp,
    dfdimCollaborateurs_silver.Affecte_a == dfdimCollaborateurs_temp.Collaborateur,
    "left_anti"
)

# Ajouter un ID_Collaborateur unique aux nouveaux collaborateurs
dfdimCollaborateurs_gold = dfdimCollaborateurs_gold.withColumn("ID_Collaborateur", monotonically_increasing_id() + MAXCollaborateurID + 1)

# Afficher les 10 premi√®res lignes
dfdimCollaborateurs_gold.show(10)


# In[43]:


#### Code de mise √† jour et insertion :

# R√©f√©rence √† la table Delta existante Dim_Societe
deltaTable = DeltaTable.forPath(spark, 'Tables/dim_collaborateur_d')

# Pr√©paration des donn√©es √† ins√©rer   
dfUpdates = dfdimCollaborateurs_gold

# Op√©ration MERGE : mise √† jour si existant, sinon insertion
deltaTable.alias('gold') \
  .merge(
    dfUpdates.alias('updates'),
    'gold.Collaborateur = updates.Affecte_a'
  ) \
   .whenMatchedUpdate(set =
    {
          
    }
  ) \
 .whenNotMatchedInsert(values =
    {
      "Collaborateur": "updates.Affecte_a",
      "ID_Collaborateur": "updates.ID_Collaborateur"
    }
  ) \
  .execute()


# In[44]:


# Charger la table Delta dans un DataFrame
df_loaded = spark.read.format("delta").table("refined_gold.dim_collaborateur_d")

# Afficher les 10 premi√®res lignes de la table charg√©e
df_loaded.show(10)


# In[45]:


# Lire la table actuelle
df_table = spark.read.format("delta").load("Tables/dim_collaborateur_d")

# Renommer la colonne
df_table_renamed = df_table.withColumnRenamed("Affecte_a", "Collaborateur")

# √âcraser l‚Äôancienne table avec la nouvelle version
df_table_renamed.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save("Tables/dim_collaborateur_d")


# In[46]:


# Charger la table Delta dans un DataFrame
df_loaded = spark.read.format("delta").table("refined_gold.dim_collaborateur_d")

# Afficher les 10 premi√®res lignes de la table charg√©e
df_loaded.show(10)


# In[47]:


# Cr√©ation de la table de dimension Dim_Plage_Horaires
DeltaTable.createIfNotExists(spark) \
    .tableName("refined_gold.Dim_Plage_Horaires_D") \
    .addColumn("ID_Plage_Horaires", LongType()) \
    .addColumn("Plages_horaires", StringType()) \
    .execute()


# In[48]:


# Suppression des doublons sur la colonne 'Ouvert_par'
dfdimPlageHoraire = dfd.dropDuplicates(["Plages_horaires"]).select(col("Plages_horaires"))
# Affichage des premi√®res lignes du DataFrame
dfdimPlageHoraire.show(10)


# In[49]:


# Charger la table existante Dim_Collaborateurs
dfdimPlageHoraire_temp = spark.read.table("refined_gold.Dim_Plage_Horaires_D")

# R√©cup√©rer le dernier ID_Collaborateur existant
MAXPlageHoraireID = dfdimPlageHoraire_temp.select(coalesce(max(col("ID_Plage_Horaires")), lit(0)).alias("MAXPlageHoraireID")).first()[0]

# Extraire les collaborateurs uniques de df et les comparer avec la table existante
dfdimPlageHoraire_silver = dfd.dropDuplicates(["Plages_horaires"]).select(col("Plages_horaires"))

# Identifier les nouveaux collaborateurs qui n'existent pas encore dans Dim_Collaborateurs
dfdimPlageHoraire_gold = dfdimPlageHoraire_silver.join(
    dfdimPlageHoraire_temp,
    dfdimPlageHoraire_silver.Plages_horaires== dfdimPlageHoraire_temp.Plages_horaires,
    "left_anti"
)

# Ajouter un ID_Collaborateur unique aux nouveaux collaborateurs
dfdimPlageHoraire_gold = dfdimPlageHoraire_gold.withColumn("ID_Plage_Horaires", monotonically_increasing_id() + MAXPlageHoraireID + 1)

# Afficher les 10 premi√®res lignes
dfdimPlageHoraire_gold.show(10)


# In[50]:


#### Code de mise √† jour et insertion :

# R√©f√©rence √† la table Delta existante Dim_Description
deltaTable = DeltaTable.forPath(spark, 'Tables/dim_plage_horaires_d')

# Pr√©paration des donn√©es √† ins√©rer   
dfUpdates = dfdimPlageHoraire_gold

# Op√©ration MERGE : mise √† jour si existant, sinon insertion
deltaTable.alias('gold') \
  .merge(
    dfUpdates.alias('updates'),
    'gold.Plages_horaires = updates.Plages_horaires'
  ) \
   .whenMatchedUpdate(set =
    {
          
    }
  ) \
 .whenNotMatchedInsert(values =
    {
      "Plages_horaires": "updates.Plages_horaires",
      "ID_Plage_Horaires": "updates.ID_Plage_Horaires"
    }
  ) \
  .execute()


# In[51]:


# Charger la table Delta dans un DataFrame
df_loaded = spark.read.format("delta").table("refined_gold.dim_plage_horaires_d")

# Afficher les 10 premi√®res lignes de la table charg√©e
df_loaded.show(10)


# In[52]:


# Cr√©ation de la table de dimension Dim_Type
DeltaTable.createIfNotExists(spark) \
    .tableName("refined_gold.Dim_Type") \
    .addColumn("ID_Type", LongType()) \
    .addColumn("Type", StringType()) \
    .execute()


# In[53]:


from pyspark.sql.functions import col

# Extraire les types distincts des deux sources
df_type_dfd = dfd.select(col("Type")).dropDuplicates()
df_type_df = df.select(col("Type")).dropDuplicates()

# Union des deux et suppression des doublons globaux
dfdimType = df_type_dfd.union(df_type_df).dropDuplicates()

dfdimType.show(10)


# In[54]:


from pyspark.sql.functions import col, coalesce, max, lit, monotonically_increasing_id

# Charger la table existante Dim_Type
dfdimType_temp = spark.read.table("refined_gold.Dim_Type")

# R√©cup√©rer le dernier ID_Type existant
MAXTypeID = dfdimType_temp.select(coalesce(max(col("ID_Type")), lit(0)).alias("MAXTypeID")).first()[0]

# Extraire les types uniques des deux tables
df_type_dfd = dfd.select(col("Type")).dropDuplicates()
df_type_df = df.select(col("Type")).dropDuplicates()

# Fusionner les deux DataFrames et supprimer les doublons globaux
dfdimType_silver = df_type_dfd.union(df_type_df).dropDuplicates()

# Identifier les nouveaux types qui n'existent pas encore dans Dim_Type
dfdimType_gold = dfdimType_silver.join(
    dfdimType_temp,
    dfdimType_silver.Type == dfdimType_temp.Type,
    "left_anti"
)

# Ajouter un ID_Type unique aux nouveaux types
dfdimType_gold = dfdimType_gold.withColumn("ID_Type", monotonically_increasing_id() + MAXTypeID + 1)

# Afficher les 10 premi√®res lignes
dfdimType_gold.show(10)


# In[55]:


from delta.tables import *
#### Code de mise √† jour et insertion :

# R√©f√©rence √† la table Delta existante Dim_Description
deltaTable = DeltaTable.forPath(spark, 'Tables/dim_type')

# Pr√©paration des donn√©es √† ins√©rer   
dfUpdates = dfdimType_gold

# Op√©ration MERGE : mise √† jour si existant, sinon insertion
deltaTable.alias('gold') \
  .merge(
    dfUpdates.alias('updates'),
    'gold.Type = updates.Type'
  ) \
   .whenMatchedUpdate(set =
    {
          
    }
  ) \
 .whenNotMatchedInsert(values =
    {
      "Type": "updates.Type",
      "ID_Type": "updates.ID_Type"
    }
  ) \
  .execute()


# In[56]:


# Charger la table Delta dans un DataFrame
df_loaded_t = spark.read.format("delta").table("refined_gold.dim_type")

# Afficher les 10 premi√®res lignes de la table charg√©e
df_loaded_t.show(10)


# In[57]:


from pyspark.sql.types import *
from delta.tables import *
# Cr√©ation de la table de fait factSupervision_gold    
DeltaTable.createIfNotExists(spark) \
    .tableName("refined_gold.factMep_gold") \
    .addColumn("Numero", StringType()) \
    .addColumn("Ouvert", DateType()) \
    .addColumn("Mis_a_jour", DateType()) \
    .addColumn("ID_description", LongType()) \
    .addColumn("Etat", StringType()) \
    .addColumn("ID_Collaborateur", LongType()) \
    .addColumn("Ovrt_h", StringType()) \
    .addColumn("Mis_h", StringType()) \
    .addColumn("ID_Plage_Horaires", LongType()) \
    .addColumn("ID_Type", LongType()) \
    .execute()


# In[58]:


from pyspark.sql.functions import col
from delta.tables import *

# Charger les tables de dimensions
dfdimDescriptions = spark.read.table("refined_gold.dim_descriptions")
dfdimCollaborateur = spark.read.table("refined_gold.dim_collaborateur")
dfdimPlageHoraires = spark.read.table("refined_gold.dim_plage_horaires")
dfdimType = spark.read.table("refined_gold.dim_type")

# Joindre les tables de dimensions avec les cl√©s correspondantes
dffactMep_gold = df.alias("df1") \
    .join(dfdimDescriptions.alias("df2"), col("df1.Description_breve") == col("df2.Description_breve"), "left") \
    .join(dfdimCollaborateur.alias("df3"), col("df1.Ouvert_par") == col("df3.Collaborateur"), "left") \
    .join(dfdimPlageHoraires.alias("df4"), col("df1.Plages_horaires") == col("df4.Plages_horaires"), "left") \
    .join(dfdimType.alias("df5"), col("df1.Type") == col("df5.Type"), "left") \
    .select(
        col("df1.Numero"),
        col("df1.Ouvert"),
        col("df1.Mis_a_jour"),
        col("df1.Etat"),
        col("df1.Ovrt_h"),
        col("df1.Mis_h"),
        col("df2.ID_description"),
        col("df3.ID_Collaborateur"),
        col("df4.ID_Plage_Horaires"),
        col("df5.ID_Type")
    ) \
    .orderBy(col("df1.Ouvert"), col("df1.Numero"))

# Affichage des 10 premi√®res lignes
display(dffactMep_gold.limit(10))


# In[59]:


from delta.tables import *

# R√©f√©rence √† la table Delta de faits (√† adapter selon ton nom de table r√©el)
deltaTable = DeltaTable.forPath(spark, 'Tables/factmep_gold')

# Pr√©paration des donn√©es √† ins√©rer
dfUpdates = dffactMep_gold

# MERGE dans la table Delta : mise √† jour si le Numero existe, sinon insertion
deltaTable.alias('target') \
  .merge(
    dfUpdates.alias('updates'),
    'target.Numero = updates.Numero'
  ) \
  .whenMatchedUpdate(set =
    {
        "Ouvert": "updates.Ouvert",
        "Mis_a_jour": "updates.Mis_a_jour",
        "Etat": "updates.Etat",
        "Ovrt_h": "updates.Ovrt_h",
        "Mis_h": "updates.Mis_h",
        "ID_description": "updates.ID_description",
        "ID_Collaborateur": "updates.ID_Collaborateur",
        "ID_Plage_Horaires": "updates.ID_Plage_Horaires",
        "ID_Type": "updates.ID_Type"
    }
  ) \
  .whenNotMatchedInsert(values =
    {
        "Numero": "updates.Numero",
        "Ouvert": "updates.Ouvert",
        "Mis_a_jour": "updates.Mis_a_jour",
        "Etat": "updates.Etat",
        "Ovrt_h": "updates.Ovrt_h",
        "Mis_h": "updates.Mis_h",
        "ID_description": "updates.ID_description",
        "ID_Collaborateur": "updates.ID_Collaborateur",
        "ID_Plage_Horaires": "updates.ID_Plage_Horaires",
        "ID_Type": "updates.ID_Type"
    }
  ) \
  .execute()


# In[60]:


display(spark.read.table("refined_gold.factmep_gold"))


# In[61]:


from pyspark.sql.types import *
from delta.tables import *
# Cr√©ation de la table de fait factSupervision_gold    
DeltaTable.createIfNotExists(spark) \
    .tableName("refined_gold.factDemande_gold") \
    .addColumn("Cree", DateType()) \
    .addColumn("Ferme", DateType()) \
    .addColumn("Numero", StringType()) \
    .addColumn("ID_description", LongType()) \
    .addColumn("ID_Collaborateur", LongType()) \
    .addColumn("Task_Duration", LongType()) \
    .addColumn("Cree_h", StringType()) \
    .addColumn("Ferme_h", StringType()) \
    .addColumn("ID_Plage_Horaires", LongType()) \
    .addColumn("ID_Type", LongType()) \
    .execute()


# In[62]:


from pyspark.sql.functions import col
from delta.tables import *

# Charger les tables de dimensions
dfdimDescriptions = spark.read.table("refined_gold.dim_descriptions_d")
dfdimCollaborateur = spark.read.table("refined_gold.dim_collaborateur_d")
dfdimPlageHoraires = spark.read.table("refined_gold.dim_plage_horaires_d")
dfdimType = spark.read.table("refined_gold.dim_type")

# Joindre les tables de dimensions avec les cl√©s correspondantes
dffactDemande_gold = dfd.alias("df1") \
    .join(dfdimDescriptions.alias("df2"), col("df1.Description_breve") == col("df2.Description_breve"), "left") \
    .join(dfdimCollaborateur.alias("df3"), col("df1.Affecte_a") == col("df3.Collaborateur"), "left") \
    .join(dfdimPlageHoraires.alias("df4"), col("df1.Plages_horaires") == col("df4.Plages_horaires"), "left") \
    .join(dfdimType.alias("df5"), col("df1.Type") == col("df5.Type"), "left") \
    .select(
        col("df1.Numero"),
        col("df1.Cree"),
        col("df1.Ferme"),
        col("df1.Task_Duration"),
        col("df1.Cree_h"),
        col("df1.Ferme_h"),
        col("df2.ID_description"),
        col("df3.ID_Collaborateur"),
        col("df4.ID_Plage_Horaires"),
        col("df5.ID_Type")
    ) \
    .orderBy(col("df1.Cree"), col("df1.Numero"))

# Affichage des 10 premi√®res lignes
display(dffactDemande_gold.limit(10))


# In[63]:


from delta.tables import *

# R√©f√©rence √† la table Delta de faits (adapt√©e pour factDemande_gold)
deltaTable = DeltaTable.forName(spark, "refined_gold.factdemande_gold")

# Pr√©paration des donn√©es √† ins√©rer
dfUpdates = dffactDemande_gold

# MERGE dans la table Delta : mise √† jour si le Numero existe, sinon insertion
deltaTable.alias('target') \
  .merge(
    dfUpdates.alias('updates'),
    'target.Numero = updates.Numero'
  ) \
  .whenMatchedUpdate(set =
    {
        "Cree": "updates.Cree",
        "Ferme": "updates.Ferme",
        "Task_Duration": "updates.Task_Duration",
        "Cree_h": "updates.Cree_h",
        "Ferme_h": "updates.Ferme_h",
        "ID_description": "updates.ID_description",
        "ID_Collaborateur": "updates.ID_Collaborateur",
        "ID_Plage_Horaires": "updates.ID_Plage_Horaires",
        "ID_Type": "updates.ID_Type"
    }
  ) \
  .whenNotMatchedInsert(values =
    {
        "Numero": "updates.Numero",
        "Cree": "updates.Cree",
        "Ferme": "updates.Ferme",
        "Task_Duration": "updates.Task_Duration",
        "Cree_h": "updates.Cree_h",
        "Ferme_h": "updates.Ferme_h",
        "ID_description": "updates.ID_description",
        "ID_Collaborateur": "updates.ID_Collaborateur",
        "ID_Plage_Horaires": "updates.ID_Plage_Horaires",
        "ID_Type": "updates.ID_Type"
    }
  ) \
  .execute()


# In[64]:


display(spark.read.table("refined_gold.factdemande_gold"))


# In[65]:


print(spark.read.table("refined_gold.factdemande_gold").count())

